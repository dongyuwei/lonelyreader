{
   "http://calendar.perfplanet.com/feed/": [
      {
         "title": "Creating a Performance Culture",
         "description": "<p>The performance community is growing. With <a title=\"Web Performance Meetups\" href=\"http://web-performance.meetup.com/all/\">17K members across 46 meetup groups</a> it&#8217;s pretty easy to find someone else who cares about performance. But what if your company is new to the world of high performance websites? How can you make performance a priority within your organization? I don&#8217;t have a guaranteed recipe, but here are some key ingredients for creating a culture of performance where you work.</p>\n<dl style=\"margin-left: 0.5em;\">\n<dt style=\"margin-top: 0.5em;\">Get Support from the Top</dt>\n<dd>If you&#8217;re lucky like me, your CEO is already on the web performance bus. It might even have been their idea to focus on performance, and you&#8217;ve been recruited to lead the charge. If this isn&#8217;t your situation, you have to start your evangelism at the top. You might start with the CEO, or perhaps COO or an SVP. The key is it has to be someone who is a leader across the different organizations within your company. The culture shift to focus on performance doesn&#8217;t happen in engineering alone. It has to happen across product management, marketing, sales, and all other parts of the company. You need to identify who the key leader or leadership team is and get them excited about web performance, and make them believe in the benefits it delivers.</dd>\n<dt style=\"margin-top: 0.5em;\">Speak the Right Vocabulary</dt>\n<dd>As an engineer you probably know how to sell to other engineers. &#8220;Optimization&#8221; makes a developer&#8217;s ears perk up. Speaking in terms of reduced regressions and fewer outages wins over folks in devops. But you also need to know how to speak across the organization both horizontally and vertically. The UX team likes hearing about better user metrics (longer sessions, more sessions per month). The folks in finance wants to hear about reduced operating costs in terms of hardware, power consumption, and data center bandwidth. Marketing and sales will light up hearing case studies about doubling unique users from search engine marketing as a result of a faster website. Make sure to use terms that resonate with your audience.</dd>\n<dd style=\"margin-top: 0.3em;\">A key skill in evangelizing to upper management is knowing how to speak hierarchically &#8211; start with the high-level stats and drill down into the details if the need arises. I see many engineers who start with the details which many folks don&#8217;t have the time or background to follow. Start by showing a median and save the logarithmic scale charts in the &#8220;more slides&#8221; section.</dd>\n<dt style=\"margin-top: 0.5em;\">Pick the Right Product</dt>\n<dd>If you&#8217;ve convinced the senior execs to focus on performance, your next step is to pick a product to focus on. You want to pick a high visibility product, so that the wins are significant. But you don&#8217;t want to pick the company&#8217;s flagship product. It&#8217;s possible you might hit a few bumps on your first forays into adopting web performance. Also, you might have to alter the release cycle as you rollout metrics and start A/B testing. That&#8217;s harder to do with a product that&#8217;s the company&#8217;s cash cow. Start with a product that&#8217;s in the top 5 or 10, but not #1.</dd>\n<dt style=\"margin-top: 0.5em;\">Pick the Right Team</dt>\n<dd>The team you choose to work with is even more important than which product you choose. It all comes down to people, and if the team is too busy, has other priorities, or simply doesn&#8217;t believe in WPO (<a href=\"http://www.stevesouders.com/blog/2010/05/07/wpo-web-performance-optimization/\">Web Performance Optimization</a>) then you should move on to another team. You can always come back and revisit this team in the future.</dd>\n<dd style=\"margin-top: 0.3em;\">I always have a kickoff meeting with a team that&#8217;s interested in working on performance, and I ask how many people they can dedicate to work on performance. I&#8217;m usually looking for at least two people full-time for 3 months. Sometimes teams think it&#8217;s sufficient to have someone spend 20% of their time working on performance, but this usually doesn&#8217;t have a positive outcome. If this is the company&#8217;s first engagement with web performance, you want to make sure to pick a team that has the mindset and resources to focus on the work ahead.</dd>\n<dt style=\"margin-top: 0.5em;\">Pick the Right Task</dt>\n<dd>It&#8217;s critical that the first performance optimization deployed has a significant impact. There&#8217;s nothing more frustrating than getting folks excited about WPO only to have their work show no improvement. For most websites it&#8217;s fairly straightforward to pick an optimization that will have a big impact. I&#8217;m reminded of <a href=\"http://ismailelshareef.com/\">Ismail Elshareef</a>&#8216;s case study about <a href=\"http://www.youtube.com/watch?v=5_-YukDEDBE&amp;list=PL60DCE94FE519EB10#t=9m17s\">Edmunds.com getting 80% faster</a>. He talks about how the first task they picked was making resources cacheable. After just a day of work they pushed the fix and cut their CDN traffic by 34%! This is the type of win you want to have right out of the gate &#8211; something that takes a small amount of work and makes a big improvement.</dd>\n<dt style=\"margin-top: 0.5em;\">Start with Metrics</dt>\n<dd>I&#8217;ve had several engagements where teams got so excited about the optimizations, they started deploying fixes before the metrics were in place. This is bad for two reasons. Without metrics you&#8217;re flying blind so you don&#8217;t know the actual impact of any fixes. But more importantly, it&#8217;s likely that the first fixes you deploy will have the biggest impact. If the metrics aren&#8217;t in place then you miss out on quantifying your best work! It&#8217;s best to establish the baseline when the site is at its worst. Occasionally teams don&#8217;t want to do this because they&#8217;re embarrassed by the slowness of the site. Just remind them how happy the execs will be to see a chart showing the site getting twice as fast.</dd>\n<dt style=\"margin-top: 0.5em;\">Identify Your Replacement</dt>\n<dd>Within the chosen team there needs to be someone who is aligned with you to take over your role. This is the person who keeps the team focused on performance after you&#8217;ve moved on to help the next team. He is the one who tracks the dashboards, identifies the changes that were deployed, analyzes the A/B test results, and prioritizes the next optimizations to work on.</dd>\n<dd style=\"margin-top: 0.3em;\">It&#8217;s not scalable for you to be the only performance expert in the company. You want to build a virtual performance team that spans all the products in the company. At Yahoo! we called this team the SpeedFreaks. We had regular gatherings, a mailing list, etc. It was a great way to share lessons learned across the different teams and re-energize our excitement about making things faster.</dd>\n<dt style=\"margin-top: 0.5em;\">Get Everyone on Board</dt>\n<dd>Making and keeping the website fast requires everyone to be thinking about performance. It&#8217;s important to keep the entire company involved. There are several ways of doing this. One technique I see often is having realtime dashboards deployed at large gathering spots in the company. The Wall of Fame is another good chart. Eventually teams that are always at the bottom will start wondering what they have to do to get to the top. Getting time during the company all-hands meetings to review current performance and highlight some wins is good. Adding performance (speed) to the annual performance (HR) review form makes everyone think about their contributions in the past and plan on how they can contribute in the future.</dd>\n<dt style=\"margin-top: 0.5em;\">Use Carrot over Stick</dt>\n<dd>If you follow these tips it&#8217;s likely that you&#8217;ll start off having successful engagements evangelizing and deploying performance best practices at your company. After working with the choicest teams, however, it&#8217;s also likely that you&#8217;ll run into a team that just isn&#8217;t drinking the WPO kool-aid. This is more likely to happen at larger companies where it&#8217;s more challenging to create a cultural shift. If you can&#8217;t convince this team to apply the right amount of focus, one possible reaction is to bring in a senior exec to command them to make performance a priority and do the work. This might work in the short term, but will fail in the long term and might even set you further back from where you started.</dd>\n<dd style=\"margin-top: 0.3em;\">Performance is a way of thinking. It requires vigilance. Anyone who has it forced upon them will likely not value performance and will instead look upon it as a nuisance that took them away from their desired focus. This person is now even harder to win over. It&#8217;s better to avoid taking the &#8220;stick&#8221; approach and instead use the &#8220;carrot&#8221; as motivation &#8211; t-shirts, bonuses, executive praise, shout outs at company all-hands, etc. No one enjoys the stick approach &#8211; the team doesn&#8217;t enjoy it and neither will you. Everyone comes away with negative memories. The carrot approach might not work in the short term, but it leaves the door open for a more positive re-engagement in the future.</dd>\n<dt style=\"margin-top: 0.5em;\">Be Passionate</dt>\n<dd>It&#8217;s likely you&#8217;re the &#8220;performance lead&#8221; within the company, or at least the one who cares the most about making performance a high priority. It&#8217;s not going to be easy to get everyone else on board. You can&#8217;t go about this half heartedly. You have to be passionate about it. John Rauser spoke (passionately) about this in <a href=\"http://www.youtube.com/watch?feature=player_embedded&amp;v=UL2WDcNu_3A\">Creating Cultural Change</a> at Velocity 2010. He says you have to be excited, and relentless. I agree.</dd>\n</dl>\n<p>Creating a culture of performance at your company is about creating a culture of quality. This is especially true because best (and worst) practices propagate quickly at web companies. Code written for product A is reused by product B. And folks who worked on team A transfer over to team C. If product A is built in a high performance way, those best practices are carried forward by the code and team members. Unfortunately, bad practices spread just as easily.</p>\n<p>Companies like <a title=\"Google's Ten things we know to be true\" href=\"http://www.google.com/about/company/philosophy/\">Google</a>, <a title=\"Etsy October 2012 Site Performance Report\" href=\"http://codeascraft.etsy.com/2012/11/09/october-2012-site-performance-report/\">Etsy</a>, and <a title=\"Betfair's Customer Commitment\" href=\"https://promotions.betfair.com/customer-commitment/\">Betfair</a> have gone so far as to publish their commitment to performance. This is a win for their customers and for their brand. It&#8217;s also a win for the performance community because these companies are more likely to share their best practices and case studies. If your company is focused on performance, please help the community by sharing your lessons learned. If your company doesn&#8217;t have a focus on performance, I hope these tips help you establish that WPO focus to create a website that has a better user experience, more traffic, greater revenue, and reduced operating expenses.</p>",
         "summary": "The performance community is growing. With 17K members across 46 meetup groups it&#8217;s pretty easy to find someone else who cares about performance. But what if your company is new to the world of high performance websites? How can you make performance a priority within your organization? I don&#8217;t have a guaranteed recipe, but here are [...]",
         "date": "2012-12-31T17:00:20.000Z",
         "pubdate": "2012-12-31T17:00:20.000Z",
         "pubDate": "2012-12-31T17:00:20.000Z",
         "link": "http://calendar.perfplanet.com/2012/creating-a-performance-culture/",
         "guid": "http://calendar.perfplanet.com/?p=1558",
         "author": "editor",
         "comments": "http://calendar.perfplanet.com/2012/creating-a-performance-culture/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "performance"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "Creating a Performance Culture"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/creating-a-performance-culture/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/creating-a-performance-culture/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Mon, 31 Dec 2012 17:00:20 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "editor"
         },
         "rss:category": {
            "@": {},
            "#": "performance"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1558"
         },
         "rss:description": {
            "@": {},
            "#": "The performance community is growing. With 17K members across 46 meetup groups it&#8217;s pretty easy to find someone else who cares about performance. But what if your company is new to the world of high performance websites? How can you make performance a priority within your organization? I don&#8217;t have a guaranteed recipe, but here are [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<p>The performance community is growing. With <a title=\"Web Performance Meetups\" href=\"http://web-performance.meetup.com/all/\">17K members across 46 meetup groups</a> it&#8217;s pretty easy to find someone else who cares about performance. But what if your company is new to the world of high performance websites? How can you make performance a priority within your organization? I don&#8217;t have a guaranteed recipe, but here are some key ingredients for creating a culture of performance where you work.</p>\n<dl style=\"margin-left: 0.5em;\">\n<dt style=\"margin-top: 0.5em;\">Get Support from the Top</dt>\n<dd>If you&#8217;re lucky like me, your CEO is already on the web performance bus. It might even have been their idea to focus on performance, and you&#8217;ve been recruited to lead the charge. If this isn&#8217;t your situation, you have to start your evangelism at the top. You might start with the CEO, or perhaps COO or an SVP. The key is it has to be someone who is a leader across the different organizations within your company. The culture shift to focus on performance doesn&#8217;t happen in engineering alone. It has to happen across product management, marketing, sales, and all other parts of the company. You need to identify who the key leader or leadership team is and get them excited about web performance, and make them believe in the benefits it delivers.</dd>\n<dt style=\"margin-top: 0.5em;\">Speak the Right Vocabulary</dt>\n<dd>As an engineer you probably know how to sell to other engineers. &#8220;Optimization&#8221; makes a developer&#8217;s ears perk up. Speaking in terms of reduced regressions and fewer outages wins over folks in devops. But you also need to know how to speak across the organization both horizontally and vertically. The UX team likes hearing about better user metrics (longer sessions, more sessions per month). The folks in finance wants to hear about reduced operating costs in terms of hardware, power consumption, and data center bandwidth. Marketing and sales will light up hearing case studies about doubling unique users from search engine marketing as a result of a faster website. Make sure to use terms that resonate with your audience.</dd>\n<dd style=\"margin-top: 0.3em;\">A key skill in evangelizing to upper management is knowing how to speak hierarchically &#8211; start with the high-level stats and drill down into the details if the need arises. I see many engineers who start with the details which many folks don&#8217;t have the time or background to follow. Start by showing a median and save the logarithmic scale charts in the &#8220;more slides&#8221; section.</dd>\n<dt style=\"margin-top: 0.5em;\">Pick the Right Product</dt>\n<dd>If you&#8217;ve convinced the senior execs to focus on performance, your next step is to pick a product to focus on. You want to pick a high visibility product, so that the wins are significant. But you don&#8217;t want to pick the company&#8217;s flagship product. It&#8217;s possible you might hit a few bumps on your first forays into adopting web performance. Also, you might have to alter the release cycle as you rollout metrics and start A/B testing. That&#8217;s harder to do with a product that&#8217;s the company&#8217;s cash cow. Start with a product that&#8217;s in the top 5 or 10, but not #1.</dd>\n<dt style=\"margin-top: 0.5em;\">Pick the Right Team</dt>\n<dd>The team you choose to work with is even more important than which product you choose. It all comes down to people, and if the team is too busy, has other priorities, or simply doesn&#8217;t believe in WPO (<a href=\"http://www.stevesouders.com/blog/2010/05/07/wpo-web-performance-optimization/\">Web Performance Optimization</a>) then you should move on to another team. You can always come back and revisit this team in the future.</dd>\n<dd style=\"margin-top: 0.3em;\">I always have a kickoff meeting with a team that&#8217;s interested in working on performance, and I ask how many people they can dedicate to work on performance. I&#8217;m usually looking for at least two people full-time for 3 months. Sometimes teams think it&#8217;s sufficient to have someone spend 20% of their time working on performance, but this usually doesn&#8217;t have a positive outcome. If this is the company&#8217;s first engagement with web performance, you want to make sure to pick a team that has the mindset and resources to focus on the work ahead.</dd>\n<dt style=\"margin-top: 0.5em;\">Pick the Right Task</dt>\n<dd>It&#8217;s critical that the first performance optimization deployed has a significant impact. There&#8217;s nothing more frustrating than getting folks excited about WPO only to have their work show no improvement. For most websites it&#8217;s fairly straightforward to pick an optimization that will have a big impact. I&#8217;m reminded of <a href=\"http://ismailelshareef.com/\">Ismail Elshareef</a>&#8216;s case study about <a href=\"http://www.youtube.com/watch?v=5_-YukDEDBE&amp;list=PL60DCE94FE519EB10#t=9m17s\">Edmunds.com getting 80% faster</a>. He talks about how the first task they picked was making resources cacheable. After just a day of work they pushed the fix and cut their CDN traffic by 34%! This is the type of win you want to have right out of the gate &#8211; something that takes a small amount of work and makes a big improvement.</dd>\n<dt style=\"margin-top: 0.5em;\">Start with Metrics</dt>\n<dd>I&#8217;ve had several engagements where teams got so excited about the optimizations, they started deploying fixes before the metrics were in place. This is bad for two reasons. Without metrics you&#8217;re flying blind so you don&#8217;t know the actual impact of any fixes. But more importantly, it&#8217;s likely that the first fixes you deploy will have the biggest impact. If the metrics aren&#8217;t in place then you miss out on quantifying your best work! It&#8217;s best to establish the baseline when the site is at its worst. Occasionally teams don&#8217;t want to do this because they&#8217;re embarrassed by the slowness of the site. Just remind them how happy the execs will be to see a chart showing the site getting twice as fast.</dd>\n<dt style=\"margin-top: 0.5em;\">Identify Your Replacement</dt>\n<dd>Within the chosen team there needs to be someone who is aligned with you to take over your role. This is the person who keeps the team focused on performance after you&#8217;ve moved on to help the next team. He is the one who tracks the dashboards, identifies the changes that were deployed, analyzes the A/B test results, and prioritizes the next optimizations to work on.</dd>\n<dd style=\"margin-top: 0.3em;\">It&#8217;s not scalable for you to be the only performance expert in the company. You want to build a virtual performance team that spans all the products in the company. At Yahoo! we called this team the SpeedFreaks. We had regular gatherings, a mailing list, etc. It was a great way to share lessons learned across the different teams and re-energize our excitement about making things faster.</dd>\n<dt style=\"margin-top: 0.5em;\">Get Everyone on Board</dt>\n<dd>Making and keeping the website fast requires everyone to be thinking about performance. It&#8217;s important to keep the entire company involved. There are several ways of doing this. One technique I see often is having realtime dashboards deployed at large gathering spots in the company. The Wall of Fame is another good chart. Eventually teams that are always at the bottom will start wondering what they have to do to get to the top. Getting time during the company all-hands meetings to review current performance and highlight some wins is good. Adding performance (speed) to the annual performance (HR) review form makes everyone think about their contributions in the past and plan on how they can contribute in the future.</dd>\n<dt style=\"margin-top: 0.5em;\">Use Carrot over Stick</dt>\n<dd>If you follow these tips it&#8217;s likely that you&#8217;ll start off having successful engagements evangelizing and deploying performance best practices at your company. After working with the choicest teams, however, it&#8217;s also likely that you&#8217;ll run into a team that just isn&#8217;t drinking the WPO kool-aid. This is more likely to happen at larger companies where it&#8217;s more challenging to create a cultural shift. If you can&#8217;t convince this team to apply the right amount of focus, one possible reaction is to bring in a senior exec to command them to make performance a priority and do the work. This might work in the short term, but will fail in the long term and might even set you further back from where you started.</dd>\n<dd style=\"margin-top: 0.3em;\">Performance is a way of thinking. It requires vigilance. Anyone who has it forced upon them will likely not value performance and will instead look upon it as a nuisance that took them away from their desired focus. This person is now even harder to win over. It&#8217;s better to avoid taking the &#8220;stick&#8221; approach and instead use the &#8220;carrot&#8221; as motivation &#8211; t-shirts, bonuses, executive praise, shout outs at company all-hands, etc. No one enjoys the stick approach &#8211; the team doesn&#8217;t enjoy it and neither will you. Everyone comes away with negative memories. The carrot approach might not work in the short term, but it leaves the door open for a more positive re-engagement in the future.</dd>\n<dt style=\"margin-top: 0.5em;\">Be Passionate</dt>\n<dd>It&#8217;s likely you&#8217;re the &#8220;performance lead&#8221; within the company, or at least the one who cares the most about making performance a high priority. It&#8217;s not going to be easy to get everyone else on board. You can&#8217;t go about this half heartedly. You have to be passionate about it. John Rauser spoke (passionately) about this in <a href=\"http://www.youtube.com/watch?feature=player_embedded&amp;v=UL2WDcNu_3A\">Creating Cultural Change</a> at Velocity 2010. He says you have to be excited, and relentless. I agree.</dd>\n</dl>\n<p>Creating a culture of performance at your company is about creating a culture of quality. This is especially true because best (and worst) practices propagate quickly at web companies. Code written for product A is reused by product B. And folks who worked on team A transfer over to team C. If product A is built in a high performance way, those best practices are carried forward by the code and team members. Unfortunately, bad practices spread just as easily.</p>\n<p>Companies like <a title=\"Google's Ten things we know to be true\" href=\"http://www.google.com/about/company/philosophy/\">Google</a>, <a title=\"Etsy October 2012 Site Performance Report\" href=\"http://codeascraft.etsy.com/2012/11/09/october-2012-site-performance-report/\">Etsy</a>, and <a title=\"Betfair's Customer Commitment\" href=\"https://promotions.betfair.com/customer-commitment/\">Betfair</a> have gone so far as to publish their commitment to performance. This is a win for their customers and for their brand. It&#8217;s also a win for the performance community because these companies are more likely to share their best practices and case studies. If your company is focused on performance, please help the community by sharing your lessons learned. If your company doesn&#8217;t have a focus on performance, I hope these tips help you establish that WPO focus to create a website that has a better user experience, more traffic, greater revenue, and reduced operating expenses.</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/creating-a-performance-culture/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "9"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      },
      {
         "title": "Speed Up Your Site Using Prefetching",
         "description": "<p>The concept of prefetching is pretty simple. We often know about resources the browser is likely to need before the browser does. Prefetching involves either giving the browser hints of pages or resources it is likely to need so that it can download them ahead of time, or actually downloading resources into the browser cache before needed so that the overhead of requesting and downloading the object can be preemptively handled or done in a non-blocking way.</p>\n<p>There are many ways to prefetch content, but here are 3 simple options.</p>\n<h2>DNS Prefetching</h2>\n<p>DNS is the protocol that converts human readable domains (mysite.com) into computer readable IPs (123.123.123.123). DNS resolution is generally pretty fast and measured in 100&#8242;s of milliseconds, but because it must happen before any request to the server can be made it can cause a cascade effect that has a real impact on the overall load time of a page. Often we know about several other domains that will need to be loaded for resources later in the page or user session, such as subdomains for static content (images.mydomain.com) or domains for 3rd party content. Some browsers support a meta tag that identifies these domains that need to be resolved so the browser can resolve them ahead of time. The tag to do this is pretty straight forward:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">//my.domain.com</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">dns-prefetch</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-brackets\">/&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">http://my.domain.com/</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">prefetch</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-brackets\">/&gt;</span><span class=\"hl-code\"> </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-code\">!– </span><span class=\"hl-var\">IE9</span><span class=\"hl-code\">+ –</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<p>Adding this tag causes the browser to do the DNS resolution ahead of time, instead of waiting until a resource requires it later. This technique is probably most valuable to preload DNS for content on other pages on your site that visitors are likely to go to. This feature is supported in Chrome, Firefox, and IE9+.</p>\n<p>Although shaving a few hundred milliseconds might seem trivial, in aggregate this can be a measurable gain. It&#8217;s also a safe optimization and easy to implement. I was curious to see how often this technique is used, so I crawled the top 100K Alexa sites. It turns out only 552 sites (0.55%) are currently using DNS prefetching. This is a cheap win, and something more sites should leverage.</p>\n<h2>Resource Prefetching</h2>\n<p>Images make up a large portion of the overall bytes of many major websites today. Often the overhead of making the requests and downloading images can have a significant performance impact. In many cases, though, the site developer knows when an image will be needed that won&#8217;t be detected early by the browser, such as an image loaded from an ajax request or other user action on the page. Resource prefetching is when you load an image, script, stylesheet, or other resource into the browser preemptively. This is most often done with images, but can be done with any type of resource that can be cached in the browser.</p>\n<p>Of the three techniques I&#8217;m covering here, this is by far the oldest and the most used. Unfortunately I can&#8217;t give a concrete number about adoption because there are too many ways to implement this to detect in my Alexa crawl. Still, many sites don&#8217;t properly leverage this technique and even just preloading a few images can make a huge difference for the user experience.</p>\n<h2>Page Prefetching / Prerendering</h2>\n<p>Page prefetching is very similar to resource prefetching, except that we actually load the new page itself preemptively. This was first made available in Firefox. You can hint to the browser that a page (or an individual resource) should be prefetched by including the following tag:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">prefetch</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">/my-next-page.htm</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<p>In the case of prerendering, the browser not only downloads the page, but also the necessary resources for that page. It also begins to render the page in memory (not visible to the user) so that when the request for the page is made it can appear nearly instantaneous to the user. Prerendering was first added in Chrome. You can hint that a page should be prerendered by including the following tag:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">prerender</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">http://mydomain.com/my-next-page.htm</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<p>This technique is by far the most controversial and the riskiest of the three. Prerendering a page should only be done when there is a high confidence that the user will go to that page next. The most well known example of this is Google Search, which will prerender the first result of the page if the confidence is high enough. I found only 95 examples of this in my crawl of the Alexa Top 100k sites. Although this technique is clearly not for every use case, I think many more sites could leverage this to improve the user experience.</p>\n<h2>The Downsides</h2>\n<p>Prefetching in general is often a controversial topic. Many people argue that it is not efficient and leads to a waste in bandwidth. It also uses client resources unnecessarily (most notably on mobile devices). Also worth mentioning is that in some cases prefetching or prerendering of pages can have adverse effects on analytics and log tracking since there is no obvious way to discern a user visiting the page (and seeing it) or simply the browser prerendering without the user&#8217;s knowledge.</p>\n<h2>Conclusion</h2>\n<p>Despite all of these cautions, prefetching can be a huge win. The fastest request is always the one we never have to make and getting as much into the cache as possible is the best way to make that happen. By making these expensive requests when the user is not waiting on them, we can greatly improve the perceived performance of even the slowest sites on the slowest networks. If you&#8217;re not already doing so, it&#8217;s worth trying these techniques on your site. The results will vary, so be sure to use Real User Measurement (e.g. <a href=\"http://torbit.com\">Torbit</a>) to find out how much of an improvement prefetching makes for you.</p>",
         "summary": "The concept of prefetching is pretty simple. We often know about resources the browser is likely to need before the browser does. Prefetching involves either giving the browser hints of pages or resources it is likely to need so that it can download them ahead of time, or actually downloading resources into the browser cache [...]",
         "date": "2012-12-30T19:20:42.000Z",
         "pubdate": "2012-12-30T19:20:42.000Z",
         "pubDate": "2012-12-30T19:20:42.000Z",
         "link": "http://calendar.perfplanet.com/2012/speed-up-your-site-using-prefetching/",
         "guid": "http://calendar.perfplanet.com/?p=1556",
         "author": "stoyan",
         "comments": "http://calendar.perfplanet.com/2012/speed-up-your-site-using-prefetching/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "performance"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "Speed Up Your Site Using Prefetching"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/speed-up-your-site-using-prefetching/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/speed-up-your-site-using-prefetching/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Sun, 30 Dec 2012 19:20:42 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "stoyan"
         },
         "rss:category": {
            "@": {},
            "#": "performance"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1556"
         },
         "rss:description": {
            "@": {},
            "#": "The concept of prefetching is pretty simple. We often know about resources the browser is likely to need before the browser does. Prefetching involves either giving the browser hints of pages or resources it is likely to need so that it can download them ahead of time, or actually downloading resources into the browser cache [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<p>The concept of prefetching is pretty simple. We often know about resources the browser is likely to need before the browser does. Prefetching involves either giving the browser hints of pages or resources it is likely to need so that it can download them ahead of time, or actually downloading resources into the browser cache before needed so that the overhead of requesting and downloading the object can be preemptively handled or done in a non-blocking way.</p>\n<p>There are many ways to prefetch content, but here are 3 simple options.</p>\n<h2>DNS Prefetching</h2>\n<p>DNS is the protocol that converts human readable domains (mysite.com) into computer readable IPs (123.123.123.123). DNS resolution is generally pretty fast and measured in 100&#8242;s of milliseconds, but because it must happen before any request to the server can be made it can cause a cascade effect that has a real impact on the overall load time of a page. Often we know about several other domains that will need to be loaded for resources later in the page or user session, such as subdomains for static content (images.mydomain.com) or domains for 3rd party content. Some browsers support a meta tag that identifies these domains that need to be resolved so the browser can resolve them ahead of time. The tag to do this is pretty straight forward:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">//my.domain.com</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">dns-prefetch</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-brackets\">/&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">http://my.domain.com/</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">prefetch</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-brackets\">/&gt;</span><span class=\"hl-code\"> </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-code\">!– </span><span class=\"hl-var\">IE9</span><span class=\"hl-code\">+ –</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<p>Adding this tag causes the browser to do the DNS resolution ahead of time, instead of waiting until a resource requires it later. This technique is probably most valuable to preload DNS for content on other pages on your site that visitors are likely to go to. This feature is supported in Chrome, Firefox, and IE9+.</p>\n<p>Although shaving a few hundred milliseconds might seem trivial, in aggregate this can be a measurable gain. It&#8217;s also a safe optimization and easy to implement. I was curious to see how often this technique is used, so I crawled the top 100K Alexa sites. It turns out only 552 sites (0.55%) are currently using DNS prefetching. This is a cheap win, and something more sites should leverage.</p>\n<h2>Resource Prefetching</h2>\n<p>Images make up a large portion of the overall bytes of many major websites today. Often the overhead of making the requests and downloading images can have a significant performance impact. In many cases, though, the site developer knows when an image will be needed that won&#8217;t be detected early by the browser, such as an image loaded from an ajax request or other user action on the page. Resource prefetching is when you load an image, script, stylesheet, or other resource into the browser preemptively. This is most often done with images, but can be done with any type of resource that can be cached in the browser.</p>\n<p>Of the three techniques I&#8217;m covering here, this is by far the oldest and the most used. Unfortunately I can&#8217;t give a concrete number about adoption because there are too many ways to implement this to detect in my Alexa crawl. Still, many sites don&#8217;t properly leverage this technique and even just preloading a few images can make a huge difference for the user experience.</p>\n<h2>Page Prefetching / Prerendering</h2>\n<p>Page prefetching is very similar to resource prefetching, except that we actually load the new page itself preemptively. This was first made available in Firefox. You can hint to the browser that a page (or an individual resource) should be prefetched by including the following tag:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">prefetch</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">/my-next-page.htm</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<p>In the case of prerendering, the browser not only downloads the page, but also the necessary resources for that page. It also begins to render the page in memory (not visible to the user) so that when the request for the page is made it can appear nearly instantaneous to the user. Prerendering was first added in Chrome. You can hint that a page should be prerendered by including the following tag:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">prerender</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">http://mydomain.com/my-next-page.htm</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<p>This technique is by far the most controversial and the riskiest of the three. Prerendering a page should only be done when there is a high confidence that the user will go to that page next. The most well known example of this is Google Search, which will prerender the first result of the page if the confidence is high enough. I found only 95 examples of this in my crawl of the Alexa Top 100k sites. Although this technique is clearly not for every use case, I think many more sites could leverage this to improve the user experience.</p>\n<h2>The Downsides</h2>\n<p>Prefetching in general is often a controversial topic. Many people argue that it is not efficient and leads to a waste in bandwidth. It also uses client resources unnecessarily (most notably on mobile devices). Also worth mentioning is that in some cases prefetching or prerendering of pages can have adverse effects on analytics and log tracking since there is no obvious way to discern a user visiting the page (and seeing it) or simply the browser prerendering without the user&#8217;s knowledge.</p>\n<h2>Conclusion</h2>\n<p>Despite all of these cautions, prefetching can be a huge win. The fastest request is always the one we never have to make and getting as much into the cache as possible is the best way to make that happen. By making these expensive requests when the user is not waiting on them, we can greatly improve the perceived performance of even the slowest sites on the slowest networks. If you&#8217;re not already doing so, it&#8217;s worth trying these techniques on your site. The results will vary, so be sure to use Real User Measurement (e.g. <a href=\"http://torbit.com\">Torbit</a>) to find out how much of an improvement prefetching makes for you.</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/speed-up-your-site-using-prefetching/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "27"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      },
      {
         "title": "Optimizing Your Network Stack for Optimal Mobile Web Performance",
         "description": "<p>We spend a lot of time at CloudFlare thinking about how to make the Internet fast on mobile devices. Currently there are over 1.2 billion active mobile users and that number is growing rapidly. Earlier this year mobile Internet access passed fixed Internet access in India and that&#8217;s likely to be repeated the world over. So, mobile network performance will only become more and more important.</p>\n<p>Most of the focus today on improving mobile performance is on Layer 7 with front end optimizations (FEO). At CloudFlare, we&#8217;ve done significant work in this area with front end optimization technologies like <a href=\"https://www.cloudflare.com/features-optimizer\">Rocket Loader, Mirage, and Polish</a> that dynamically modify web content to make it load quickly whatever device is being used. However, while FEO is important to make mobile fast, the unique characteristics of mobile networks also means we have to pay attention to the underlying performance of the technologies down at Layer 4 of the network stack.</p>\n<p>This article is about the challenges mobile devices present, how the default TCP configuration is ill-suited for optimal mobile performance, and what you can do to improve performance for visitors connecting via mobile networks. Before diving into the details, a quick technical note. At CloudFlare, we&#8217;ve built most of our systems on top of a custom version of Linux so, while the underlying technologies can apply to other operating systems, the examples I&#8217;ll use are from Linux.</p>\n<h3>TCP Congestion Control</h3>\n<p>To understand the challenges of mobile network performance at Layer 4 of the networking stack you need to understand TCP Congestion Control. TCP Congestion Control is the gatekeeper that determines how to control the flow of packets from your server to your clients. Its goal is to prevent Internet congestion by detecting when congestion occurs and slowing down the rate data is transmitted. This helps ensure that the Internet is available to everyone, but can cause problems on mobile network when TCP mistakes mobile network problems for congestion.</p>\n<p>TCP Congestion Control holds back the floodgates if it detects congestion (i.e. packet loss) on the remote end. A network is, inherently, a shared resource. The purpose of TCP Congestion Control was to ensure that every device on the network cooperates to not overwhelm its resource. On a wired network, if packet loss is detected it is a fairly reliable indicator that a port along the connection is overburdened. What is typically going on in these cases is that a memory buffer in a switch somewhere has filled beyond its capacity because packets are coming in faster than they can be sent out and data is being discarded. TCP Congestion Control on clients and servers is setup to &#8220;back off&#8221; in these cases in order to ensure that the network remains available for all its users.</p>\n<p>But figuring out what packet loss means on a mobile network is a different matter. Radio networks are inherently susceptible to interference which results in packet loss. If packets are being dropped does that mean a switch is overburdened, like we can infer on a wired network? Or did someone travel from an under-subscribed wireless cell to an oversubscribed one? Or did someone just turn on a microwave? Or maybe it was just a random solar flare? Since it&#8217;s not as clear what packet loss means on a mobile network, it&#8217;s not clear what action a TCP Congestion Control algorithm should take.</p>\n<h3>A Series of Leaky Tubes</h3>\n<p>To optimize networks for lossy networks like those on mobile networks, it&#8217;s important to understand exactly how TCP Congestion Control algorithms are designed. While the high level concept makes sense, the details of TCP Congestion Control are not widely understood by most people working in the web performance industry. That said, it is an important core part of what makes the Internet reliable and the subject of very active research and development.</p>\n<p>To understand how TCP Congestion Control algorithms work, imagine the following analogy. Think of your web server as your local water utility plant. You&#8217;ve built on a large network of pipes in your hometown and you need to guarantee that each pipe is as pressurized as possible for delivery, but you don&#8217;t want to burst the pipes. (Note: I recognize the late Senator Ted Stevens got a lot of flack for describing the Internet as a &#8220;series of tubes,&#8221; but the metaphor is surprisingly accurate.)</p>\n<p>Your client, Crazy Arty, runs a local water bottling plant that connects to your pipe network. Crazy Arty&#8217;s infrastructure is built on old pipes that are leaky and brittle. For you to get water to them without bursting his pipes, you need to infer the capability of Crazy Arty&#8217;s system. If you don&#8217;t know in advance then you do a test — you send a known amount of water to the line and then measure the pressure. If the pressure is suddenly lost then you can infer that you broke a pipe. If not, then that level is likely safe and you can add more water pressure and repeat the test. You can iterate this test until you burst a pipe, see the drop in pressure, write down the maximum water volume, and going forward ensure you never exceed it.</p>\n<p>Imagine, however, that there&#8217;s some exogenous factor that could decrease the pressure in the pipe without actually indicating a pipe had burst. What if, for example, Crazy Arty ran a pump that he only turned on randomly from time to time and you didn&#8217;t know about. If the only signal you have is observing a loss in pressure, you&#8217;d have no way of knowing whether you&#8217;d burst a pipe or if Crazy Arty had just plugged in the pump. The effect would be that you&#8217;d likely record a pressure level much less than the amount the pipes could actually withstand — leading to all your customers on the network potentially having lower water pressure than they should.</p>\n<h3>Optimizing for Congestion or Loss</h3>\n<p>If you&#8217;ve been following up to this point then you already know more about TCP Congestion Control than you would guess. The initial amount of water we talked about in TCP is known as the Initial Congestion Window (initcwnd) it is the initial number of packets in flight across the network. The congestion window (cwnd) either shrinks, grows, or stays the same depending on how many packets make it back and how fast (in ACK trains) they return after the initial burst. In essence, TCP Congestion Control is just like the water utility — measuring the pressure a network can withstand and then adjusting the volume in an attempt to maximize flow without bursting any pipes.</p>\n<p>When a TCP connection is first established it attempts to ramp up the cwnd quickly. This phase of the connection, where TCP grows the cwnd rapidly, is called Slow Start. That&#8217;s a bit of a misnomer since it is generally an exponential growth function which is quite fast and aggressive. Just like when the water utility in the example above detects a drop in pressure it turns down the volume of water, when TCP detects packets are lost it reduces the size of the cwnd and delays the time before another burst of packets is delivered. The time between packet bursts is known as the Retransmission Timeout (RTO). The algorithm within TCP that controls these processes is called the Congestion Control Algorithm. There are many congestion control algorithms and clients and servers can use different strategies based in the characteristics of their networks. Most of Congestion Control Algorithms focus on optimizing for one type of network loss or another: congestive loss (like you see on wired networks) or random loss (like you see on mobile networks).</p>\n<p>In the example above, a pipe bursting would be an indication of congestive loss. There was a physical limit to the pipes, it is exceeded, and the appropriate response is to back off. On the other hand, Crazy Arty&#8217;s pump is analogous to random loss. The capacity is still available on the network and only a temporary disturbance causes the water utility to see the pipes as overfull. The Internet started as a network of wired devices, and, as its name suggests, congestion control was largely designed to optimize for congestive loss. As a result, the default Congestion Control Algorithm in many operating systems is good for communicating wired networks but not as good for communicating with mobile networks.</p>\n<p>A few Congestion Control algorithms try to bridge the gap by using the time of the delay in the &#8220;pressure increase&#8221; to &#8220;expected capacity&#8221; to figure out the cause of the loss. These are known as bandwidth estimation algorithms, and examples include <a href=\"http://en.wikipedia.org/wiki/TCP_Vegas\">Vegas</a>, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.5736\">Veno</a> and <a href=\"http://en.wikipedia.org/wiki/TCP_Westwood_plus\">Westwood+</a>. Unfortunately, all of these methods are reactive and reuse no information across two similar streams.</p>\n<p>At companies that see a significant amount of network traffic, like CloudFlare or Google, it is possible to map the characteristics of the Internet&#8217;s networks and choose a specific congestion control algorithm in order to maximize performance for that network. Unfortunately, unless you are seeing the large amounts of traffic as we do and can record data on network performance, the ability to instrument your congestion control or build a &#8220;weather forecast&#8221; is usually impossible. Fortunately, there are still several things you can do to make your server more responsive to visitors even when they&#8217;re coming from lossy, mobile devices.</p>\n<h3>Compelling Reasons to Upgrade Your Kernel</h3>\n<p>The Linux network stack has been under extensive development to bring about some sensible defaults and mechanisms for dealing with the network topology of 2012. A mixed network of high bandwidth low latency and high bandwidth, high latency, lossy connections was never fully anticipated by the kernel developers of 2009 and if you check your server&#8217;s kernel version chances are its running a 2.6.32.x kernel from that era.</p>\n<p><code>uname -a</code></p>\n<p>There are a number of reasons that if you&#8217;re running an old kernel on your web server and want to increase web performance, especially for mobile devices, you should investigate upgrading. To begin, Linux 2.6.38 bumps the default initcwnd and initrwnd (inital receive window) from <a href=\"http://www.ietf.org/rfc/rfc3390.txt\">3 to 10</a>. This is an easy, big win. It allows for 14.2KB (vs 5.7KB) of data to be sent or received in the initial round trip before slow start grows the cwnd further. This is important for HTTP and SSL because it gives you more room to fit the header in the initial set of packets. If you are running an older kernel you may be able to run the following command on a bash shell (use caution) to set all of your routes&#8217; initcwnd and initrwnd to 10. On average, this small change can be one of the biggest boosts when you&#8217;re trying to maximize web performance.</p>\n<p><code>ip route | while read p; do ip route change $p initcwnd 10 initrwnd 10; done</code></p>\n<p>Linux kernel 3.2 implements <a href=\"http://tools.ietf.org/html/draft-mathis-tcpm-proportional-rate-reduction-01\">Proportional Rate Reduction (PRR)</a>. PRR decreases the time it takes for a lossy connection to recover its full speed, potentially improving HTTP response times by 3-10%. The benefits of PRR are significant for mobile networks. To understand why, it&#8217;s worth diving back into the details of how previous congestion control strategies interacted with loss.</p>\n<p>Many congestion control algorithms halve the cwnd when a loss is detected. When multiple losses occur this can result in a case where the cwnd is lower than the slow start threshold. Unfortunately, the connection never goes through slow start again. The result is that a few network interruptions can result in TCP slowing to a crawl for all the connections in the session.</p>\n<p>This is even more deadly when combined with tcp_no_metrics_save=0 sysctl setting on unpatched kernels before 3.2. This setting will save data on connections and attempt to use it to optimize the network. Unfortunately, this can actually make performance worse because TCP will apply the exception case to every new connection from a client within a window of a few minutes. In other words, in some cases, one person surfing your site from a mobile phone who has some random packet loss can reduce your server&#8217;s performance to this visitor even when their temporary loss has cleared.</p>\n<p>If you expect your visitors to be coming from mobile, lossy connections and you cannot upgrade or patch your kernel I recommend setting tcp_no_metrics_save=1. If you&#8217;re comfortable doing some hacking, you can <a href=\"http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commitdiff;h=a262f0cdf1f2916ea918dc329492abb5323d9a6c\">patch older kernels.</a></p>\n<p>The good news is that Linux 3.2 implements PRR, which decreases the amount of time that a lossy connection will impact TCP performance. If you can upgrade, it may be one of the most significant things you can do in order to increase your web performance.</p>\n<h3>More Improvements Ahead</h3>\n<p>Linux 3.2 also has another important improvement with RFC2099bis. The initial Retransmission Timeout (initRTO) has been changed to 1s from 3s. If loss happens after sending the initcwnd two seconds waiting time are saved when trying to resend the data. With TCP streams being so short this can have a very noticeable improvement if a connection experiences loss at the beginning of the stream. Like the PRR patch this can also be applied (with modification) to older kernels if for some reason you cannot upgrade (<a href=\"http://git.kernel.org/?p=linux/kernel/git/stable/linux-stable.git;a=commit;h=9ad7c049f0f79c418e293b1b68cf10d68f54fcdb\">here&#8217;s the patch</a>).</p>\n<p>Looking forward, Linux 3.3 has Byte Queue Limits when teamed with CoDel (controlled delay) in the 3.5 kernel helps fight the long standing issue of <a href=\"http://www.bufferbloat.net/projects/bloat/wiki/Introduction\">Bufferbloat</a> by intelligently managing packet queues. Bufferbloat is when the queuing overhead on the network stack becomes backed up because its littered with stale data. Linux 3.3 has features to auto QoS important packets (SYN/DNS/ARP/etc.,) keep down buffer queues thereby reducing bufferbloat and improving latency on loaded servers.</p>\n<p>Linux 3.5 implements <a href=\"http://tools.ietf.org/html/rfc5827\">TCP Early Retransmit</a> with some safeguards for connections that have a small amount of packet reordering. This allows connections, under certain conditions, to trigger fast retransmit and bypass the costly Retransmission Timeout (RTO) mentioned earlier. By default it is enabled in the failsafe mode tcp_early_retrans=2. If for some reason you are sure your clients have loss but no reordering then you could set tcp_early_retrans=1 to save one quarter a RTT on recovery.</p>\n<p>One of the most extensive changes to 3.6 that hasn&#8217;t got much press is the removal of the IPv4 routing cache. In a nutshell it was an extraneous caching layer in the kernel that mapped interfaces to routes to IPs and saved a lookup to the Forward Information Base (FIB). The FIB is a routing table within the network stack. The IPv4 routing cache was intended to eliminate a FIB lookup and increase performance. While a good idea in principle, unfortunately it provided a very small performance boost in less than 10% of connections. In the 3.2.x-3.5.x kernels it was extremely vulnerable to certain DDoS techniques so it has been removed.</p>\n<p>Finally, one important setting you should check, regardless of the Linux kernel you are running: tcp_slow_start_after_idle. If you&#8217;re concerned about web performance, it has been proclaimed sysctl setting of the year. It can be enabled in almost any kernel. By default this is set to 1 which will aggressively reduce cwnd on idle connections and negatively impact any long lived connections such as SSL. The following command will set it to 0 and can significantly improve performance:</p>\n<p><code>sysctl -w tcp_slow_start_after_idle=0</code></p>\n<h3>The Missing Congestion Control Algorithm</h3>\n<p>You may be curious as to why I haven&#8217;t made a recommendation as far as a quick and easy change of congestion control algorithms. Since Linux 2.6.19, the default congestion control algorithm in the Linux kernel is CUBIC, which is time based and optimized for high speed and high latency networks. Its killer feature, known as called Hybrid Slow Start (HyStart), allows it to safely exit slow start by measuring the ACK trains and not overshoot the cwnd. It can improve startup throughput by up to 200-300%.</p>\n<p>While other Congestion Control Algorithms may seem like performance wins on connections experiencing high amounts of loss (>.1%) (e.g., TCP Westwood+ or Hybla), unfortunately these algorithms don&#8217;t include HyStart. The net effect is that, in our tests, they under perform CUBIC for general network performance. Unless a majority of your clients are on lossy connections, I recommend staying with CUBIC.</p>\n<p>Of course the real answer here is to dynamically swap out congestion control algorithms based on historical data to better serve these edge cases. Unfortunately, that is difficult for the average web server unless you&#8217;re seeing a very high volume of traffic and are able to record and analyze network characteristics across multiple connections. The good news is that loss predictors and hybrid congestion control algorithms are continuing to mature, so maybe we will have an answer in an upcoming kernel.</p>",
         "summary": "We spend a lot of time at CloudFlare thinking about how to make the Internet fast on mobile devices. Currently there are over 1.2 billion active mobile users and that number is growing rapidly. Earlier this year mobile Internet access passed fixed Internet access in India and that&#8217;s likely to be repeated the world over. [...]",
         "date": "2012-12-30T19:19:51.000Z",
         "pubdate": "2012-12-30T19:19:51.000Z",
         "pubDate": "2012-12-30T19:19:51.000Z",
         "link": "http://calendar.perfplanet.com/2012/optimizing-your-network-stack-for-optimal-mobile-web-performance/",
         "guid": "http://calendar.perfplanet.com/?p=1549",
         "author": "stoyan",
         "comments": "http://calendar.perfplanet.com/2012/optimizing-your-network-stack-for-optimal-mobile-web-performance/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "performance"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "Optimizing Your Network Stack for Optimal Mobile Web Performance"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/optimizing-your-network-stack-for-optimal-mobile-web-performance/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/optimizing-your-network-stack-for-optimal-mobile-web-performance/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Sun, 30 Dec 2012 19:19:51 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "stoyan"
         },
         "rss:category": {
            "@": {},
            "#": "performance"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1549"
         },
         "rss:description": {
            "@": {},
            "#": "We spend a lot of time at CloudFlare thinking about how to make the Internet fast on mobile devices. Currently there are over 1.2 billion active mobile users and that number is growing rapidly. Earlier this year mobile Internet access passed fixed Internet access in India and that&#8217;s likely to be repeated the world over. [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<p>We spend a lot of time at CloudFlare thinking about how to make the Internet fast on mobile devices. Currently there are over 1.2 billion active mobile users and that number is growing rapidly. Earlier this year mobile Internet access passed fixed Internet access in India and that&#8217;s likely to be repeated the world over. So, mobile network performance will only become more and more important.</p>\n<p>Most of the focus today on improving mobile performance is on Layer 7 with front end optimizations (FEO). At CloudFlare, we&#8217;ve done significant work in this area with front end optimization technologies like <a href=\"https://www.cloudflare.com/features-optimizer\">Rocket Loader, Mirage, and Polish</a> that dynamically modify web content to make it load quickly whatever device is being used. However, while FEO is important to make mobile fast, the unique characteristics of mobile networks also means we have to pay attention to the underlying performance of the technologies down at Layer 4 of the network stack.</p>\n<p>This article is about the challenges mobile devices present, how the default TCP configuration is ill-suited for optimal mobile performance, and what you can do to improve performance for visitors connecting via mobile networks. Before diving into the details, a quick technical note. At CloudFlare, we&#8217;ve built most of our systems on top of a custom version of Linux so, while the underlying technologies can apply to other operating systems, the examples I&#8217;ll use are from Linux.</p>\n<h3>TCP Congestion Control</h3>\n<p>To understand the challenges of mobile network performance at Layer 4 of the networking stack you need to understand TCP Congestion Control. TCP Congestion Control is the gatekeeper that determines how to control the flow of packets from your server to your clients. Its goal is to prevent Internet congestion by detecting when congestion occurs and slowing down the rate data is transmitted. This helps ensure that the Internet is available to everyone, but can cause problems on mobile network when TCP mistakes mobile network problems for congestion.</p>\n<p>TCP Congestion Control holds back the floodgates if it detects congestion (i.e. packet loss) on the remote end. A network is, inherently, a shared resource. The purpose of TCP Congestion Control was to ensure that every device on the network cooperates to not overwhelm its resource. On a wired network, if packet loss is detected it is a fairly reliable indicator that a port along the connection is overburdened. What is typically going on in these cases is that a memory buffer in a switch somewhere has filled beyond its capacity because packets are coming in faster than they can be sent out and data is being discarded. TCP Congestion Control on clients and servers is setup to &#8220;back off&#8221; in these cases in order to ensure that the network remains available for all its users.</p>\n<p>But figuring out what packet loss means on a mobile network is a different matter. Radio networks are inherently susceptible to interference which results in packet loss. If packets are being dropped does that mean a switch is overburdened, like we can infer on a wired network? Or did someone travel from an under-subscribed wireless cell to an oversubscribed one? Or did someone just turn on a microwave? Or maybe it was just a random solar flare? Since it&#8217;s not as clear what packet loss means on a mobile network, it&#8217;s not clear what action a TCP Congestion Control algorithm should take.</p>\n<h3>A Series of Leaky Tubes</h3>\n<p>To optimize networks for lossy networks like those on mobile networks, it&#8217;s important to understand exactly how TCP Congestion Control algorithms are designed. While the high level concept makes sense, the details of TCP Congestion Control are not widely understood by most people working in the web performance industry. That said, it is an important core part of what makes the Internet reliable and the subject of very active research and development.</p>\n<p>To understand how TCP Congestion Control algorithms work, imagine the following analogy. Think of your web server as your local water utility plant. You&#8217;ve built on a large network of pipes in your hometown and you need to guarantee that each pipe is as pressurized as possible for delivery, but you don&#8217;t want to burst the pipes. (Note: I recognize the late Senator Ted Stevens got a lot of flack for describing the Internet as a &#8220;series of tubes,&#8221; but the metaphor is surprisingly accurate.)</p>\n<p>Your client, Crazy Arty, runs a local water bottling plant that connects to your pipe network. Crazy Arty&#8217;s infrastructure is built on old pipes that are leaky and brittle. For you to get water to them without bursting his pipes, you need to infer the capability of Crazy Arty&#8217;s system. If you don&#8217;t know in advance then you do a test — you send a known amount of water to the line and then measure the pressure. If the pressure is suddenly lost then you can infer that you broke a pipe. If not, then that level is likely safe and you can add more water pressure and repeat the test. You can iterate this test until you burst a pipe, see the drop in pressure, write down the maximum water volume, and going forward ensure you never exceed it.</p>\n<p>Imagine, however, that there&#8217;s some exogenous factor that could decrease the pressure in the pipe without actually indicating a pipe had burst. What if, for example, Crazy Arty ran a pump that he only turned on randomly from time to time and you didn&#8217;t know about. If the only signal you have is observing a loss in pressure, you&#8217;d have no way of knowing whether you&#8217;d burst a pipe or if Crazy Arty had just plugged in the pump. The effect would be that you&#8217;d likely record a pressure level much less than the amount the pipes could actually withstand — leading to all your customers on the network potentially having lower water pressure than they should.</p>\n<h3>Optimizing for Congestion or Loss</h3>\n<p>If you&#8217;ve been following up to this point then you already know more about TCP Congestion Control than you would guess. The initial amount of water we talked about in TCP is known as the Initial Congestion Window (initcwnd) it is the initial number of packets in flight across the network. The congestion window (cwnd) either shrinks, grows, or stays the same depending on how many packets make it back and how fast (in ACK trains) they return after the initial burst. In essence, TCP Congestion Control is just like the water utility — measuring the pressure a network can withstand and then adjusting the volume in an attempt to maximize flow without bursting any pipes.</p>\n<p>When a TCP connection is first established it attempts to ramp up the cwnd quickly. This phase of the connection, where TCP grows the cwnd rapidly, is called Slow Start. That&#8217;s a bit of a misnomer since it is generally an exponential growth function which is quite fast and aggressive. Just like when the water utility in the example above detects a drop in pressure it turns down the volume of water, when TCP detects packets are lost it reduces the size of the cwnd and delays the time before another burst of packets is delivered. The time between packet bursts is known as the Retransmission Timeout (RTO). The algorithm within TCP that controls these processes is called the Congestion Control Algorithm. There are many congestion control algorithms and clients and servers can use different strategies based in the characteristics of their networks. Most of Congestion Control Algorithms focus on optimizing for one type of network loss or another: congestive loss (like you see on wired networks) or random loss (like you see on mobile networks).</p>\n<p>In the example above, a pipe bursting would be an indication of congestive loss. There was a physical limit to the pipes, it is exceeded, and the appropriate response is to back off. On the other hand, Crazy Arty&#8217;s pump is analogous to random loss. The capacity is still available on the network and only a temporary disturbance causes the water utility to see the pipes as overfull. The Internet started as a network of wired devices, and, as its name suggests, congestion control was largely designed to optimize for congestive loss. As a result, the default Congestion Control Algorithm in many operating systems is good for communicating wired networks but not as good for communicating with mobile networks.</p>\n<p>A few Congestion Control algorithms try to bridge the gap by using the time of the delay in the &#8220;pressure increase&#8221; to &#8220;expected capacity&#8221; to figure out the cause of the loss. These are known as bandwidth estimation algorithms, and examples include <a href=\"http://en.wikipedia.org/wiki/TCP_Vegas\">Vegas</a>, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.5736\">Veno</a> and <a href=\"http://en.wikipedia.org/wiki/TCP_Westwood_plus\">Westwood+</a>. Unfortunately, all of these methods are reactive and reuse no information across two similar streams.</p>\n<p>At companies that see a significant amount of network traffic, like CloudFlare or Google, it is possible to map the characteristics of the Internet&#8217;s networks and choose a specific congestion control algorithm in order to maximize performance for that network. Unfortunately, unless you are seeing the large amounts of traffic as we do and can record data on network performance, the ability to instrument your congestion control or build a &#8220;weather forecast&#8221; is usually impossible. Fortunately, there are still several things you can do to make your server more responsive to visitors even when they&#8217;re coming from lossy, mobile devices.</p>\n<h3>Compelling Reasons to Upgrade Your Kernel</h3>\n<p>The Linux network stack has been under extensive development to bring about some sensible defaults and mechanisms for dealing with the network topology of 2012. A mixed network of high bandwidth low latency and high bandwidth, high latency, lossy connections was never fully anticipated by the kernel developers of 2009 and if you check your server&#8217;s kernel version chances are its running a 2.6.32.x kernel from that era.</p>\n<p><code>uname -a</code></p>\n<p>There are a number of reasons that if you&#8217;re running an old kernel on your web server and want to increase web performance, especially for mobile devices, you should investigate upgrading. To begin, Linux 2.6.38 bumps the default initcwnd and initrwnd (inital receive window) from <a href=\"http://www.ietf.org/rfc/rfc3390.txt\">3 to 10</a>. This is an easy, big win. It allows for 14.2KB (vs 5.7KB) of data to be sent or received in the initial round trip before slow start grows the cwnd further. This is important for HTTP and SSL because it gives you more room to fit the header in the initial set of packets. If you are running an older kernel you may be able to run the following command on a bash shell (use caution) to set all of your routes&#8217; initcwnd and initrwnd to 10. On average, this small change can be one of the biggest boosts when you&#8217;re trying to maximize web performance.</p>\n<p><code>ip route | while read p; do ip route change $p initcwnd 10 initrwnd 10; done</code></p>\n<p>Linux kernel 3.2 implements <a href=\"http://tools.ietf.org/html/draft-mathis-tcpm-proportional-rate-reduction-01\">Proportional Rate Reduction (PRR)</a>. PRR decreases the time it takes for a lossy connection to recover its full speed, potentially improving HTTP response times by 3-10%. The benefits of PRR are significant for mobile networks. To understand why, it&#8217;s worth diving back into the details of how previous congestion control strategies interacted with loss.</p>\n<p>Many congestion control algorithms halve the cwnd when a loss is detected. When multiple losses occur this can result in a case where the cwnd is lower than the slow start threshold. Unfortunately, the connection never goes through slow start again. The result is that a few network interruptions can result in TCP slowing to a crawl for all the connections in the session.</p>\n<p>This is even more deadly when combined with tcp_no_metrics_save=0 sysctl setting on unpatched kernels before 3.2. This setting will save data on connections and attempt to use it to optimize the network. Unfortunately, this can actually make performance worse because TCP will apply the exception case to every new connection from a client within a window of a few minutes. In other words, in some cases, one person surfing your site from a mobile phone who has some random packet loss can reduce your server&#8217;s performance to this visitor even when their temporary loss has cleared.</p>\n<p>If you expect your visitors to be coming from mobile, lossy connections and you cannot upgrade or patch your kernel I recommend setting tcp_no_metrics_save=1. If you&#8217;re comfortable doing some hacking, you can <a href=\"http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commitdiff;h=a262f0cdf1f2916ea918dc329492abb5323d9a6c\">patch older kernels.</a></p>\n<p>The good news is that Linux 3.2 implements PRR, which decreases the amount of time that a lossy connection will impact TCP performance. If you can upgrade, it may be one of the most significant things you can do in order to increase your web performance.</p>\n<h3>More Improvements Ahead</h3>\n<p>Linux 3.2 also has another important improvement with RFC2099bis. The initial Retransmission Timeout (initRTO) has been changed to 1s from 3s. If loss happens after sending the initcwnd two seconds waiting time are saved when trying to resend the data. With TCP streams being so short this can have a very noticeable improvement if a connection experiences loss at the beginning of the stream. Like the PRR patch this can also be applied (with modification) to older kernels if for some reason you cannot upgrade (<a href=\"http://git.kernel.org/?p=linux/kernel/git/stable/linux-stable.git;a=commit;h=9ad7c049f0f79c418e293b1b68cf10d68f54fcdb\">here&#8217;s the patch</a>).</p>\n<p>Looking forward, Linux 3.3 has Byte Queue Limits when teamed with CoDel (controlled delay) in the 3.5 kernel helps fight the long standing issue of <a href=\"http://www.bufferbloat.net/projects/bloat/wiki/Introduction\">Bufferbloat</a> by intelligently managing packet queues. Bufferbloat is when the queuing overhead on the network stack becomes backed up because its littered with stale data. Linux 3.3 has features to auto QoS important packets (SYN/DNS/ARP/etc.,) keep down buffer queues thereby reducing bufferbloat and improving latency on loaded servers.</p>\n<p>Linux 3.5 implements <a href=\"http://tools.ietf.org/html/rfc5827\">TCP Early Retransmit</a> with some safeguards for connections that have a small amount of packet reordering. This allows connections, under certain conditions, to trigger fast retransmit and bypass the costly Retransmission Timeout (RTO) mentioned earlier. By default it is enabled in the failsafe mode tcp_early_retrans=2. If for some reason you are sure your clients have loss but no reordering then you could set tcp_early_retrans=1 to save one quarter a RTT on recovery.</p>\n<p>One of the most extensive changes to 3.6 that hasn&#8217;t got much press is the removal of the IPv4 routing cache. In a nutshell it was an extraneous caching layer in the kernel that mapped interfaces to routes to IPs and saved a lookup to the Forward Information Base (FIB). The FIB is a routing table within the network stack. The IPv4 routing cache was intended to eliminate a FIB lookup and increase performance. While a good idea in principle, unfortunately it provided a very small performance boost in less than 10% of connections. In the 3.2.x-3.5.x kernels it was extremely vulnerable to certain DDoS techniques so it has been removed.</p>\n<p>Finally, one important setting you should check, regardless of the Linux kernel you are running: tcp_slow_start_after_idle. If you&#8217;re concerned about web performance, it has been proclaimed sysctl setting of the year. It can be enabled in almost any kernel. By default this is set to 1 which will aggressively reduce cwnd on idle connections and negatively impact any long lived connections such as SSL. The following command will set it to 0 and can significantly improve performance:</p>\n<p><code>sysctl -w tcp_slow_start_after_idle=0</code></p>\n<h3>The Missing Congestion Control Algorithm</h3>\n<p>You may be curious as to why I haven&#8217;t made a recommendation as far as a quick and easy change of congestion control algorithms. Since Linux 2.6.19, the default congestion control algorithm in the Linux kernel is CUBIC, which is time based and optimized for high speed and high latency networks. Its killer feature, known as called Hybrid Slow Start (HyStart), allows it to safely exit slow start by measuring the ACK trains and not overshoot the cwnd. It can improve startup throughput by up to 200-300%.</p>\n<p>While other Congestion Control Algorithms may seem like performance wins on connections experiencing high amounts of loss (>.1%) (e.g., TCP Westwood+ or Hybla), unfortunately these algorithms don&#8217;t include HyStart. The net effect is that, in our tests, they under perform CUBIC for general network performance. Unless a majority of your clients are on lossy connections, I recommend staying with CUBIC.</p>\n<p>Of course the real answer here is to dynamically swap out congestion control algorithms based on historical data to better serve these edge cases. Unfortunately, that is difficult for the average web server unless you&#8217;re seeing a very high volume of traffic and are able to record and analyze network characteristics across multiple connections. The good news is that loss predictors and hybrid congestion control algorithms are continuing to mature, so maybe we will have an answer in an upcoming kernel.</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/optimizing-your-network-stack-for-optimal-mobile-web-performance/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "8"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      },
      {
         "title": "Giving Your Images An Extra Squeeze",
         "description": "<p>According to the latest HTTP archive stats, the average Web page weighs 1286KB, and 60% of that is image data. That means that properly compressing image data is of utmost importance for the overall page content size and hence its loading time. It also has a significant impact on the data plan hit users incur when they browse the Web on their mobile devices.</p>\n<p><img title=\"\" alt=\"Byte distribution per content type: Images 793KB, Scripts 211KB, Stylesheets 35KB, Flash 92KB, HTML 54KB, Other 101KB - Total 1286KB\" src=\"http://chart.apis.google.com/chart?chs=400x225&amp;cht=p&amp;chco=007099&amp;chd=t:54,793,211,35,92,101&amp;chds=0,793&amp;chdlp=b&amp;chdl=total%201286%20kB&amp;chl=HTML+-+54+kB%7CImages+-+793+kB%7CScripts+-+211+kB%7CStylesheets+-+35+kB%7CFlash+-+92+kB%7COther+-+101+kB&amp;chma=|5&amp;chtt=Average+Bytes+per+Page+by+Content+Type\" /></p>\n<p>Yet, when we look at the actual numbers &#8220;in the wild&#8221;, we see that few developers actually compress their images, and even for those that do, the results are not always ideal.</p>\n<p>A few months ago, I downloaded 5.8 million images from Alexa&#8217;s top 200,000 sites. Using that image data, I&#8217;ll demonstrate how much data can be saved by properly compressing images.</p>\n<h2>Image Formats</h2>\n<p>I&#8217;m sure most of you know this by now, but here is a short overview of the image formats on the Web:</p>\n<ul>\n<li><strong>GIF &#8211; </strong>Best suited for computer generated images with relatively few number of colors. It works by choosing a palette of up to 256 colors that best fits the image, creating a bitmap that represents the image using the palette&#8217;s color numbers, and then compressing that bitmap using a generic compression algorithm. The format supports animation and transparency, but not a full alpha channel.</li>\n<li><strong>PNG &#8211; </strong>Best suited for computer generated images, but can represent more than 256 colors. The format has several subtypes. The subtype usually referred to as PNG8 is very similar to GIF, but uses a different compression algorithm. It does not support animation, but does support a full alpha channel. The subtypes referred to as PNG24 and PNG24α can represent the full RGB color space, with the latter also supporting a full alpha channel. The downside is that both PNG24 subtypes are represented as bitmaps to which a generic compression algorithm is applied. This is usually not ideal in terms of byte size.</li>\n<li><strong>JPEG &#8211; </strong>Best suited for real life photos. It is not a bitmap based format, but represents the images by storing the frequency of color changes between different pixels, eliminating high frequencies that humans are likely not to notice anyway, and then compressing that. It is a <em>lossy</em> image format, which means a JPEG cannot be converted to the original bitmap image with perfect accuracy. For most uses on the Web, this is not a limitation.</li>\n<li><strong>WebP &#8211; </strong>Best suited for <em>both</em> real life photos and computer generated images, since it can employ both lossy and lossless techniques. Based on the VP8 video codec, the WebP format uses predictive coding to achieve its high lossy compression rates and the latest entropy coding techniques to achieve better lossless results. It also supports a full alpha channel and animation.What&#8217;s the catch, then? The main issue is that WebP is not really part of the Web platform&#8217;s &#8220;official&#8221; formats since it is only supported by Chrome and Opera at the present. The lack of simple fallback mechanisms (both <a href=\"https://www.w3.org/Bugs/Public/show_bug.cgi?id=20214\">client</a> and <a href=\"http://www.igvita.com/2012/12/18/deploying-new-image-formats-on-the-web/\">server</a> side) poses a high barrier of entry for developers that want to use WebP today.</li>\n</ul>\n<p>Here&#8217;s a look at the presence each format has on the Web today based on bytes.</p>\n<h2>Format Distribution</h2>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<th>Image format</th>\n<th>% in bytes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JPG</td>\n<td>66.9%</td>\n</tr>\n<tr>\n<td>Animated GIF</td>\n<td>6.4%</td>\n</tr>\n<tr>\n<td>Non-animated GIF</td>\n<td>5.3%</td>\n</tr>\n<tr>\n<td>PNG8</td>\n<td>1.3%</td>\n</tr>\n<tr>\n<td>PNG24</td>\n<td>5.2%</td>\n</tr>\n<tr>\n<td>PNG24α</td>\n<td>14.3%</td>\n</tr>\n<tr>\n<td>icons</td>\n<td>0.4%</td>\n</tr>\n<tr>\n<td>bitmaps</td>\n<td>0.2%</td>\n</tr>\n</tbody>\n</table>\n<p>Some of you may say: &#8220;You forgot SVG!&#8221;. I didn&#8217;t. SVG comprise only 0.001% of the overall image data, so it didn&#8217;t make it into the format distribution table. Sad, but true.</p>\n<h2>Lossless Optimization</h2>\n<p>In my quest for finding image optimization opportunities, I first sought to find the savings that could be achieved without any compromise on quality. I ran lossless optimizations on JPEG and PNG using the <code>jpegtran</code> and <code>pngcrush</code> utilities, as well as conversion to lossless WebP. The results are below.</p>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<td>Optimization</td>\n<td>Data Reduction</td>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JPEG EXIF removal</td>\n<td>6.6%</td>\n</tr>\n<tr>\n<td>JPEG EXIF removal, optimized Huffman</td>\n<td>13.3%</td>\n</tr>\n<tr>\n<td>JPEG EXIF removal, optimized Huffman, Convert to progressive</td>\n<td>15.1%</td>\n</tr>\n<tr>\n<td>PNG8 pngcrush</td>\n<td>2.6%</td>\n</tr>\n<tr>\n<td>PNG8 lossless WebP</td>\n<td>23%</td>\n</tr>\n<tr>\n<td>PNG24 pngcrush</td>\n<td>11%</td>\n</tr>\n<tr>\n<td>PNG24 lossless WebP</td>\n<td>33.1%</td>\n</tr>\n<tr>\n<td>PNG24α pngcrush</td>\n<td>14.4%</td>\n</tr>\n<tr>\n<td>PNG24α lossless WebP</td>\n<td>42.5%</td>\n</tr>\n</tbody>\n</table>\n<p>Overall with these lossless optimization techniques about 12.75% of image data can be saved. That is 101KB for an average page! If we use the lossless variant of WebP, we can save 18.2% of overall image data for browsers that support it, which is 144KB.</p>\n<h2>Lossy Optimization</h2>\n<p>Now let&#8217;s see what happens when we are willing to (slightly) compromise quality for the sake of data savings. I used the <a href=\"http://en.wikipedia.org/wiki/Structural_similarity\">SSIM</a> index in order to get an objective idea of the trade-off we make between quality and byte size. Basically, an SSIM score of 100% means identical images. Lower SSIM score means a bigger difference between the images.</p>\n<h3>JPEG</h3>\n<p>Using <code>ImageMagick</code> I compressed JPEGs several levels of quality. Then I applied the lossless optimizations that we saw above to them, in order to squeeze these images some more. I also compressed the images using <a href=\"https://github.com/rflynn/imgmin\">imgmin</a> which is a utility that deploys binary search to find the ideal quality level for each image. Finally, I ran JPEG to WebP conversion to see if the benefits match Google&#8217;s result of 30% data reduction.</p>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<th>Quality Level</th>\n<th>Data Reduction</th>\n<th>SSIM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>75</td>\n<td>50%</td>\n<td>96.22%</td>\n</tr>\n<tr>\n<td>50</td>\n<td>64.6%</td>\n<td>92.28%</td>\n</tr>\n<tr>\n<td>30</td>\n<td>73.3%</td>\n<td>89.13%</td>\n</tr>\n<tr>\n<td>imgmin</td>\n<td>38.6%</td>\n<td>97.52%</td>\n</tr>\n<tr>\n<td>WebP 75</td>\n<td>68%</td>\n<td>95.28%</td>\n</tr>\n</tbody>\n</table>\n<p>WebP gives us compression levels close to &#8220;quality 30&#8243; with &#8220;quality 75&#8243; image quality. Another way to look at this is that WebP files are 37% smaller than the size of JPEGs with equivalent quality.</p>\n<h3>PNG24</h3>\n<p>I tried several lossy optimizations on these images: <a href=\"https://twitter.com/pornelski\">Kornel Lesiński</a>&#8216;s <a href=\"https://github.com/pornel/improved-pngquant\">improved pngquant</a>, conversion to JPEG using ImageMagick+jpegtran and conversion to WebP.</p>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<th>Method</th>\n<th>Setting</th>\n<th>Data Reduction</th>\n<th>SSIM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>pngquant</td>\n<td>256</td>\n<td>57.1%</td>\n<td>99.8%</td>\n</tr>\n<tr>\n<td>pngquant + lossless WebP</td>\n<td>256</td>\n<td>63.2%</td>\n<td>99.8%</td>\n</tr>\n<tr>\n<td>JPEG</td>\n<td>75</td>\n<td>77%</td>\n<td>94.6%</td>\n</tr>\n<tr>\n<td>WebP</td>\n<td>75</td>\n<td>84.7%</td>\n<td>95.1%</td>\n</tr>\n</tbody>\n</table>\n<p>I&#8217;m not sure what&#8217;s more impressive here, pngquant&#8217;s 57.1% data reduction with practically zero quality loss, or JPEG&#8217;s and WebP&#8217;s results. Here again, the WebP files were 33% smaller than JPEG. Lossless WebP gave an extra 14.2% compression when applied to PNGs after pngquant. Note: I avoided converting PNGs smaller than 500 bytes to JPEG since this usually resulted in larger file sizes.</p>\n<h3>PNG24α</h3>\n<p>For PNGs with an alpha channel, I couldn&#8217;t use the above conversion to JPEG, since JPEG doesn&#8217;t have an alpha channel. Also, because of problems the original <a href=\"http://mehdi.rabah.free.fr/SSIM/\">SSIM</a> utility I used had with a full alpha channel, I used Kornel&#8217;s <a href=\"https://github.com/pornel/dssim\">dssim</a> utility instead.</p>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<th>Method</th>\n<th>Setting</th>\n<th>Data Reduction</th>\n<th>SSIM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>pngquant</td>\n<td>256</td>\n<td>63.1%</td>\n<td>99.8%</td>\n<td></td>\n</tr>\n<tr>\n<td>pngquant + lossless WebP</td>\n<td>256</td>\n<td>69%</td>\n<td>99.8%</td>\n</tr>\n<tr>\n<td>WebP</td>\n<td>75</td>\n<td>77.9%</td>\n<td>94.8%</td>\n</tr>\n</tbody>\n</table>\n<p>Again, pngquant&#8217;s results are extremely impressive, providing files that are almost 3 times smaller with negligible quality loss. Lossless WebP gave an extra 15.8% compression on these pngquant results. Lossy WebP provides even better compression results with files that are 40% smaller than pngquant and almost 5 time smaller than the original PNGs, although it does that with slight visual quality loss.</p>\n<h2>Why Don&#8217;t Developers Compress Their Images?</h2>\n<p>While I have no evidence to support that theory, I suspect most developers don&#8217;t compress their images since there is no automated process in place. Depending on the workflow, there are a few options to automate image compression:</p>\n<ul>\n<li><strong>Build time &#8211; </strong>For static images, adding image compression utilities to the build process can make sure that no static uncompressed images make it through.</li>\n<li><strong>Upload time &#8211; </strong>For images that are dynamically added by the site&#8217;s users or administrators, the developers should find a way to add image compression utilities to the upload process. That may not always be easy (e.g. when working with legacy CMSs), but it is essential to avoid serving bloated images to users.</li>\n<li><strong>Serving time &#8211; </strong>If neither of the previous options is feasible, there&#8217;s always the possibility to apply image compression before the images are served to the user. The open source option here is <a href=\"http://code.google.com/p/modpagespeed/\">mod page speed</a>&#8216;s <a href=\"https://developers.google.com/speed/docs/mod_pagespeed/filter-image-optimize\">image optimization filters</a>. Otherwise, commercial options are also available.</li>\n</ul>\n<p>Each developer should choose the optimization options that fit his workflow best, but <em>everyone</em> should automate image optimization, otherwise there&#8217;s a strong chance it will not happen.</p>\n<h2>Conclusions</h2>\n<p>Even though every Web developer knows that images must be properly compressed, very few actually do that optimally, as we can see from the extra compression we can squeeze out of the Web&#8217;s images, with no or little compromise in terms of quality.</p>\n<p>Even if developers only choose the truly lossless path, image data can be reduced by 12.75% or 100KB per page. Using lossless WebP turns that into 18.2% or 144KB for supporting browsers.</p>\n<p>If every Web developer would employ maximal lossy and lossless techniques to compress his site&#8217;s images to the maximal extent, with practically non existent visual impact (i.e. imgmin for JPEG, pngquant for PNG24), the current average page size image data could be reduced by 37.8% or 300KB!</p>\n<p>Willingness to apply more lossy techniques (but still maintain good visual quality), can result in 47.5% image data savings or 368KB.</p>\n<p>Using WebP would increase the savings to 61% of image data or 483KB for browsers that support it.</p>\n<p>That&#8217;s huge! Image compression is something that every one of us should add into our workflow, since it can save a large chunk of your site&#8217;s Web traffic. All the tools I used are free and open-source software. There are no excuses!</p>",
         "summary": "According to the latest HTTP archive stats, the average Web page weighs 1286KB, and 60% of that is image data. That means that properly compressing image data is of utmost importance for the overall page content size and hence its loading time. It also has a significant impact on the data plan hit users incur [...]",
         "date": "2012-12-29T22:53:04.000Z",
         "pubdate": "2012-12-29T22:53:04.000Z",
         "pubDate": "2012-12-29T22:53:04.000Z",
         "link": "http://calendar.perfplanet.com/2012/giving-your-images-an-extra-squeeze/",
         "guid": "http://calendar.perfplanet.com/?p=1560",
         "author": "stoyan",
         "comments": "http://calendar.perfplanet.com/2012/giving-your-images-an-extra-squeeze/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "images",
            "performance"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "Giving Your Images An Extra Squeeze"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/giving-your-images-an-extra-squeeze/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/giving-your-images-an-extra-squeeze/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Sat, 29 Dec 2012 22:53:04 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "stoyan"
         },
         "rss:category": [
            {
               "@": {},
               "#": "images"
            },
            {
               "@": {},
               "#": "performance"
            }
         ],
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1560"
         },
         "rss:description": {
            "@": {},
            "#": "According to the latest HTTP archive stats, the average Web page weighs 1286KB, and 60% of that is image data. That means that properly compressing image data is of utmost importance for the overall page content size and hence its loading time. It also has a significant impact on the data plan hit users incur [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<p>According to the latest HTTP archive stats, the average Web page weighs 1286KB, and 60% of that is image data. That means that properly compressing image data is of utmost importance for the overall page content size and hence its loading time. It also has a significant impact on the data plan hit users incur when they browse the Web on their mobile devices.</p>\n<p><img title=\"\" alt=\"Byte distribution per content type: Images 793KB, Scripts 211KB, Stylesheets 35KB, Flash 92KB, HTML 54KB, Other 101KB - Total 1286KB\" src=\"http://chart.apis.google.com/chart?chs=400x225&amp;cht=p&amp;chco=007099&amp;chd=t:54,793,211,35,92,101&amp;chds=0,793&amp;chdlp=b&amp;chdl=total%201286%20kB&amp;chl=HTML+-+54+kB%7CImages+-+793+kB%7CScripts+-+211+kB%7CStylesheets+-+35+kB%7CFlash+-+92+kB%7COther+-+101+kB&amp;chma=|5&amp;chtt=Average+Bytes+per+Page+by+Content+Type\" /></p>\n<p>Yet, when we look at the actual numbers &#8220;in the wild&#8221;, we see that few developers actually compress their images, and even for those that do, the results are not always ideal.</p>\n<p>A few months ago, I downloaded 5.8 million images from Alexa&#8217;s top 200,000 sites. Using that image data, I&#8217;ll demonstrate how much data can be saved by properly compressing images.</p>\n<h2>Image Formats</h2>\n<p>I&#8217;m sure most of you know this by now, but here is a short overview of the image formats on the Web:</p>\n<ul>\n<li><strong>GIF &#8211; </strong>Best suited for computer generated images with relatively few number of colors. It works by choosing a palette of up to 256 colors that best fits the image, creating a bitmap that represents the image using the palette&#8217;s color numbers, and then compressing that bitmap using a generic compression algorithm. The format supports animation and transparency, but not a full alpha channel.</li>\n<li><strong>PNG &#8211; </strong>Best suited for computer generated images, but can represent more than 256 colors. The format has several subtypes. The subtype usually referred to as PNG8 is very similar to GIF, but uses a different compression algorithm. It does not support animation, but does support a full alpha channel. The subtypes referred to as PNG24 and PNG24α can represent the full RGB color space, with the latter also supporting a full alpha channel. The downside is that both PNG24 subtypes are represented as bitmaps to which a generic compression algorithm is applied. This is usually not ideal in terms of byte size.</li>\n<li><strong>JPEG &#8211; </strong>Best suited for real life photos. It is not a bitmap based format, but represents the images by storing the frequency of color changes between different pixels, eliminating high frequencies that humans are likely not to notice anyway, and then compressing that. It is a <em>lossy</em> image format, which means a JPEG cannot be converted to the original bitmap image with perfect accuracy. For most uses on the Web, this is not a limitation.</li>\n<li><strong>WebP &#8211; </strong>Best suited for <em>both</em> real life photos and computer generated images, since it can employ both lossy and lossless techniques. Based on the VP8 video codec, the WebP format uses predictive coding to achieve its high lossy compression rates and the latest entropy coding techniques to achieve better lossless results. It also supports a full alpha channel and animation.What&#8217;s the catch, then? The main issue is that WebP is not really part of the Web platform&#8217;s &#8220;official&#8221; formats since it is only supported by Chrome and Opera at the present. The lack of simple fallback mechanisms (both <a href=\"https://www.w3.org/Bugs/Public/show_bug.cgi?id=20214\">client</a> and <a href=\"http://www.igvita.com/2012/12/18/deploying-new-image-formats-on-the-web/\">server</a> side) poses a high barrier of entry for developers that want to use WebP today.</li>\n</ul>\n<p>Here&#8217;s a look at the presence each format has on the Web today based on bytes.</p>\n<h2>Format Distribution</h2>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<th>Image format</th>\n<th>% in bytes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JPG</td>\n<td>66.9%</td>\n</tr>\n<tr>\n<td>Animated GIF</td>\n<td>6.4%</td>\n</tr>\n<tr>\n<td>Non-animated GIF</td>\n<td>5.3%</td>\n</tr>\n<tr>\n<td>PNG8</td>\n<td>1.3%</td>\n</tr>\n<tr>\n<td>PNG24</td>\n<td>5.2%</td>\n</tr>\n<tr>\n<td>PNG24α</td>\n<td>14.3%</td>\n</tr>\n<tr>\n<td>icons</td>\n<td>0.4%</td>\n</tr>\n<tr>\n<td>bitmaps</td>\n<td>0.2%</td>\n</tr>\n</tbody>\n</table>\n<p>Some of you may say: &#8220;You forgot SVG!&#8221;. I didn&#8217;t. SVG comprise only 0.001% of the overall image data, so it didn&#8217;t make it into the format distribution table. Sad, but true.</p>\n<h2>Lossless Optimization</h2>\n<p>In my quest for finding image optimization opportunities, I first sought to find the savings that could be achieved without any compromise on quality. I ran lossless optimizations on JPEG and PNG using the <code>jpegtran</code> and <code>pngcrush</code> utilities, as well as conversion to lossless WebP. The results are below.</p>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<td>Optimization</td>\n<td>Data Reduction</td>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JPEG EXIF removal</td>\n<td>6.6%</td>\n</tr>\n<tr>\n<td>JPEG EXIF removal, optimized Huffman</td>\n<td>13.3%</td>\n</tr>\n<tr>\n<td>JPEG EXIF removal, optimized Huffman, Convert to progressive</td>\n<td>15.1%</td>\n</tr>\n<tr>\n<td>PNG8 pngcrush</td>\n<td>2.6%</td>\n</tr>\n<tr>\n<td>PNG8 lossless WebP</td>\n<td>23%</td>\n</tr>\n<tr>\n<td>PNG24 pngcrush</td>\n<td>11%</td>\n</tr>\n<tr>\n<td>PNG24 lossless WebP</td>\n<td>33.1%</td>\n</tr>\n<tr>\n<td>PNG24α pngcrush</td>\n<td>14.4%</td>\n</tr>\n<tr>\n<td>PNG24α lossless WebP</td>\n<td>42.5%</td>\n</tr>\n</tbody>\n</table>\n<p>Overall with these lossless optimization techniques about 12.75% of image data can be saved. That is 101KB for an average page! If we use the lossless variant of WebP, we can save 18.2% of overall image data for browsers that support it, which is 144KB.</p>\n<h2>Lossy Optimization</h2>\n<p>Now let&#8217;s see what happens when we are willing to (slightly) compromise quality for the sake of data savings. I used the <a href=\"http://en.wikipedia.org/wiki/Structural_similarity\">SSIM</a> index in order to get an objective idea of the trade-off we make between quality and byte size. Basically, an SSIM score of 100% means identical images. Lower SSIM score means a bigger difference between the images.</p>\n<h3>JPEG</h3>\n<p>Using <code>ImageMagick</code> I compressed JPEGs several levels of quality. Then I applied the lossless optimizations that we saw above to them, in order to squeeze these images some more. I also compressed the images using <a href=\"https://github.com/rflynn/imgmin\">imgmin</a> which is a utility that deploys binary search to find the ideal quality level for each image. Finally, I ran JPEG to WebP conversion to see if the benefits match Google&#8217;s result of 30% data reduction.</p>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<th>Quality Level</th>\n<th>Data Reduction</th>\n<th>SSIM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>75</td>\n<td>50%</td>\n<td>96.22%</td>\n</tr>\n<tr>\n<td>50</td>\n<td>64.6%</td>\n<td>92.28%</td>\n</tr>\n<tr>\n<td>30</td>\n<td>73.3%</td>\n<td>89.13%</td>\n</tr>\n<tr>\n<td>imgmin</td>\n<td>38.6%</td>\n<td>97.52%</td>\n</tr>\n<tr>\n<td>WebP 75</td>\n<td>68%</td>\n<td>95.28%</td>\n</tr>\n</tbody>\n</table>\n<p>WebP gives us compression levels close to &#8220;quality 30&#8243; with &#8220;quality 75&#8243; image quality. Another way to look at this is that WebP files are 37% smaller than the size of JPEGs with equivalent quality.</p>\n<h3>PNG24</h3>\n<p>I tried several lossy optimizations on these images: <a href=\"https://twitter.com/pornelski\">Kornel Lesiński</a>&#8216;s <a href=\"https://github.com/pornel/improved-pngquant\">improved pngquant</a>, conversion to JPEG using ImageMagick+jpegtran and conversion to WebP.</p>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<th>Method</th>\n<th>Setting</th>\n<th>Data Reduction</th>\n<th>SSIM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>pngquant</td>\n<td>256</td>\n<td>57.1%</td>\n<td>99.8%</td>\n</tr>\n<tr>\n<td>pngquant + lossless WebP</td>\n<td>256</td>\n<td>63.2%</td>\n<td>99.8%</td>\n</tr>\n<tr>\n<td>JPEG</td>\n<td>75</td>\n<td>77%</td>\n<td>94.6%</td>\n</tr>\n<tr>\n<td>WebP</td>\n<td>75</td>\n<td>84.7%</td>\n<td>95.1%</td>\n</tr>\n</tbody>\n</table>\n<p>I&#8217;m not sure what&#8217;s more impressive here, pngquant&#8217;s 57.1% data reduction with practically zero quality loss, or JPEG&#8217;s and WebP&#8217;s results. Here again, the WebP files were 33% smaller than JPEG. Lossless WebP gave an extra 14.2% compression when applied to PNGs after pngquant. Note: I avoided converting PNGs smaller than 500 bytes to JPEG since this usually resulted in larger file sizes.</p>\n<h3>PNG24α</h3>\n<p>For PNGs with an alpha channel, I couldn&#8217;t use the above conversion to JPEG, since JPEG doesn&#8217;t have an alpha channel. Also, because of problems the original <a href=\"http://mehdi.rabah.free.fr/SSIM/\">SSIM</a> utility I used had with a full alpha channel, I used Kornel&#8217;s <a href=\"https://github.com/pornel/dssim\">dssim</a> utility instead.</p>\n<table>\n<thead>\n<tr class=\"imagemenot\">\n<th>Method</th>\n<th>Setting</th>\n<th>Data Reduction</th>\n<th>SSIM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>pngquant</td>\n<td>256</td>\n<td>63.1%</td>\n<td>99.8%</td>\n<td></td>\n</tr>\n<tr>\n<td>pngquant + lossless WebP</td>\n<td>256</td>\n<td>69%</td>\n<td>99.8%</td>\n</tr>\n<tr>\n<td>WebP</td>\n<td>75</td>\n<td>77.9%</td>\n<td>94.8%</td>\n</tr>\n</tbody>\n</table>\n<p>Again, pngquant&#8217;s results are extremely impressive, providing files that are almost 3 times smaller with negligible quality loss. Lossless WebP gave an extra 15.8% compression on these pngquant results. Lossy WebP provides even better compression results with files that are 40% smaller than pngquant and almost 5 time smaller than the original PNGs, although it does that with slight visual quality loss.</p>\n<h2>Why Don&#8217;t Developers Compress Their Images?</h2>\n<p>While I have no evidence to support that theory, I suspect most developers don&#8217;t compress their images since there is no automated process in place. Depending on the workflow, there are a few options to automate image compression:</p>\n<ul>\n<li><strong>Build time &#8211; </strong>For static images, adding image compression utilities to the build process can make sure that no static uncompressed images make it through.</li>\n<li><strong>Upload time &#8211; </strong>For images that are dynamically added by the site&#8217;s users or administrators, the developers should find a way to add image compression utilities to the upload process. That may not always be easy (e.g. when working with legacy CMSs), but it is essential to avoid serving bloated images to users.</li>\n<li><strong>Serving time &#8211; </strong>If neither of the previous options is feasible, there&#8217;s always the possibility to apply image compression before the images are served to the user. The open source option here is <a href=\"http://code.google.com/p/modpagespeed/\">mod page speed</a>&#8216;s <a href=\"https://developers.google.com/speed/docs/mod_pagespeed/filter-image-optimize\">image optimization filters</a>. Otherwise, commercial options are also available.</li>\n</ul>\n<p>Each developer should choose the optimization options that fit his workflow best, but <em>everyone</em> should automate image optimization, otherwise there&#8217;s a strong chance it will not happen.</p>\n<h2>Conclusions</h2>\n<p>Even though every Web developer knows that images must be properly compressed, very few actually do that optimally, as we can see from the extra compression we can squeeze out of the Web&#8217;s images, with no or little compromise in terms of quality.</p>\n<p>Even if developers only choose the truly lossless path, image data can be reduced by 12.75% or 100KB per page. Using lossless WebP turns that into 18.2% or 144KB for supporting browsers.</p>\n<p>If every Web developer would employ maximal lossy and lossless techniques to compress his site&#8217;s images to the maximal extent, with practically non existent visual impact (i.e. imgmin for JPEG, pngquant for PNG24), the current average page size image data could be reduced by 37.8% or 300KB!</p>\n<p>Willingness to apply more lossy techniques (but still maintain good visual quality), can result in 47.5% image data savings or 368KB.</p>\n<p>Using WebP would increase the savings to 61% of image data or 483KB for browsers that support it.</p>\n<p>That&#8217;s huge! Image compression is something that every one of us should add into our workflow, since it can save a large chunk of your site&#8217;s Web traffic. All the tools I used are free and open-source software. There are no excuses!</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/giving-your-images-an-extra-squeeze/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "12"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      },
      {
         "title": "Progressive jpegs: a new best practice",
         "description": "<style><!--\n.positiveCell {background-color: #c0e9b1;} .negativeCell {background-color: #fbbfbc;} .version {font-size: smaller;color: #666;}\n--></style>\n<p>Bandwidth-wise, images are hogs. They are the largest average web site payload (<a href=\"http://httparchive.org/interesting.php\">62%</a>), and they are most often the content bottleneck. When images arrive, they come tripping onto the page, pushing other elements around and triggering a clumsy repaint. They come “chop chop chop chop chop down” or you get nothing until suddenly “boom!” out of nowhere there it is. We all know what I’m talking about when I say “chop chop down” and “boom” and it makes us a little bit sick, because we sense how much time we’ve lost of our precious, short lives, waiting for pictures to download.</p>\n<h2>A missed opportunity</h2>\n<p>Photos are the main culprit when it comes to slow rendering. They are the <a href=\"http://httparchive.org/interesting.php\">most common type of image requested</a> and <a href=\"http://httparchive.org/interesting.php\">on average weigh more</a>. They are millions of colors and pixel depth is increasing. They are beautiful, and we don&#8217;t want to compromise on quality. </p>\n<p>Web-optimized photos are jpegs, and jpegs come in two flavors: baseline and progressive. A baseline jpeg is a full-resolution top-to-bottom scan of the image, and a progressive jpeg is a series of scans of increasing quality. And that&#8217;s how they render; baseline jpegs paint top to bottom (&#8220;chop chop chop&#8230;&#8221;), and progressive jpegs quickly stake out their territory and refine (or at least that&#8217;s the idea).</p>\n<p>Progressive jpegs are better because they are faster. Appearing faster is being faster, and <b>perceived speed is more important that actual speed</b>. Even if we are being greedy about what we are trying to deliver, progressive jpegs give us as much as possible as soon as possible. They assist us in our challenge of delivering big beautiful photos today.</p>\n<p>Experimenting locally with a throttled bandwidth, an 80K progressive jpeg beats a 5K baseline jpeg (the same image, downsized) to the page in Firefox on Windows. This should blow your mind. Sure, the progressive jpeg&#8217;s first pass is low-resolution, but it contains as much information, or more, as the small image. And if you are zoomed out, perhaps on a mobile device, you will not notice it&#8217;s low-res. That&#8217;s responsive images working for us right now!</p>\n<p><img alt=\"Progressive jpeg example\" src=\"http://annrobson.com/img/kickass-pjpeg.jpg\" /></p>\n<p>Basically, progressive jpegs are better. So what&#8217;s the most common type of jpeg online? You guessed it: <b>baseline</b>, and by a very wide margin. In a thousand-image sample, 92.6% are baseline.</p>\n<p>No worries, we just need to declare progressive jpegs a best practice and get the rest of the world on-board with us. But in order to declare progressive jpeg a best practice, we need to be confident that it is. And to do so we need to first understand what browser support for this type of jpeg looks like today.</p>\n<h2>Reality Check #1</h2>\n<p>Progressive jpegs are displayed in all browsers, that&#8217;s not a worry. Our concern is how they render.</p>\n<h3>Behavior of progressive jpegs across browsers</h3>\n<table>\n<tbody>\n<tr class=\"imagemenot\">\n<th><strong>Browser <span class=\"version\">(specific version tested)</span></strong></th>\n<th><strong>Foreground progressive jpeg renders</strong></th>\n<th><strong>Background progressive jpeg renders</strong></th>\n</tr>\n<tr>\n<td>Chrome <span class=\"version\">(v 25.0.1323.1 dev Mac, 23.0.1271.97 m Win)</span></td>\n<td class=\"positiveCell\">progressively (superfast!)</td>\n<td class=\"positiveCell\">progressively (superfast!)</td>\n</tr>\n<tr>\n<td>Firefox <span class=\"version\">(v 15.0.1 Mac, 12.0 Win)</span></td>\n<td class=\"positiveCell\">progressively (superfast!)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n<tr>\n<td>Internet Explorer 8</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n<tr>\n<td>Internet Explorer 9</td>\n<td class=\"positiveCell\">progressively (superfast!)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n<tr>\n<td>Safari <span class=\"version\">(v 6.0 Desktop, v 6.0 Mobile)</span></td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n<tr>\n<td>Opera <span class=\"version\">(v 11.60)</span></td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n</tbody>\n</table>\n<p>These are disappointing results, but overall, market share and progressive rendering for progressive jpegs are trending upward. Support is currently at about 65% (Chrome + Firefox + IE9).</p>\n<p>Unfortunately, the browsers that do not render progressive jpegs progressively render them all at once after download is complete, which makes them less progressive and slower than baseline jpegs. While baseline rendering is not as immediate and smooth as progressive rendering, at least it&#8217;s something while we wait, and the &#8220;chop chop&#8221; is a kind of progress indicator (a good thing). We can&#8217;t underestimate the reassurance we give users when they see something is happening.</p>\n<p>By choosing progressive jpegs we are giving a majority of users an excellent experience and a minority — but a significant minority — a worse experience. But if we select baseline jpegs because it is a less poor experience in a minority of views, that&#8217;s a terrible compromise. We need to offer the best experience to our users, and look ahead.</p>\n<h2>Reality Check #2</h2>\n<p>You might ask &#8220;Aren&#8217;t progressive jpegs bigger than regular jpegs? Don&#8217;t we pay for the &#8216;layers&#8217;?&#8221; This is true for other types of interlaced images, but not jpegs. A progressive jpeg is usually a few kilobytes smaller than its baseline version. Plotting the savings of <a href=\"http://www.bookofspeed.com/chapter5.html\">10000 random baseline jpegs converted to progressive</a>, Stoyan Stefanov discovered a valuable rule of thumb: files that are over 10K will generally be smaller using the progressive option.</p>\n<p>It would be an easier sell if we could say progressive jpegs are always smaller, so always make progressive jpegs. Stoyan helps us out here. He says &#8220;One observation about the 10K rule is that when baseline is smaller, it&#8217;s smaller by a small margin. When progressive is smaller it&#8217;s usually a lot smaller. So it&#8217;s ok to say go 100% progressive and you&#8217;ll do better.&#8221;</p>\n<p>That&#8217;s exactly what I wanted to hear! For all the baseline jpegs we&#8217;ve been serving, we&#8217;ve been missing opportunities in file size and perceived speed. Choosing the progressive option is win-win, and should always be the default. Then, after all jpegs are progressive, if we want to optimize further, it&#8217;s just a few bytes we&#8217;ll save and only on our smallest images.</p>\n<p>The reason baseline jpegs are most common online, no doubt, is because image-optimization tools make them by default. However, all of the ones I looked at &#8212; Photoshop, Fireworks, ImageMagick, jpegtran &#8212; have a progressive option. Therefore, to serve progressive jpegs you&#8217;ll need to consciously modify your image optimization process.</p>\n<p>I&#8217;d expect <a href=\"http://www.smushit.com/ysmush.it/\">Smushit</a> to translate baseline jpegs to progressive, and <a href=\"http://developer.yahoo.com/yslow/smushit/faq.html\">sure enough it does</a>. (Smushit, btw, can be run from the command line and integrated into your image optimization process.)</p>\n<p>How do you know if your jpegs are progressive? Here are a few ways to identify jpeg type:</p>\n<ol>\n<li><b>ImageMagick</b> — On the command line run: identify -verbose mystery.jpg | grep Interlace The output will either be &#8220;Interlace: JPEG&#8221; or &#8220;Interlace: None.&#8221;</li>\n<li><b>Photoshop</b> — Open file. Select File -&gt; Save for Web &amp; Devices. If it&#8217;s a progressive jpeg, the Progressive checkbox will be selected.</li>\n<li><b>Any browser</b> — Baselines jpegs will load top to bottom, and progressive jpegs will do something else. If the file loads too fast you may need to add bandwidth throttling. I use ipfw on my Mac.</li>\n</ol>\n<h3>Reality Check #3</h3>\n<p>According to <a href=\"http://www.faqs.org/faqs/jpeg-faq/part1/section-11.html\"> this progressive jpeg FAQ</a>, each progressive scan requires about the same amount of CPU as the entire baseline jpeg would take to render. This is not a concern for desktops but possibly for mobile devices.</p>\n<p>The extra computation is a disadvantage but not a deal breaker. Delivering photos on small hardware is a challenge regardless. I know this because I&#8217;m writing a photo gallery application with infinite scrolling and it crashes on iPad. If you are handling a lot of images, you will have challenges on mobile anyway &#8212; different challenges.</p>\n<p>As we&#8217;ve seen in the chart, Mobile Safari does not render progressive jpegs progressively anyway, and probably because they tax the CPU. But this is not a new image file format. Therefore, it wasn&#8217;t an option for browsers, even mobile browsers, to choose not to support progressive jpegs. Hopefully soon mobile browsers will leverage progressive rendering, but it makes sense why they currently don&#8217;t. It&#8217;s also a crying shame; we could really use the speed and file size savings progressive jpegs give us for mobile. When I said they are a kind of solution for responsive images right now, well, they would be, but aren&#8217;t yet.</p>\n<h2>Onward</h2>\n<p>In the last few days, Google got on-board with their <a href=\"https://code.google.com/p/modpagespeed/\">Mod_Pagespeed</a> service, making convert_jpeg_to_progressive a <a href=\"http://googledevelopers.blogspot.com/2012/12/new-modpagespeed-cache-advances.html\">core filter</a>. <a href=\"http://www.chromium.org/spdy/spdy-whitepaper\">SPDY</a> does as well, <a href=\"https://developers.google.com/speed/docs/mod_pagespeed/filter-image-optimize#progressive\">translating jpegs that are over 10K to progressive by default</a>, following Stoyan&#8217;s rule of thumb. This will make browsers that support incremental display seem much faster. As you can see in the chart above that includes Google Chrome, so it makes sense that Google would make this choice. I&#8217;m not going to say that because &#8220;do-no-evil-make-the-web-faster&#8221; Google has selected progressive jpeg to be a best practice so should we. But it&#8217;s more data and validation. Most importantly, it shows that progressive jpeg &#8212; a format that has been in a kind of deep freeze for a decade &#8212; has staged a comeback.</p>\n<p>And even though not all current browsers make use of progressive jpeg&#8217;s progressive rendering, the ones that do really benefit, and we get file size savings across the board. It&#8217;s our best option today and we should use it. Progressive jpegs are the future, not the past.</p>",
         "summary": "Bandwidth-wise, images are hogs. They are the largest average web site payload (62%), and they are most often the content bottleneck. When images arrive, they come tripping onto the page, pushing other elements around and triggering a clumsy repaint. They come “chop chop chop chop chop down” or you get nothing until suddenly “boom!” out [...]",
         "date": "2012-12-28T23:42:25.000Z",
         "pubdate": "2012-12-28T23:42:25.000Z",
         "pubDate": "2012-12-28T23:42:25.000Z",
         "link": "http://calendar.perfplanet.com/2012/progressive-jpegs-a-new-best-practice/",
         "guid": "http://calendar.perfplanet.com/?p=1557",
         "author": "stoyan",
         "comments": "http://calendar.perfplanet.com/2012/progressive-jpegs-a-new-best-practice/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "performance"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "Progressive jpegs: a new best practice"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/progressive-jpegs-a-new-best-practice/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/progressive-jpegs-a-new-best-practice/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Fri, 28 Dec 2012 23:42:25 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "stoyan"
         },
         "rss:category": {
            "@": {},
            "#": "performance"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1557"
         },
         "rss:description": {
            "@": {},
            "#": "Bandwidth-wise, images are hogs. They are the largest average web site payload (62%), and they are most often the content bottleneck. When images arrive, they come tripping onto the page, pushing other elements around and triggering a clumsy repaint. They come “chop chop chop chop chop down” or you get nothing until suddenly “boom!” out [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<style><!--\n.positiveCell {background-color: #c0e9b1;} .negativeCell {background-color: #fbbfbc;} .version {font-size: smaller;color: #666;}\n--></style>\n<p>Bandwidth-wise, images are hogs. They are the largest average web site payload (<a href=\"http://httparchive.org/interesting.php\">62%</a>), and they are most often the content bottleneck. When images arrive, they come tripping onto the page, pushing other elements around and triggering a clumsy repaint. They come “chop chop chop chop chop down” or you get nothing until suddenly “boom!” out of nowhere there it is. We all know what I’m talking about when I say “chop chop down” and “boom” and it makes us a little bit sick, because we sense how much time we’ve lost of our precious, short lives, waiting for pictures to download.</p>\n<h2>A missed opportunity</h2>\n<p>Photos are the main culprit when it comes to slow rendering. They are the <a href=\"http://httparchive.org/interesting.php\">most common type of image requested</a> and <a href=\"http://httparchive.org/interesting.php\">on average weigh more</a>. They are millions of colors and pixel depth is increasing. They are beautiful, and we don&#8217;t want to compromise on quality. </p>\n<p>Web-optimized photos are jpegs, and jpegs come in two flavors: baseline and progressive. A baseline jpeg is a full-resolution top-to-bottom scan of the image, and a progressive jpeg is a series of scans of increasing quality. And that&#8217;s how they render; baseline jpegs paint top to bottom (&#8220;chop chop chop&#8230;&#8221;), and progressive jpegs quickly stake out their territory and refine (or at least that&#8217;s the idea).</p>\n<p>Progressive jpegs are better because they are faster. Appearing faster is being faster, and <b>perceived speed is more important that actual speed</b>. Even if we are being greedy about what we are trying to deliver, progressive jpegs give us as much as possible as soon as possible. They assist us in our challenge of delivering big beautiful photos today.</p>\n<p>Experimenting locally with a throttled bandwidth, an 80K progressive jpeg beats a 5K baseline jpeg (the same image, downsized) to the page in Firefox on Windows. This should blow your mind. Sure, the progressive jpeg&#8217;s first pass is low-resolution, but it contains as much information, or more, as the small image. And if you are zoomed out, perhaps on a mobile device, you will not notice it&#8217;s low-res. That&#8217;s responsive images working for us right now!</p>\n<p><img alt=\"Progressive jpeg example\" src=\"http://annrobson.com/img/kickass-pjpeg.jpg\" /></p>\n<p>Basically, progressive jpegs are better. So what&#8217;s the most common type of jpeg online? You guessed it: <b>baseline</b>, and by a very wide margin. In a thousand-image sample, 92.6% are baseline.</p>\n<p>No worries, we just need to declare progressive jpegs a best practice and get the rest of the world on-board with us. But in order to declare progressive jpeg a best practice, we need to be confident that it is. And to do so we need to first understand what browser support for this type of jpeg looks like today.</p>\n<h2>Reality Check #1</h2>\n<p>Progressive jpegs are displayed in all browsers, that&#8217;s not a worry. Our concern is how they render.</p>\n<h3>Behavior of progressive jpegs across browsers</h3>\n<table>\n<tbody>\n<tr class=\"imagemenot\">\n<th><strong>Browser <span class=\"version\">(specific version tested)</span></strong></th>\n<th><strong>Foreground progressive jpeg renders</strong></th>\n<th><strong>Background progressive jpeg renders</strong></th>\n</tr>\n<tr>\n<td>Chrome <span class=\"version\">(v 25.0.1323.1 dev Mac, 23.0.1271.97 m Win)</span></td>\n<td class=\"positiveCell\">progressively (superfast!)</td>\n<td class=\"positiveCell\">progressively (superfast!)</td>\n</tr>\n<tr>\n<td>Firefox <span class=\"version\">(v 15.0.1 Mac, 12.0 Win)</span></td>\n<td class=\"positiveCell\">progressively (superfast!)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n<tr>\n<td>Internet Explorer 8</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n<tr>\n<td>Internet Explorer 9</td>\n<td class=\"positiveCell\">progressively (superfast!)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n<tr>\n<td>Safari <span class=\"version\">(v 6.0 Desktop, v 6.0 Mobile)</span></td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n<tr>\n<td>Opera <span class=\"version\">(v 11.60)</span></td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n<td class=\"negativeCell\">instantly after file download (slow)</td>\n</tr>\n</tbody>\n</table>\n<p>These are disappointing results, but overall, market share and progressive rendering for progressive jpegs are trending upward. Support is currently at about 65% (Chrome + Firefox + IE9).</p>\n<p>Unfortunately, the browsers that do not render progressive jpegs progressively render them all at once after download is complete, which makes them less progressive and slower than baseline jpegs. While baseline rendering is not as immediate and smooth as progressive rendering, at least it&#8217;s something while we wait, and the &#8220;chop chop&#8221; is a kind of progress indicator (a good thing). We can&#8217;t underestimate the reassurance we give users when they see something is happening.</p>\n<p>By choosing progressive jpegs we are giving a majority of users an excellent experience and a minority — but a significant minority — a worse experience. But if we select baseline jpegs because it is a less poor experience in a minority of views, that&#8217;s a terrible compromise. We need to offer the best experience to our users, and look ahead.</p>\n<h2>Reality Check #2</h2>\n<p>You might ask &#8220;Aren&#8217;t progressive jpegs bigger than regular jpegs? Don&#8217;t we pay for the &#8216;layers&#8217;?&#8221; This is true for other types of interlaced images, but not jpegs. A progressive jpeg is usually a few kilobytes smaller than its baseline version. Plotting the savings of <a href=\"http://www.bookofspeed.com/chapter5.html\">10000 random baseline jpegs converted to progressive</a>, Stoyan Stefanov discovered a valuable rule of thumb: files that are over 10K will generally be smaller using the progressive option.</p>\n<p>It would be an easier sell if we could say progressive jpegs are always smaller, so always make progressive jpegs. Stoyan helps us out here. He says &#8220;One observation about the 10K rule is that when baseline is smaller, it&#8217;s smaller by a small margin. When progressive is smaller it&#8217;s usually a lot smaller. So it&#8217;s ok to say go 100% progressive and you&#8217;ll do better.&#8221;</p>\n<p>That&#8217;s exactly what I wanted to hear! For all the baseline jpegs we&#8217;ve been serving, we&#8217;ve been missing opportunities in file size and perceived speed. Choosing the progressive option is win-win, and should always be the default. Then, after all jpegs are progressive, if we want to optimize further, it&#8217;s just a few bytes we&#8217;ll save and only on our smallest images.</p>\n<p>The reason baseline jpegs are most common online, no doubt, is because image-optimization tools make them by default. However, all of the ones I looked at &#8212; Photoshop, Fireworks, ImageMagick, jpegtran &#8212; have a progressive option. Therefore, to serve progressive jpegs you&#8217;ll need to consciously modify your image optimization process.</p>\n<p>I&#8217;d expect <a href=\"http://www.smushit.com/ysmush.it/\">Smushit</a> to translate baseline jpegs to progressive, and <a href=\"http://developer.yahoo.com/yslow/smushit/faq.html\">sure enough it does</a>. (Smushit, btw, can be run from the command line and integrated into your image optimization process.)</p>\n<p>How do you know if your jpegs are progressive? Here are a few ways to identify jpeg type:</p>\n<ol>\n<li><b>ImageMagick</b> — On the command line run: identify -verbose mystery.jpg | grep Interlace The output will either be &#8220;Interlace: JPEG&#8221; or &#8220;Interlace: None.&#8221;</li>\n<li><b>Photoshop</b> — Open file. Select File -&gt; Save for Web &amp; Devices. If it&#8217;s a progressive jpeg, the Progressive checkbox will be selected.</li>\n<li><b>Any browser</b> — Baselines jpegs will load top to bottom, and progressive jpegs will do something else. If the file loads too fast you may need to add bandwidth throttling. I use ipfw on my Mac.</li>\n</ol>\n<h3>Reality Check #3</h3>\n<p>According to <a href=\"http://www.faqs.org/faqs/jpeg-faq/part1/section-11.html\"> this progressive jpeg FAQ</a>, each progressive scan requires about the same amount of CPU as the entire baseline jpeg would take to render. This is not a concern for desktops but possibly for mobile devices.</p>\n<p>The extra computation is a disadvantage but not a deal breaker. Delivering photos on small hardware is a challenge regardless. I know this because I&#8217;m writing a photo gallery application with infinite scrolling and it crashes on iPad. If you are handling a lot of images, you will have challenges on mobile anyway &#8212; different challenges.</p>\n<p>As we&#8217;ve seen in the chart, Mobile Safari does not render progressive jpegs progressively anyway, and probably because they tax the CPU. But this is not a new image file format. Therefore, it wasn&#8217;t an option for browsers, even mobile browsers, to choose not to support progressive jpegs. Hopefully soon mobile browsers will leverage progressive rendering, but it makes sense why they currently don&#8217;t. It&#8217;s also a crying shame; we could really use the speed and file size savings progressive jpegs give us for mobile. When I said they are a kind of solution for responsive images right now, well, they would be, but aren&#8217;t yet.</p>\n<h2>Onward</h2>\n<p>In the last few days, Google got on-board with their <a href=\"https://code.google.com/p/modpagespeed/\">Mod_Pagespeed</a> service, making convert_jpeg_to_progressive a <a href=\"http://googledevelopers.blogspot.com/2012/12/new-modpagespeed-cache-advances.html\">core filter</a>. <a href=\"http://www.chromium.org/spdy/spdy-whitepaper\">SPDY</a> does as well, <a href=\"https://developers.google.com/speed/docs/mod_pagespeed/filter-image-optimize#progressive\">translating jpegs that are over 10K to progressive by default</a>, following Stoyan&#8217;s rule of thumb. This will make browsers that support incremental display seem much faster. As you can see in the chart above that includes Google Chrome, so it makes sense that Google would make this choice. I&#8217;m not going to say that because &#8220;do-no-evil-make-the-web-faster&#8221; Google has selected progressive jpeg to be a best practice so should we. But it&#8217;s more data and validation. Most importantly, it shows that progressive jpeg &#8212; a format that has been in a kind of deep freeze for a decade &#8212; has staged a comeback.</p>\n<p>And even though not all current browsers make use of progressive jpeg&#8217;s progressive rendering, the ones that do really benefit, and we get file size savings across the board. It&#8217;s our best option today and we should use it. Progressive jpegs are the future, not the past.</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/progressive-jpegs-a-new-best-practice/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "101"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      },
      {
         "title": "SPOFCheck – Fighting Frontend SPOF at its root",
         "description": "<p>With the increase in 3rd party widgets and modernization of web applications, Frontend Single Point Of Failure (<a href=\"http://en.wikipedia.org/wiki/Single_point_of_failure\">SPOF</a>) has become a critical focus point. Thanks to <a href=\"https://twitter.com/souders\">Steve Souders</a> for his <a href=\"http://www.stevesouders.com/blog/2010/06/01/frontend-spof/\">initial research</a> on this topic, we now have a list of common patterns which causes SPOF. The awareness of Frontend SPOF has also increased tremendously among engineers, thanks to some of the recent blogs and <a href=\"http://calendar.perfplanet.com/2012/spof-bug/\">articles</a> emphasizing the importance of it.</p>\n<p>There are already a bunch of utilities and plugins out there which can detect possible SPOF vulnerabilities in a web application. The most notable ones being <a href=\"http://blog.patrickmeenan.com/2011/10/testing-for-frontend-spof.html\">webpagetest.org</a>, <a href=\"https://chrome.google.com/webstore/detail/spof-o-matic/plikhggfbplemddobondkeogomgoodeg?hl=en-US\">SPOF-O-Matic</a> chrome plugin and <a href=\"http://www.phpied.com/3po/\">YSlow 3PO</a> extension. At eBay we wanted to detect SPOF at a very early stage, during the development cycle itself. This means an additional hook in our automated testing pipeline. The solution resulted in creating a simple tool which works on our test URLs and produces SPOF alerts. The tool is <strong><a href=\"http://senthilp.github.com/spofcheck/\">SPOFCheck</a></strong>.</p>\n<p>SPOFCheck is a <a href=\"http://en.wikipedia.org/wiki/Command-line_interface\">Command Line Interface</a> (CLI) built in Node.js to detect possible Frontend SPOF for web pages. The output is generated in an XML format that can be consumed and reported by <a href=\"http://en.wikipedia.org/wiki/Continuous_integration\">CI</a> jobs. The tool is integrated with our secondary jobs, which run daily automation on a testing server where a development branch is deployed. In case of a SPOF alert, engineers are notified and they act on it accordingly. This process ensures that SPOFs are contained within the development cycle and do not sneak into staging or production.</p>\n<h2>The command line interface</h2>\n<p>SPOFCheck provides a simple command line interface and runs on Node.js</p>\n<p>To install SPOFCheck run the following</p>\n<pre>$ npm install -g spofcheck</pre>\n<p>To run SPOFCheck, use the following format</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-identifier\">spofcheck</span><span class=\"hl-brackets\">[</span><span class=\"hl-var\">options</span><span class=\"hl-brackets\">]</span><span class=\"hl-identifier\">*</span><span class=\"hl-brackets\">[</span><span class=\"hl-var\">urls</span><span class=\"hl-brackets\">]</span><span class=\"hl-identifier\">*</span><span class=\"hl-identifier\">Options</span><span class=\"hl-code\">\r\n--</span><span class=\"hl-identifier\">help</span><span class=\"hl-code\"> | -</span><span class=\"hl-identifier\">h</span><span class=\"hl-identifier\">Displays</span><span class=\"hl-identifier\">this</span><span class=\"hl-identifier\">information</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">format</span><span class=\"hl-code\">=&lt;</span><span class=\"hl-identifier\">format</span><span class=\"hl-code\">&gt; | -</span><span class=\"hl-identifier\">f</span><span class=\"hl-code\"> &lt;</span><span class=\"hl-identifier\">format</span><span class=\"hl-code\">&gt;             </span><span class=\"hl-identifier\">Indicate</span><span class=\"hl-identifier\">which</span><span class=\"hl-identifier\">format</span><span class=\"hl-brackets\">[</span><span class=\"hl-var\">junit-xml</span><span class=\"hl-code\"> | </span><span class=\"hl-var\">spof-xml</span><span class=\"hl-code\"> | </span><span class=\"hl-var\">text</span><span class=\"hl-brackets\">]</span><span class=\"hl-identifier\">to</span><span class=\"hl-identifier\">use</span><span class=\"hl-identifier\">for</span><span class=\"hl-identifier\">output</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">outputdir</span><span class=\"hl-code\">=&lt;</span><span class=\"hl-identifier\">dir</span><span class=\"hl-code\">&gt; | -</span><span class=\"hl-identifier\">o</span><span class=\"hl-code\"> &lt;</span><span class=\"hl-identifier\">dir</span><span class=\"hl-code\">&gt;                </span><span class=\"hl-identifier\">Outputs</span><span class=\"hl-identifier\">the</span><span class=\"hl-identifier\">spof</span><span class=\"hl-identifier\">results</span><span class=\"hl-identifier\">to</span><span class=\"hl-identifier\">this</span><span class=\"hl-identifier\">directory</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">rules</span><span class=\"hl-code\">=&lt;</span><span class=\"hl-identifier\">rule</span><span class=\"hl-brackets\">[</span><span class=\"hl-code\">,</span><span class=\"hl-var\">rule</span><span class=\"hl-brackets\">]</span><span class=\"hl-code\">+&gt; | -</span><span class=\"hl-identifier\">r</span><span class=\"hl-code\"> &lt;</span><span class=\"hl-identifier\">rule</span><span class=\"hl-brackets\">[</span><span class=\"hl-code\">,</span><span class=\"hl-var\">rule</span><span class=\"hl-brackets\">]</span><span class=\"hl-code\">+&gt;  </span><span class=\"hl-identifier\">Indicate</span><span class=\"hl-identifier\">which</span><span class=\"hl-identifier\">rules</span><span class=\"hl-identifier\">to</span><span class=\"hl-identifier\">include</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">print</span><span class=\"hl-code\"> | -</span><span class=\"hl-identifier\">p</span><span class=\"hl-identifier\">Outputs</span><span class=\"hl-identifier\">the</span><span class=\"hl-identifier\">results</span><span class=\"hl-identifier\">in</span><span class=\"hl-identifier\">console</span><span class=\"hl-code\">,\r\n                                              </span><span class=\"hl-identifier\">instead</span><span class=\"hl-identifier\">of</span><span class=\"hl-identifier\">saving</span><span class=\"hl-identifier\">to</span><span class=\"hl-identifier\">a</span><span class=\"hl-identifier\">file</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">quiet</span><span class=\"hl-code\"> | -</span><span class=\"hl-identifier\">q</span><span class=\"hl-identifier\">Keeps</span><span class=\"hl-identifier\">the</span><span class=\"hl-identifier\">console</span><span class=\"hl-identifier\">clear</span><span class=\"hl-identifier\">from</span><span class=\"hl-identifier\">logging</span><span class=\"hl-code\">.</span></pre>\n</div>\n<p>Example</p>\n<pre>$ spofcheck -f junit-xml -o /tests www.ebay.com www.amazon.com</pre>\n<h2>Rules</h2>\n<p>SPOFCheck by default runs with 5 rules (checks). The rules are maintained in the <a href=\"https://github.com/senthilp/spofcheck/blob/master/lib/rules.js\">rules.js</a> file. New rules are easily added by pushing entries to the <a href=\"https://github.com/senthilp/spofcheck/blob/master/lib/rules.js#L6\">rules</a> array or calling the spof API <a href=\"https://github.com/senthilp/spofcheck/blob/master/lib/engine.js#L142\">registerRules</a>. The default rules come from Souders’s original <a href=\"http://www.stevesouders.com/blog/2010/06/01/frontend-spof/\">list</a> outlined below.</p>\n<ol>\n<li><code>3rdparty-scripts</code> &#8211; Always load 3rd party external scripts asyncronously in a non-blocking pattern</li>\n<li><code>application-js</code> &#8211; Load application JS in a non-blocking pattern or towards the end of page</li>\n<li><code>fontface-stylesheet</code> &#8211; Try to inline @font-face style. Also make the font files compressed and cacheable</li>\n<li><code>fontface-inline</code> &#8211; Make sure the fonts files are compressed, cached and small in size</li>\n<li><code>fontface-inline-precede-script-IE</code> &#8211; Make sure inlined @font-face is not preceded by a SCRIPT tag, causes SPOF in IE</li>\n</ol>\n<h2 id=\"output\">Output</h2>\n<p>SPOFCheck creates a file and writes results in one of these formats:</p>\n<ul>\n<li><code>junit-xml</code> &#8211; a format most CI servers can parse (the default format)</li>\n<li><code>spof-xml</code> &#8211; an XML format that can be consumed by other utilities</li>\n<li><code>text</code> &#8211; a textual representation of the results</li>\n</ul>\n<p>The format can be specified using the <code>--format</code> or <code>-f</code> option. For just printing results, i.e. no file creation, use the <code>--print</code> or <code>-p</code> option.</p>\n<h2 id=\"a_spof_free_world\">A SPOF Free World</h2>\n<p>Our primary goal was to eradicate frontend SPOF before it creeps in, and SPOFCheck is a step towards it. Go ahead, give <a href=\"http://senthilp.github.com/spofcheck/\">SPOFCheck</a> a try and integrate it with your CI environments. Let’s build a SPOF Free World <img alt=\":-)\" src=\"http://calendar.perfplanet.com/wp-includes/images/smilies/icon_smile.gif\" />.</p>\n<p>Thanks to github projects <a href=\"https://github.com/pmeenan/spof-o-matic\">spof-o-matic</a> and <a href=\"https://github.com/stoyan/yslow\">3po</a>, a lot of the code logic has been re-used here. The design and packaging of the tool is based on <a href=\"https://github.com/stubbornella/csslint\">csslint</a>, thanks to <a href=\"https://twitter.com/slicknet\">Nicholas Zakas</a> and <a href=\"https://twitter.com/stubbornella\">Nicole Sullivan</a>.</p>",
         "summary": "With the increase in 3rd party widgets and modernization of web applications, Frontend Single Point Of Failure (SPOF) has become a critical focus point. Thanks to Steve Souders for his initial research on this topic, we now have a list of common patterns which causes SPOF. The awareness of Frontend SPOF has also increased tremendously [...]",
         "date": "2012-12-27T19:21:02.000Z",
         "pubdate": "2012-12-27T19:21:02.000Z",
         "pubDate": "2012-12-27T19:21:02.000Z",
         "link": "http://calendar.perfplanet.com/2012/spofcheck-fighting-frontend-spof-at-its-root/",
         "guid": "http://calendar.perfplanet.com/?p=1553",
         "author": "stoyan",
         "comments": "http://calendar.perfplanet.com/2012/spofcheck-fighting-frontend-spof-at-its-root/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "performance",
            "tools"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "SPOFCheck – Fighting Frontend SPOF at its root"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/spofcheck-fighting-frontend-spof-at-its-root/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/spofcheck-fighting-frontend-spof-at-its-root/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Thu, 27 Dec 2012 19:21:02 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "stoyan"
         },
         "rss:category": [
            {
               "@": {},
               "#": "performance"
            },
            {
               "@": {},
               "#": "tools"
            }
         ],
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1553"
         },
         "rss:description": {
            "@": {},
            "#": "With the increase in 3rd party widgets and modernization of web applications, Frontend Single Point Of Failure (SPOF) has become a critical focus point. Thanks to Steve Souders for his initial research on this topic, we now have a list of common patterns which causes SPOF. The awareness of Frontend SPOF has also increased tremendously [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<p>With the increase in 3rd party widgets and modernization of web applications, Frontend Single Point Of Failure (<a href=\"http://en.wikipedia.org/wiki/Single_point_of_failure\">SPOF</a>) has become a critical focus point. Thanks to <a href=\"https://twitter.com/souders\">Steve Souders</a> for his <a href=\"http://www.stevesouders.com/blog/2010/06/01/frontend-spof/\">initial research</a> on this topic, we now have a list of common patterns which causes SPOF. The awareness of Frontend SPOF has also increased tremendously among engineers, thanks to some of the recent blogs and <a href=\"http://calendar.perfplanet.com/2012/spof-bug/\">articles</a> emphasizing the importance of it.</p>\n<p>There are already a bunch of utilities and plugins out there which can detect possible SPOF vulnerabilities in a web application. The most notable ones being <a href=\"http://blog.patrickmeenan.com/2011/10/testing-for-frontend-spof.html\">webpagetest.org</a>, <a href=\"https://chrome.google.com/webstore/detail/spof-o-matic/plikhggfbplemddobondkeogomgoodeg?hl=en-US\">SPOF-O-Matic</a> chrome plugin and <a href=\"http://www.phpied.com/3po/\">YSlow 3PO</a> extension. At eBay we wanted to detect SPOF at a very early stage, during the development cycle itself. This means an additional hook in our automated testing pipeline. The solution resulted in creating a simple tool which works on our test URLs and produces SPOF alerts. The tool is <strong><a href=\"http://senthilp.github.com/spofcheck/\">SPOFCheck</a></strong>.</p>\n<p>SPOFCheck is a <a href=\"http://en.wikipedia.org/wiki/Command-line_interface\">Command Line Interface</a> (CLI) built in Node.js to detect possible Frontend SPOF for web pages. The output is generated in an XML format that can be consumed and reported by <a href=\"http://en.wikipedia.org/wiki/Continuous_integration\">CI</a> jobs. The tool is integrated with our secondary jobs, which run daily automation on a testing server where a development branch is deployed. In case of a SPOF alert, engineers are notified and they act on it accordingly. This process ensures that SPOFs are contained within the development cycle and do not sneak into staging or production.</p>\n<h2>The command line interface</h2>\n<p>SPOFCheck provides a simple command line interface and runs on Node.js</p>\n<p>To install SPOFCheck run the following</p>\n<pre>$ npm install -g spofcheck</pre>\n<p>To run SPOFCheck, use the following format</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-identifier\">spofcheck</span><span class=\"hl-brackets\">[</span><span class=\"hl-var\">options</span><span class=\"hl-brackets\">]</span><span class=\"hl-identifier\">*</span><span class=\"hl-brackets\">[</span><span class=\"hl-var\">urls</span><span class=\"hl-brackets\">]</span><span class=\"hl-identifier\">*</span><span class=\"hl-identifier\">Options</span><span class=\"hl-code\">\r\n--</span><span class=\"hl-identifier\">help</span><span class=\"hl-code\"> | -</span><span class=\"hl-identifier\">h</span><span class=\"hl-identifier\">Displays</span><span class=\"hl-identifier\">this</span><span class=\"hl-identifier\">information</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">format</span><span class=\"hl-code\">=&lt;</span><span class=\"hl-identifier\">format</span><span class=\"hl-code\">&gt; | -</span><span class=\"hl-identifier\">f</span><span class=\"hl-code\"> &lt;</span><span class=\"hl-identifier\">format</span><span class=\"hl-code\">&gt;             </span><span class=\"hl-identifier\">Indicate</span><span class=\"hl-identifier\">which</span><span class=\"hl-identifier\">format</span><span class=\"hl-brackets\">[</span><span class=\"hl-var\">junit-xml</span><span class=\"hl-code\"> | </span><span class=\"hl-var\">spof-xml</span><span class=\"hl-code\"> | </span><span class=\"hl-var\">text</span><span class=\"hl-brackets\">]</span><span class=\"hl-identifier\">to</span><span class=\"hl-identifier\">use</span><span class=\"hl-identifier\">for</span><span class=\"hl-identifier\">output</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">outputdir</span><span class=\"hl-code\">=&lt;</span><span class=\"hl-identifier\">dir</span><span class=\"hl-code\">&gt; | -</span><span class=\"hl-identifier\">o</span><span class=\"hl-code\"> &lt;</span><span class=\"hl-identifier\">dir</span><span class=\"hl-code\">&gt;                </span><span class=\"hl-identifier\">Outputs</span><span class=\"hl-identifier\">the</span><span class=\"hl-identifier\">spof</span><span class=\"hl-identifier\">results</span><span class=\"hl-identifier\">to</span><span class=\"hl-identifier\">this</span><span class=\"hl-identifier\">directory</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">rules</span><span class=\"hl-code\">=&lt;</span><span class=\"hl-identifier\">rule</span><span class=\"hl-brackets\">[</span><span class=\"hl-code\">,</span><span class=\"hl-var\">rule</span><span class=\"hl-brackets\">]</span><span class=\"hl-code\">+&gt; | -</span><span class=\"hl-identifier\">r</span><span class=\"hl-code\"> &lt;</span><span class=\"hl-identifier\">rule</span><span class=\"hl-brackets\">[</span><span class=\"hl-code\">,</span><span class=\"hl-var\">rule</span><span class=\"hl-brackets\">]</span><span class=\"hl-code\">+&gt;  </span><span class=\"hl-identifier\">Indicate</span><span class=\"hl-identifier\">which</span><span class=\"hl-identifier\">rules</span><span class=\"hl-identifier\">to</span><span class=\"hl-identifier\">include</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">print</span><span class=\"hl-code\"> | -</span><span class=\"hl-identifier\">p</span><span class=\"hl-identifier\">Outputs</span><span class=\"hl-identifier\">the</span><span class=\"hl-identifier\">results</span><span class=\"hl-identifier\">in</span><span class=\"hl-identifier\">console</span><span class=\"hl-code\">,\r\n                                              </span><span class=\"hl-identifier\">instead</span><span class=\"hl-identifier\">of</span><span class=\"hl-identifier\">saving</span><span class=\"hl-identifier\">to</span><span class=\"hl-identifier\">a</span><span class=\"hl-identifier\">file</span><span class=\"hl-code\">.\r\n--</span><span class=\"hl-identifier\">quiet</span><span class=\"hl-code\"> | -</span><span class=\"hl-identifier\">q</span><span class=\"hl-identifier\">Keeps</span><span class=\"hl-identifier\">the</span><span class=\"hl-identifier\">console</span><span class=\"hl-identifier\">clear</span><span class=\"hl-identifier\">from</span><span class=\"hl-identifier\">logging</span><span class=\"hl-code\">.</span></pre>\n</div>\n<p>Example</p>\n<pre>$ spofcheck -f junit-xml -o /tests www.ebay.com www.amazon.com</pre>\n<h2>Rules</h2>\n<p>SPOFCheck by default runs with 5 rules (checks). The rules are maintained in the <a href=\"https://github.com/senthilp/spofcheck/blob/master/lib/rules.js\">rules.js</a> file. New rules are easily added by pushing entries to the <a href=\"https://github.com/senthilp/spofcheck/blob/master/lib/rules.js#L6\">rules</a> array or calling the spof API <a href=\"https://github.com/senthilp/spofcheck/blob/master/lib/engine.js#L142\">registerRules</a>. The default rules come from Souders’s original <a href=\"http://www.stevesouders.com/blog/2010/06/01/frontend-spof/\">list</a> outlined below.</p>\n<ol>\n<li><code>3rdparty-scripts</code> &#8211; Always load 3rd party external scripts asyncronously in a non-blocking pattern</li>\n<li><code>application-js</code> &#8211; Load application JS in a non-blocking pattern or towards the end of page</li>\n<li><code>fontface-stylesheet</code> &#8211; Try to inline @font-face style. Also make the font files compressed and cacheable</li>\n<li><code>fontface-inline</code> &#8211; Make sure the fonts files are compressed, cached and small in size</li>\n<li><code>fontface-inline-precede-script-IE</code> &#8211; Make sure inlined @font-face is not preceded by a SCRIPT tag, causes SPOF in IE</li>\n</ol>\n<h2 id=\"output\">Output</h2>\n<p>SPOFCheck creates a file and writes results in one of these formats:</p>\n<ul>\n<li><code>junit-xml</code> &#8211; a format most CI servers can parse (the default format)</li>\n<li><code>spof-xml</code> &#8211; an XML format that can be consumed by other utilities</li>\n<li><code>text</code> &#8211; a textual representation of the results</li>\n</ul>\n<p>The format can be specified using the <code>--format</code> or <code>-f</code> option. For just printing results, i.e. no file creation, use the <code>--print</code> or <code>-p</code> option.</p>\n<h2 id=\"a_spof_free_world\">A SPOF Free World</h2>\n<p>Our primary goal was to eradicate frontend SPOF before it creeps in, and SPOFCheck is a step towards it. Go ahead, give <a href=\"http://senthilp.github.com/spofcheck/\">SPOFCheck</a> a try and integrate it with your CI environments. Let’s build a SPOF Free World <img alt=\":-)\" src=\"http://calendar.perfplanet.com/wp-includes/images/smilies/icon_smile.gif\" />.</p>\n<p>Thanks to github projects <a href=\"https://github.com/pmeenan/spof-o-matic\">spof-o-matic</a> and <a href=\"https://github.com/stoyan/yslow\">3po</a>, a lot of the code logic has been re-used here. The design and packaging of the tool is based on <a href=\"https://github.com/stubbornella/csslint\">csslint</a>, thanks to <a href=\"https://twitter.com/slicknet\">Nicholas Zakas</a> and <a href=\"https://twitter.com/stubbornella\">Nicole Sullivan</a>.</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/spofcheck-fighting-frontend-spof-at-its-root/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "3"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      },
      {
         "title": "Deciphering the Critical Rendering Path",
         "description": "<p>As Steve pointed out in an <a href=\"http://calendar.perfplanet.com/2012/moving-beyond-window-onload/\">earlier post</a>, window.onload is not the best metric for measuring website speed. It is a convenient metric, and a familiar one, but it fails to capture the dynamic nature of most modern pages. Instead,  we want to think about the user perceived performance of the page: how quickly can the user begin interacting with the page?</p>\n<p>The definition of &#8220;interacting&#8221; will vary depending on your page. For some, this may be as simple as getting the text visible on the page, such that the user can begin consuming the information they requested (e.g. this page). For others, this may require wiring up dozens of JavaScript components to build up a JavaScript UI (e.g. Gmail). However, in both cases, there is one prerequisite: the user must be able to see the page, which is to say, the browser needs to render <em>something</em> to the screen.</p>\n<p>So, with that in mind, what does it actually take to do a first content render in a modern browser?</p>\n<h2>DOM + CSSOM = Render Tree</h2>\n<p>The exact timing and behavior of the rendering pipeline will, of course, vary based on the parsing, layout and compositing pipelines of the browser. However, implementation differences aside, to get anything visible on the screen, all browsers must construct something resembling a &#8220;render tree&#8221;.</p>\n<p><img src=\"http://www.igvita.com/posts/12/doc-render.png\" alt=\"document render steps\" /></p>\n<p>The parsing of the HTML document is what constructs the DOM. In parallel, there is an oft forgotten cousin, the CSSOM, which is constructed from the specified stylesheet rules and resources. The two are then combined to create the &#8220;render tree&#8221;, at which point the browser has enough information to perform a layout and paint something to the screen. So far, so good.</p>\n<p>However, the diagram above shows an optimistic case: both the CSSOM and the DOM trees are shown as being constructed in parallel. This is where we must, unfortunately, introduce our favorite friend and foe &#8211; JavaScript.</p>\n<ul>\n<li>Synchronous JavaScript can issue a doc.write at any point; hence the DOM tree construction is blocked anytime a synchronous script is encountered</li>\n<li>JavaScript can query for a computed style of any object, which means it can also block on CSS</li>\n</ul>\n<p><img src=\"http://www.igvita.com/posts/12/doc-render-js.png\" alt=\"document render steps, with JavaScript\" /></p>\n<p>Instead of nice, parallel construction of the DOM and CSSOM objects shown in the earlier diagram, the two are now potentially intertwined: DOM construction can&#8217;t proceed until JavaScript is executed, and JavaScript can&#8217;t proceed until CSSOM is available. Yikes.</p>\n<p>Depending on how this dependency graph is resolved on your pages, which is governed by how, and how many resources you include in that first &#8220;critical path&#8221; of the page load, the time to first render will vary accordingly. Can we get some metrics, or insights into this process? Turns out, yes we can!</p>\n<h2>Document Interactive &amp; DOMContentLoaded</h2>\n<p>The HTML5 spec defines a <a href=\"http://www.w3.org/TR/html5/syntax.html#the-end\">well documented sequence of steps</a> which the user agent must follow while constructing the page. Specifically, the end sequence captures two states, which can help answer our earlier question:</p>\n<ul>\n<li>The document is marked as &#8220;interactive&#8221; when the user agent stops parsing the document. Meaning, the DOM tree is ready.</li>\n<li>The user agent fires the DOMContentLoaded (DCL) event once any scripts marked with &#8220;defer have been executed, and there are no stylesheets that are blocking scripts. Meaning, the CSSOM is ready.</li>\n</ul>\n<p>If no synchronous JavaScript is thrown into the mix, then the DOM and CSSOM construction can proceed in parallel. Things get more interesting once we introduce JavaScript into the picture.</p>\n<p>If you add a script and tag it with &#8220;defer&#8221;, then you unblock the construction of the DOM: the document interactive state does not have to wait for execution of JavaScript. However, note that this same script will be executed <b>before</b> DCL is fired. Further, recall that JavaScript may query CSSOM, which means that the DCL event may be held until the CSSOM is ready, at which point the script will be executed. In short: we&#8217;ve unblocked the &#8220;document interactive&#8221; state, but we&#8217;re still potentially blocking DCL.</p>\n<p>If you add a script and tag it with &#8220;async&#8221;, then you inherit similar behavior as above, but with one distinction: DCL does not have to wait for execution of async scripts!</p>\n<p>The first important takeaway here is that by default, JavaScript will block DOM construction, which may block on CSSOM. Sync scripts are bad, but you already knew that. Marking scripts with &#8220;defer&#8221; and &#8220;async&#8221; makes an implicit promise to the document parser that you will not use doc.write, which in turn allows it to unblock DOM construction.</p>\n<p>Second takeaways is: if at any point we must wait for JavaScript execution, then we will have to first wait for the CSSOM construction to finish. In other words, there is a hard dependency edge between JavaScript and CSS&#8230; Stylesheets at the top, scripts at the bottom? Now you know why.</p>\n<p>Ok! This is all great in theory, but is this practical knowledge to help us optimize pages? Neither metric is a direct indicator of when the page will be painted, but monitoring either or both is a step in the right direction towards our ultimate goal of improving perceived performance.</p>\n<h2>Tracking the critical path of your page</h2>\n<p>If nothing else, <a href=\"https://developer.mozilla.org/en-US/docs/DOM/document.readyState\">monitoring</a> &#8220;document interactive&#8221; will give you a good indicator of whether you are blocking DOM construction due to synchronous scripts. Sometimes, there is no way around this behavior, but this should be a known fact and a tradeoff, not an implicit &#8220;that&#8217;s how it works&#8221;.</p>\n<p>The DCL event is also a critical milestone. Many popular libraries, such as JQuery, will begin executing their code once it fires. In other words, this is likely the first point at which your client code can begin interacting with the page, as well as provide meaningful feedback to the user. If you do your job right, then through the magic of progressive enhancement, you can get the skeleton of the page up, such that the user can begin interacting with the page while the browser continues to load the remaining assets. The IE team has an excellent example illustrating the <a href=\"http://ie.microsoft.com/testdrive/HTML5/DOMContentLoaded/Default.html\">difference between DCL and the window.onload</a> events.</p>\n<h2>When does your DOMContentLoaded fire?</h2>\n<p>What you can measure, you can optimize. Even better, Navigation Timing spec already captures all the events we need: domInteractive, domContentLoadedEvent{Start,End}, and loadEvent{Start,End}. If you are already tracking the onload event already, then you might want to add the two events we&#8217;ve have covered here as well!</p>\n<p>On that note, if you are using Google Analytics, then Christmas came early this year. The team recently added a new &#8220;<b>DOM Timings</b>&#8221; section. Guess which values it tracks? Yep.</p2>\n<p><img src=\"http://www.igvita.com/posts/12/ga-dcl.png\" alt=\"Google Analytics DOM timing report\" /></p>\n<p>Login into your GA account and head to &#8220;Content > <a href=\"http://support.google.com/analytics/bin/answer.py?hl=en&amp;answer=1205784\">Site Speed</a>&#8220;. Once there, head to the &#8220;Performance&#8221; tab to see the timing histograms for all of your pages, or drill into the stats for a particular page. From there, you can track your document interactive, DCL, and onload events.</p>\n<p>Just for fun, here is a side by side comparison of the DCL vs. onload histograms for my site:</p>\n<p><img src=\"http://www.igvita.com/posts/12/igvita-dcl-onload.png\" alt=\"DCL vs. onload histogram\" /></p>\n<p>The median time to DCL is under 1s, whereas the median for onload is ~1.5s. The relatively high DCL timing immediately tells me that there is likely a script that is blocking the construction of the DOM &#8211; something I should revisit. Having said that, the fact that there is a ~0.5s delta between DCL and onload tells me that I&#8217;m not forcing users to wait for all the assets to download before they can see <i>some of the content</i>.</p>\n<p>When do your document interactive and DCL events fire?</p>",
         "summary": "As Steve pointed out in an earlier post, window.onload is not the best metric for measuring website speed. It is a convenient metric, and a familiar one, but it fails to capture the dynamic nature of most modern pages. Instead, we want to think about the user perceived performance of the page: how quickly can [...]",
         "date": "2012-12-26T23:30:20.000Z",
         "pubdate": "2012-12-26T23:30:20.000Z",
         "pubDate": "2012-12-26T23:30:20.000Z",
         "link": "http://calendar.perfplanet.com/2012/deciphering-the-critical-rendering-path/",
         "guid": "http://calendar.perfplanet.com/?p=1548",
         "author": "stoyan",
         "comments": "http://calendar.perfplanet.com/2012/deciphering-the-critical-rendering-path/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "performance"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "Deciphering the Critical Rendering Path"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/deciphering-the-critical-rendering-path/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/deciphering-the-critical-rendering-path/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Wed, 26 Dec 2012 23:30:20 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "stoyan"
         },
         "rss:category": {
            "@": {},
            "#": "performance"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1548"
         },
         "rss:description": {
            "@": {},
            "#": "As Steve pointed out in an earlier post, window.onload is not the best metric for measuring website speed. It is a convenient metric, and a familiar one, but it fails to capture the dynamic nature of most modern pages. Instead, we want to think about the user perceived performance of the page: how quickly can [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<p>As Steve pointed out in an <a href=\"http://calendar.perfplanet.com/2012/moving-beyond-window-onload/\">earlier post</a>, window.onload is not the best metric for measuring website speed. It is a convenient metric, and a familiar one, but it fails to capture the dynamic nature of most modern pages. Instead,  we want to think about the user perceived performance of the page: how quickly can the user begin interacting with the page?</p>\n<p>The definition of &#8220;interacting&#8221; will vary depending on your page. For some, this may be as simple as getting the text visible on the page, such that the user can begin consuming the information they requested (e.g. this page). For others, this may require wiring up dozens of JavaScript components to build up a JavaScript UI (e.g. Gmail). However, in both cases, there is one prerequisite: the user must be able to see the page, which is to say, the browser needs to render <em>something</em> to the screen.</p>\n<p>So, with that in mind, what does it actually take to do a first content render in a modern browser?</p>\n<h2>DOM + CSSOM = Render Tree</h2>\n<p>The exact timing and behavior of the rendering pipeline will, of course, vary based on the parsing, layout and compositing pipelines of the browser. However, implementation differences aside, to get anything visible on the screen, all browsers must construct something resembling a &#8220;render tree&#8221;.</p>\n<p><img src=\"http://www.igvita.com/posts/12/doc-render.png\" alt=\"document render steps\" /></p>\n<p>The parsing of the HTML document is what constructs the DOM. In parallel, there is an oft forgotten cousin, the CSSOM, which is constructed from the specified stylesheet rules and resources. The two are then combined to create the &#8220;render tree&#8221;, at which point the browser has enough information to perform a layout and paint something to the screen. So far, so good.</p>\n<p>However, the diagram above shows an optimistic case: both the CSSOM and the DOM trees are shown as being constructed in parallel. This is where we must, unfortunately, introduce our favorite friend and foe &#8211; JavaScript.</p>\n<ul>\n<li>Synchronous JavaScript can issue a doc.write at any point; hence the DOM tree construction is blocked anytime a synchronous script is encountered</li>\n<li>JavaScript can query for a computed style of any object, which means it can also block on CSS</li>\n</ul>\n<p><img src=\"http://www.igvita.com/posts/12/doc-render-js.png\" alt=\"document render steps, with JavaScript\" /></p>\n<p>Instead of nice, parallel construction of the DOM and CSSOM objects shown in the earlier diagram, the two are now potentially intertwined: DOM construction can&#8217;t proceed until JavaScript is executed, and JavaScript can&#8217;t proceed until CSSOM is available. Yikes.</p>\n<p>Depending on how this dependency graph is resolved on your pages, which is governed by how, and how many resources you include in that first &#8220;critical path&#8221; of the page load, the time to first render will vary accordingly. Can we get some metrics, or insights into this process? Turns out, yes we can!</p>\n<h2>Document Interactive &amp; DOMContentLoaded</h2>\n<p>The HTML5 spec defines a <a href=\"http://www.w3.org/TR/html5/syntax.html#the-end\">well documented sequence of steps</a> which the user agent must follow while constructing the page. Specifically, the end sequence captures two states, which can help answer our earlier question:</p>\n<ul>\n<li>The document is marked as &#8220;interactive&#8221; when the user agent stops parsing the document. Meaning, the DOM tree is ready.</li>\n<li>The user agent fires the DOMContentLoaded (DCL) event once any scripts marked with &#8220;defer have been executed, and there are no stylesheets that are blocking scripts. Meaning, the CSSOM is ready.</li>\n</ul>\n<p>If no synchronous JavaScript is thrown into the mix, then the DOM and CSSOM construction can proceed in parallel. Things get more interesting once we introduce JavaScript into the picture.</p>\n<p>If you add a script and tag it with &#8220;defer&#8221;, then you unblock the construction of the DOM: the document interactive state does not have to wait for execution of JavaScript. However, note that this same script will be executed <b>before</b> DCL is fired. Further, recall that JavaScript may query CSSOM, which means that the DCL event may be held until the CSSOM is ready, at which point the script will be executed. In short: we&#8217;ve unblocked the &#8220;document interactive&#8221; state, but we&#8217;re still potentially blocking DCL.</p>\n<p>If you add a script and tag it with &#8220;async&#8221;, then you inherit similar behavior as above, but with one distinction: DCL does not have to wait for execution of async scripts!</p>\n<p>The first important takeaway here is that by default, JavaScript will block DOM construction, which may block on CSSOM. Sync scripts are bad, but you already knew that. Marking scripts with &#8220;defer&#8221; and &#8220;async&#8221; makes an implicit promise to the document parser that you will not use doc.write, which in turn allows it to unblock DOM construction.</p>\n<p>Second takeaways is: if at any point we must wait for JavaScript execution, then we will have to first wait for the CSSOM construction to finish. In other words, there is a hard dependency edge between JavaScript and CSS&#8230; Stylesheets at the top, scripts at the bottom? Now you know why.</p>\n<p>Ok! This is all great in theory, but is this practical knowledge to help us optimize pages? Neither metric is a direct indicator of when the page will be painted, but monitoring either or both is a step in the right direction towards our ultimate goal of improving perceived performance.</p>\n<h2>Tracking the critical path of your page</h2>\n<p>If nothing else, <a href=\"https://developer.mozilla.org/en-US/docs/DOM/document.readyState\">monitoring</a> &#8220;document interactive&#8221; will give you a good indicator of whether you are blocking DOM construction due to synchronous scripts. Sometimes, there is no way around this behavior, but this should be a known fact and a tradeoff, not an implicit &#8220;that&#8217;s how it works&#8221;.</p>\n<p>The DCL event is also a critical milestone. Many popular libraries, such as JQuery, will begin executing their code once it fires. In other words, this is likely the first point at which your client code can begin interacting with the page, as well as provide meaningful feedback to the user. If you do your job right, then through the magic of progressive enhancement, you can get the skeleton of the page up, such that the user can begin interacting with the page while the browser continues to load the remaining assets. The IE team has an excellent example illustrating the <a href=\"http://ie.microsoft.com/testdrive/HTML5/DOMContentLoaded/Default.html\">difference between DCL and the window.onload</a> events.</p>\n<h2>When does your DOMContentLoaded fire?</h2>\n<p>What you can measure, you can optimize. Even better, Navigation Timing spec already captures all the events we need: domInteractive, domContentLoadedEvent{Start,End}, and loadEvent{Start,End}. If you are already tracking the onload event already, then you might want to add the two events we&#8217;ve have covered here as well!</p>\n<p>On that note, if you are using Google Analytics, then Christmas came early this year. The team recently added a new &#8220;<b>DOM Timings</b>&#8221; section. Guess which values it tracks? Yep.</p2>\n<p><img src=\"http://www.igvita.com/posts/12/ga-dcl.png\" alt=\"Google Analytics DOM timing report\" /></p>\n<p>Login into your GA account and head to &#8220;Content > <a href=\"http://support.google.com/analytics/bin/answer.py?hl=en&amp;answer=1205784\">Site Speed</a>&#8220;. Once there, head to the &#8220;Performance&#8221; tab to see the timing histograms for all of your pages, or drill into the stats for a particular page. From there, you can track your document interactive, DCL, and onload events.</p>\n<p>Just for fun, here is a side by side comparison of the DCL vs. onload histograms for my site:</p>\n<p><img src=\"http://www.igvita.com/posts/12/igvita-dcl-onload.png\" alt=\"DCL vs. onload histogram\" /></p>\n<p>The median time to DCL is under 1s, whereas the median for onload is ~1.5s. The relatively high DCL timing immediately tells me that there is likely a script that is blocking the construction of the DOM &#8211; something I should revisit. Having said that, the fact that there is a ~0.5s delta between DCL and onload tells me that I&#8217;m not forcing users to wait for all the assets to download before they can see <i>some of the content</i>.</p>\n<p>When do your document interactive and DCL events fire?</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/deciphering-the-critical-rendering-path/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "6"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      },
      {
         "title": "Life on the edge with ESI",
         "description": "<p>Let&#8217;s start with some numbers. The speed of light is 299,792 kilometers per second. The refractive index of optic fiber is about 1.5. That means light travels slower inside optic fiber. How slow? That&#8217;s roughly 299,792 kilometers per second divided by 1.5. In general, we round it off to 200,000 kilometers per second. The distance from Seoul to Buenos Aires is about 19454 kilometers. So theoretically it will take around 100 ms to transfer a signal from Seoul to Buenos Aires. </p>\n<p>In reality, there are many more things to consider for web applications when transferring web contents from Seoul to Buenos Aires. First of all, the line of optic fiber is definitely not straight. Thus the actual distance is much longer. Also there is 3 way handshake needed to establish a TCP connection between the two points. And then there can be multiple objects such as JavaScript, CSS and images to transfer in addition to HTML. Finally there are the actual time to process a request and provide a response in the web server.</p>\n<p>According to some studies from Google, users consider anything more than 100ms to be a perceivable delay. i.e. if your web site does not respond within 100ms, it is considered to be slow. The interaction have to be smooth, just like flipping a magazine. However, as the numbers above have shown, it can be quite a challenge. </p>\n<p>One of the more popular tricks to lower the latency for web applications is to use edge cache. The idea is to cache JavaScript, CSS and images in servers that are close to the user (i.e. the edge of the Internet). So the latency to access these resources will be much lower. </p>\n<p>But what about the HTML page? Normally there are many information captured in a HTML page. Some (e.g. news article) are cacheable. Some (e.g. personal information) are not. That makes it hard to cache the entire HTML page. And that&#8217;s where Edge Side Include (ESI) can help.</p>\n<h3>ESI</h3>\n<p>So how can ESI help? We can take a look at the diagram below. Suppose we have a news article page with some user specific modules such as advertisement and recently read news. The edge cache will forward the user request for the article page to a special ESI service. The ESI service will then return a cacheable ESI document.The document will contain the content of the news article as well as one or more &#8220;ESI includes&#8221; for each of the user specific modules. The ESI includes are markup that instructs the edge cache to request these modules from origin servers. The edge cache will replace the ESI include markups with the actual contents of these requests before sending the final processed response back to user.</p>\n<p><img src=\"http://calendar.perfplanet.com/wp-content/uploads/2012/12/esi.png\" alt=\"esi\" width=\"851\" height=\"644\" class=\"alignnone size-full wp-image-1547\" /></p>\n<p>For subsequent requests from other users, the ESI document with the news article is already cached in the edge cache and the ESI includes will be processed for each user to request the corresponding personalized modules. The overall latency can be lower because much of the HTML page is coming from an edge cache server close to the user and only parts for the personalized module are coming directly from origin servers where the latency in between could be higher.</p>\n<p>One can also use client side Ajax JavaScripts to fetch modules to display on the page. However, JavaScript may not always be supported (e.g. in cell phone) or enabled (e.g. user can disable it in browser) in all situations. Also this requires extra coding while the ESI solution returns a normal HTML page transparent to browser. Finally as a rule of thumb, we would always like to limit the number of HTTP connections from the client but the Ajax solution is actually creating more. </p>\n<h3>Performance Characteristics</h3>\n<p>There are two performance characteristics that we need to consider before we talk about how to start using ESI.</p>\n<ol>\n<li><b>Concurrent Requests for ESI includes</b> &#8211; When there are more than one ESI includes in the ESI document, we would want all these requests to be executed concurrently.</li>\n<li><b>First Byte Flush</b> &#8211; After the edge cache server executes the requests for the ESI includes, it should start flushing the ESI document to user till the first ESI include. Then when the request for that include is done with a response, the edge cache should flush the content of the include to user and the rest of the ESI document till the next ESI include is reached.</li>\n</ol>\n<h3>Where do I get started?</h3>\n<p>CDN vendors such as F5 and Akamai provide support for ESI. So if you are using services from these vendors, you can take advantage of that. Akamai even extended the original specification. You can read more about it on its <a href=\"http://www.akamai.com/dl/technical_publications/akamai_esi_extensions.pdf\">web site</a>.</p>\n<p>If you aren&#8217;t using a CDN vendor, there are still other open source proxy software that support ESI, such as Varnish and Apache Traffic Server. Varnish supported a minimum subset of the ESI specification. However, error handling is missing. So when the ESI includes fail, you cannot use ESI markup to instruct Varnish to take alternative action, such as rendering an error message or fetch a different module. You can read more about the Varnish ESI support <a href=\"https://www.varnish-cache.org/docs/3.0/tutorial/esi.html\">here</a>. Apache Traffic Server curreently has an experimental ESI plugin available. It supports most of the specification and you can read more about it <a href=\"https://github.com/apache/trafficserver/blob/master/plugins/experimental/esi/README\">here</a>. </p>\n<p>In terms of the performance characteristics described above, Varnish supports first byte flush but not concurrent requests for ESI includes. And it is the opposite for Apache Traffic Server ESI plugin. Work is under way to provide first byte flush support for the Apache Traffic Server ESI plugin and it should be available soon.</p>\n<h3>Future &amp; Conclusion</h3>\n<p>ESI can be a very powerful tool to lower latency for web sites and applications. However, it had been around for over 10 years without any change. And the markup is not powerful and expressive enough to support some modern use cases. e.g. If we want to render different modules for different device, we need to be able to use a custom function on the user agent request header to determine the device type and use different ESI includes for the modules. So an update on the specification is definitely needed and we should see some progress of it in 2013</p>\n<p>For more information, you can also check out my <a href=\"http://velocity.oreilly.com.cn/2012/index.php?func=session&#038;id=2\">presentation</a> and the corresponding <a href=\"http://velocity.oreilly.com.cn/2012/ppts/VelocityChina2012Kit.pdf\">pdf</a> on this topic at <a href=\"http://velocity.oreilly.com.cn/2012/\">Velocity China 2012</a>.</p>",
         "summary": "Let&#8217;s start with some numbers. The speed of light is 299,792 kilometers per second. The refractive index of optic fiber is about 1.5. That means light travels slower inside optic fiber. How slow? That&#8217;s roughly 299,792 kilometers per second divided by 1.5. In general, we round it off to 200,000 kilometers per second. The distance [...]",
         "date": "2012-12-25T23:41:52.000Z",
         "pubdate": "2012-12-25T23:41:52.000Z",
         "pubDate": "2012-12-25T23:41:52.000Z",
         "link": "http://calendar.perfplanet.com/2012/life-on-the-edge-with-esi/",
         "guid": "http://calendar.perfplanet.com/?p=1546",
         "author": "stoyan",
         "comments": "http://calendar.perfplanet.com/2012/life-on-the-edge-with-esi/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "performance"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "Life on the edge with ESI"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/life-on-the-edge-with-esi/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/life-on-the-edge-with-esi/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Tue, 25 Dec 2012 23:41:52 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "stoyan"
         },
         "rss:category": {
            "@": {},
            "#": "performance"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1546"
         },
         "rss:description": {
            "@": {},
            "#": "Let&#8217;s start with some numbers. The speed of light is 299,792 kilometers per second. The refractive index of optic fiber is about 1.5. That means light travels slower inside optic fiber. How slow? That&#8217;s roughly 299,792 kilometers per second divided by 1.5. In general, we round it off to 200,000 kilometers per second. The distance [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<p>Let&#8217;s start with some numbers. The speed of light is 299,792 kilometers per second. The refractive index of optic fiber is about 1.5. That means light travels slower inside optic fiber. How slow? That&#8217;s roughly 299,792 kilometers per second divided by 1.5. In general, we round it off to 200,000 kilometers per second. The distance from Seoul to Buenos Aires is about 19454 kilometers. So theoretically it will take around 100 ms to transfer a signal from Seoul to Buenos Aires. </p>\n<p>In reality, there are many more things to consider for web applications when transferring web contents from Seoul to Buenos Aires. First of all, the line of optic fiber is definitely not straight. Thus the actual distance is much longer. Also there is 3 way handshake needed to establish a TCP connection between the two points. And then there can be multiple objects such as JavaScript, CSS and images to transfer in addition to HTML. Finally there are the actual time to process a request and provide a response in the web server.</p>\n<p>According to some studies from Google, users consider anything more than 100ms to be a perceivable delay. i.e. if your web site does not respond within 100ms, it is considered to be slow. The interaction have to be smooth, just like flipping a magazine. However, as the numbers above have shown, it can be quite a challenge. </p>\n<p>One of the more popular tricks to lower the latency for web applications is to use edge cache. The idea is to cache JavaScript, CSS and images in servers that are close to the user (i.e. the edge of the Internet). So the latency to access these resources will be much lower. </p>\n<p>But what about the HTML page? Normally there are many information captured in a HTML page. Some (e.g. news article) are cacheable. Some (e.g. personal information) are not. That makes it hard to cache the entire HTML page. And that&#8217;s where Edge Side Include (ESI) can help.</p>\n<h3>ESI</h3>\n<p>So how can ESI help? We can take a look at the diagram below. Suppose we have a news article page with some user specific modules such as advertisement and recently read news. The edge cache will forward the user request for the article page to a special ESI service. The ESI service will then return a cacheable ESI document.The document will contain the content of the news article as well as one or more &#8220;ESI includes&#8221; for each of the user specific modules. The ESI includes are markup that instructs the edge cache to request these modules from origin servers. The edge cache will replace the ESI include markups with the actual contents of these requests before sending the final processed response back to user.</p>\n<p><img src=\"http://calendar.perfplanet.com/wp-content/uploads/2012/12/esi.png\" alt=\"esi\" width=\"851\" height=\"644\" class=\"alignnone size-full wp-image-1547\" /></p>\n<p>For subsequent requests from other users, the ESI document with the news article is already cached in the edge cache and the ESI includes will be processed for each user to request the corresponding personalized modules. The overall latency can be lower because much of the HTML page is coming from an edge cache server close to the user and only parts for the personalized module are coming directly from origin servers where the latency in between could be higher.</p>\n<p>One can also use client side Ajax JavaScripts to fetch modules to display on the page. However, JavaScript may not always be supported (e.g. in cell phone) or enabled (e.g. user can disable it in browser) in all situations. Also this requires extra coding while the ESI solution returns a normal HTML page transparent to browser. Finally as a rule of thumb, we would always like to limit the number of HTTP connections from the client but the Ajax solution is actually creating more. </p>\n<h3>Performance Characteristics</h3>\n<p>There are two performance characteristics that we need to consider before we talk about how to start using ESI.</p>\n<ol>\n<li><b>Concurrent Requests for ESI includes</b> &#8211; When there are more than one ESI includes in the ESI document, we would want all these requests to be executed concurrently.</li>\n<li><b>First Byte Flush</b> &#8211; After the edge cache server executes the requests for the ESI includes, it should start flushing the ESI document to user till the first ESI include. Then when the request for that include is done with a response, the edge cache should flush the content of the include to user and the rest of the ESI document till the next ESI include is reached.</li>\n</ol>\n<h3>Where do I get started?</h3>\n<p>CDN vendors such as F5 and Akamai provide support for ESI. So if you are using services from these vendors, you can take advantage of that. Akamai even extended the original specification. You can read more about it on its <a href=\"http://www.akamai.com/dl/technical_publications/akamai_esi_extensions.pdf\">web site</a>.</p>\n<p>If you aren&#8217;t using a CDN vendor, there are still other open source proxy software that support ESI, such as Varnish and Apache Traffic Server. Varnish supported a minimum subset of the ESI specification. However, error handling is missing. So when the ESI includes fail, you cannot use ESI markup to instruct Varnish to take alternative action, such as rendering an error message or fetch a different module. You can read more about the Varnish ESI support <a href=\"https://www.varnish-cache.org/docs/3.0/tutorial/esi.html\">here</a>. Apache Traffic Server curreently has an experimental ESI plugin available. It supports most of the specification and you can read more about it <a href=\"https://github.com/apache/trafficserver/blob/master/plugins/experimental/esi/README\">here</a>. </p>\n<p>In terms of the performance characteristics described above, Varnish supports first byte flush but not concurrent requests for ESI includes. And it is the opposite for Apache Traffic Server ESI plugin. Work is under way to provide first byte flush support for the Apache Traffic Server ESI plugin and it should be available soon.</p>\n<h3>Future &amp; Conclusion</h3>\n<p>ESI can be a very powerful tool to lower latency for web sites and applications. However, it had been around for over 10 years without any change. And the markup is not powerful and expressive enough to support some modern use cases. e.g. If we want to render different modules for different device, we need to be able to use a custom function on the user agent request header to determine the device type and use different ESI includes for the modules. So an update on the specification is definitely needed and we should see some progress of it in 2013</p>\n<p>For more information, you can also check out my <a href=\"http://velocity.oreilly.com.cn/2012/index.php?func=session&#038;id=2\">presentation</a> and the corresponding <a href=\"http://velocity.oreilly.com.cn/2012/ppts/VelocityChina2012Kit.pdf\">pdf</a> on this topic at <a href=\"http://velocity.oreilly.com.cn/2012/\">Velocity China 2012</a>.</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/life-on-the-edge-with-esi/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "0"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      },
      {
         "title": "Proactive Web Performance Optimization",
         "description": "<p>I recently spoke at a <a href=\"http://2012.highload.co/\">few</a> <a href=\"http://velocity.oreilly.com.cn/2012/index.php?func=autobio&amp;id=36\">conferences</a> about how to avoid performance regression which I called <a href=\"http://www.slideshare.net/marcelduran/velocity-china-2012-pwpo\">Proactive Web Performance Optimization</a> or PWPO. This is nothing much than the ordinary <a href=\"http://en.wikipedia.org/wiki/Web_performance_optimization\">WPO</a> we already know. The only difference is where/when in the development cycle we should apply that proactively.</p>\n<p>Performance is a vigilante task and as such one has to always keep an eye on the application performance monitoring. This is especially true after new releases when new features, bug fixes and other changes might unintentionally affect the application performance, eventually breaking end user experience.</p>\n<p>Web Performance Optimization best practices should always be applied while developing an application. Whereas some tools might help identifying potential performance issues during the development cycle, it is a matter of where in the development cycle should WPO tools be run.</p>\n<h3>Worst case scenario: no instrumentation</h3>\n<p>In a development cycle without any instrumentation we have no idea how the application is performing for end users. Even worse we have no clue how good or bad the user experience is. In this scenario when a performance regression is introduced, the end user is the one having a bad experience and ultimately raising the red flag for performance. With luck some bad review will be published forcing us to reactively fix the issue and start over. This might last a few cycles until no one cares and then sadly the application is abandoned for good.</p>\n<p><img alt=\"worst case scenario: no instrumentation development cycle\" src=\"http://i.imgur.com/lgcte.jpg\" /></p>\n<ul>\n<li>Build the application</li>\n<li>Test to ensure nothing is broken</li>\n<li>Deploy to production</li>\n<li>Possible happy users and angry ones raising the red flag for performance</li>\n<li>Angry users write a bad review</li>\n<li>Reading the news, it&#8217;s time to improve performance and start over</li>\n</ul>\n<h3>Better case: RUM</h3>\n<p><a href=\"http://en.wikipedia.org/wiki/Real_user_monitoring\">Real User Measurement</a> (RUM) is an essential piece of instrumentation for every web application. RUM gives the real status of what is going on for the user end. It provides valuable data such as bandwidth, page load times, etc. which allows monitoring and estimating what the end user experience is like. In the case of a performance regression, RUM tells when exactly the regression happens. Nevertheless, the end users are the one suffering with a bad experience. Reactively the issue should be fixed for a next cycle release.</p>\n<p><img alt=\"better case: RUM development cycle\" src=\"http://i.imgur.com/CiMIq.jpg\" /></p>\n<ul>\n<li>Build the application</li>\n<li>Test to ensure nothing is broken</li>\n<li>Deploy to production</li>\n<li>Possible happy and angry users</li>\n<li>Possible RUM raising the red flag for performance knowing end users get bad experience</li>\n<li>It&#8217;s time to improve performance and start over</li>\n</ul>\n<h3>YSlow</h3>\n<p><a href=\"http://yslow.org/\">YSlow</a> was initially developed to run manually in order to perform static performance analysis of the page, reporting any issues found based on a set of performance rules. There was some attempts at automation like hosting a real browser with YSlow installed as an extension and scheduling URLs to be loaded and analyzed.</p>\n<p>Since 2011 YSlow has also been available from the command line for NodeJS using HAR files to perform the static analysis. As of early 2012 YSlow is also available for <a href=\"http://phantomjs.org/\">PhantomJS</a> (Headless WebKit browser) which allows for static analysis of a URL loaded by PhantomJS and analyzed by YSlow reporting the results all via command line. <a href=\"https://github.com/marcelduran/yslow/wiki/PhantomJS\">YSlow for PhantomJS</a> also provides two new output test formats: <a href=\"http://en.wikipedia.org/wiki/Test_Anything_Protocol\">TAP</a> and <a href=\"http://en.wikipedia.org/wiki/Junit\">JUnit</a>. Both techniques test all the rules based on a configurable threshold, producing an indication of exactly which tests pass.</p>\n<h3>Even better case: RUM + YSlow on CI</h3>\n<p>With the advent of YSlow for PhantomJS it becomes easy to integrate YSlow into the development cycle plugging it into the continous integration (CI) pipeline. If there is a performance regression, it breaks the build avoiding a potential performance regression from being pushed to production. This saves the users from ultimately getting a bad experience, as CI is the one raising the red flag for performance regressions. RUM will show no regression was introduced intentionally, however other causes might affect performance and RUM will notify something went wrong.</p>\n<p>There is a comprehensive section on <a href=\"https://github.com/marcelduran/yslow/wiki\">YSlow Wiki</a> that explains <a href=\"https://github.com/marcelduran/yslow/wiki/PhantomJS#wiki-jenkins-integration\">how to plug YSlow + PhantomJS into Jenkins</a>, but it&#8217;s also worth noting that the <code>--threshold</code> parameter is the ultimate way to configure the desired performance acceptance criteria for CI.</p>\n<p><img alt=\"even better case: RUM + YSlow on CI development cycle\" src=\"http://i.imgur.com/RBl9J.jpg\" /></p>\n<ul>\n<li>Build the application</li>\n<li>Test to ensure nothing is broken</li>\n<li>Analyze build with YSlow to either pass or fail web performance acceptance criteria</li>\n<li><em>If it fails, proactively go back and fix the performance issue</em></li>\n<li>Once performance is fine, deploy to production</li>\n<li>Hopefully happy users only</li>\n<li>Keep monitoring RUM for performance issues</li>\n<li>It&#8217;s always time to improve performance and start over</li>\n</ul>\n<h3>Best case: RUM + YSlow on CI + WPT</h3>\n<p>For high performance applications in a well defined building cycle, YSlow scores might become stale always reporting A or B. This doesn&#8217;t tell much about smaller performance regressions and still might lead to some regression being pushed to the end user. It&#8217;s very important to keep monitoring RUM data in order to detect any unplanned variation. This is the most accurate info one can get about the user experience.</p>\n<p>Once the YSlow score is satisfied, i.e., it doesn&#8217;t break the build, the next layer of proactive WPO is benchmarking the build in real browsers with a reasonable sample (the greater the better) to avoid variations. Use either the median or average of these runs. This should be compared to the current production baseline and within a certain threshold this should either pass or break the build to avoid fine performance regression.</p>\n<p>To automate the benchmarking part, <a href=\"http://www.webpagetest.org/\">WebPagetest</a> is a good fit and the <a href=\"http://marcelduran.com/webpagetest-api/\">WebPagetest API Wrapper</a> can be used power NodeJS applications (more info on <a href=\"http://calendar.perfplanet.com/2012/xmas-gift-webpagetest-api-swiss-army-knife/\">previous post</a>).</p>\n<p><img alt=\"best case: RUM + YSlow on CI + WPT development cycle\" src=\"http://i.imgur.com/3UsfC.jpg\" /></p>\n<ul>\n<li>Build the application</li>\n<li>Test to ensure nothing is broken</li>\n<li>Analyze build with YSlow to either pass or fail web performance acceptance criteria</li>\n<li><em>If it fails, proactively go back and fix the performance issue</em></li>\n<li>Once YSlow is satisfied, it&#8217;s time to benchmark and compare against the current production baseline</li>\n<li><em>If it fails, proactively go back and fix the performance issue</em></li>\n<li>Once the two layers of performance prevention are satisfied, deploy to production</li>\n<li>Very likely lots of happy users only</li>\n<li>Keep monitoring RUM for other performance issues</li>\n<li>There&#8217;s always a few <em>ms</em> to squeeze in order to improve performance and start over</li>\n</ul>\n<p>When comparing new builds (branches) to the production baseline, the ideal scenario is to have performance boxes as close as possible to production boxes (replica ideally) and performance WPT benchmarks in an isolated environment so test results are reproducible and don&#8217;t get too much deviation.</p>\n<p>The last two cases: <strong>RUM + YSlow on CI</strong> <em>vs</em> <strong>RUM + YSlow on CI + WPT</strong> are quite similar to prevent performance regression whereas the latter gets more in-depth performance metrics to either pass or fail minimum performance acceptance criteria. <strong>RUM + YSlow on CI</strong> is analogous to going to the doctor for a routine checkup. The doctor asks a few questions and checks for heart beat amongst other superficial exams, eventually recommending lab exams. <strong>RUM + YSlow on CI + WPT</strong> on the other hand is analogous to going straight to the lab for a full body exam. It&#8217;s more invasive however more precise and would tell exactly what&#8217;s wrong.</p>\n<h3>Takeaway</h3>\n<p>Stop introducing performance regressions. Don&#8217;t let your end users be the ones raising the red flag for performance when you can proactively prevent regressions by simply plugging YSlow into the CI pipeline, and even better by benchmarking before releasing.</p>",
         "summary": "I recently spoke at a few conferences about how to avoid performance regression which I called Proactive Web Performance Optimization or PWPO. This is nothing much than the ordinary WPO we already know. The only difference is where/when in the development cycle we should apply that proactively. Performance is a vigilante task and as such [...]",
         "date": "2012-12-24T19:06:55.000Z",
         "pubdate": "2012-12-24T19:06:55.000Z",
         "pubDate": "2012-12-24T19:06:55.000Z",
         "link": "http://calendar.perfplanet.com/2012/proactive-web-performance-optimization/",
         "guid": "http://calendar.perfplanet.com/?p=1531",
         "author": "editor",
         "comments": "http://calendar.perfplanet.com/2012/proactive-web-performance-optimization/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "performance"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "Proactive Web Performance Optimization"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/proactive-web-performance-optimization/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/proactive-web-performance-optimization/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Mon, 24 Dec 2012 19:06:55 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "editor"
         },
         "rss:category": {
            "@": {},
            "#": "performance"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1531"
         },
         "rss:description": {
            "@": {},
            "#": "I recently spoke at a few conferences about how to avoid performance regression which I called Proactive Web Performance Optimization or PWPO. This is nothing much than the ordinary WPO we already know. The only difference is where/when in the development cycle we should apply that proactively. Performance is a vigilante task and as such [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<p>I recently spoke at a <a href=\"http://2012.highload.co/\">few</a> <a href=\"http://velocity.oreilly.com.cn/2012/index.php?func=autobio&amp;id=36\">conferences</a> about how to avoid performance regression which I called <a href=\"http://www.slideshare.net/marcelduran/velocity-china-2012-pwpo\">Proactive Web Performance Optimization</a> or PWPO. This is nothing much than the ordinary <a href=\"http://en.wikipedia.org/wiki/Web_performance_optimization\">WPO</a> we already know. The only difference is where/when in the development cycle we should apply that proactively.</p>\n<p>Performance is a vigilante task and as such one has to always keep an eye on the application performance monitoring. This is especially true after new releases when new features, bug fixes and other changes might unintentionally affect the application performance, eventually breaking end user experience.</p>\n<p>Web Performance Optimization best practices should always be applied while developing an application. Whereas some tools might help identifying potential performance issues during the development cycle, it is a matter of where in the development cycle should WPO tools be run.</p>\n<h3>Worst case scenario: no instrumentation</h3>\n<p>In a development cycle without any instrumentation we have no idea how the application is performing for end users. Even worse we have no clue how good or bad the user experience is. In this scenario when a performance regression is introduced, the end user is the one having a bad experience and ultimately raising the red flag for performance. With luck some bad review will be published forcing us to reactively fix the issue and start over. This might last a few cycles until no one cares and then sadly the application is abandoned for good.</p>\n<p><img alt=\"worst case scenario: no instrumentation development cycle\" src=\"http://i.imgur.com/lgcte.jpg\" /></p>\n<ul>\n<li>Build the application</li>\n<li>Test to ensure nothing is broken</li>\n<li>Deploy to production</li>\n<li>Possible happy users and angry ones raising the red flag for performance</li>\n<li>Angry users write a bad review</li>\n<li>Reading the news, it&#8217;s time to improve performance and start over</li>\n</ul>\n<h3>Better case: RUM</h3>\n<p><a href=\"http://en.wikipedia.org/wiki/Real_user_monitoring\">Real User Measurement</a> (RUM) is an essential piece of instrumentation for every web application. RUM gives the real status of what is going on for the user end. It provides valuable data such as bandwidth, page load times, etc. which allows monitoring and estimating what the end user experience is like. In the case of a performance regression, RUM tells when exactly the regression happens. Nevertheless, the end users are the one suffering with a bad experience. Reactively the issue should be fixed for a next cycle release.</p>\n<p><img alt=\"better case: RUM development cycle\" src=\"http://i.imgur.com/CiMIq.jpg\" /></p>\n<ul>\n<li>Build the application</li>\n<li>Test to ensure nothing is broken</li>\n<li>Deploy to production</li>\n<li>Possible happy and angry users</li>\n<li>Possible RUM raising the red flag for performance knowing end users get bad experience</li>\n<li>It&#8217;s time to improve performance and start over</li>\n</ul>\n<h3>YSlow</h3>\n<p><a href=\"http://yslow.org/\">YSlow</a> was initially developed to run manually in order to perform static performance analysis of the page, reporting any issues found based on a set of performance rules. There was some attempts at automation like hosting a real browser with YSlow installed as an extension and scheduling URLs to be loaded and analyzed.</p>\n<p>Since 2011 YSlow has also been available from the command line for NodeJS using HAR files to perform the static analysis. As of early 2012 YSlow is also available for <a href=\"http://phantomjs.org/\">PhantomJS</a> (Headless WebKit browser) which allows for static analysis of a URL loaded by PhantomJS and analyzed by YSlow reporting the results all via command line. <a href=\"https://github.com/marcelduran/yslow/wiki/PhantomJS\">YSlow for PhantomJS</a> also provides two new output test formats: <a href=\"http://en.wikipedia.org/wiki/Test_Anything_Protocol\">TAP</a> and <a href=\"http://en.wikipedia.org/wiki/Junit\">JUnit</a>. Both techniques test all the rules based on a configurable threshold, producing an indication of exactly which tests pass.</p>\n<h3>Even better case: RUM + YSlow on CI</h3>\n<p>With the advent of YSlow for PhantomJS it becomes easy to integrate YSlow into the development cycle plugging it into the continous integration (CI) pipeline. If there is a performance regression, it breaks the build avoiding a potential performance regression from being pushed to production. This saves the users from ultimately getting a bad experience, as CI is the one raising the red flag for performance regressions. RUM will show no regression was introduced intentionally, however other causes might affect performance and RUM will notify something went wrong.</p>\n<p>There is a comprehensive section on <a href=\"https://github.com/marcelduran/yslow/wiki\">YSlow Wiki</a> that explains <a href=\"https://github.com/marcelduran/yslow/wiki/PhantomJS#wiki-jenkins-integration\">how to plug YSlow + PhantomJS into Jenkins</a>, but it&#8217;s also worth noting that the <code>--threshold</code> parameter is the ultimate way to configure the desired performance acceptance criteria for CI.</p>\n<p><img alt=\"even better case: RUM + YSlow on CI development cycle\" src=\"http://i.imgur.com/RBl9J.jpg\" /></p>\n<ul>\n<li>Build the application</li>\n<li>Test to ensure nothing is broken</li>\n<li>Analyze build with YSlow to either pass or fail web performance acceptance criteria</li>\n<li><em>If it fails, proactively go back and fix the performance issue</em></li>\n<li>Once performance is fine, deploy to production</li>\n<li>Hopefully happy users only</li>\n<li>Keep monitoring RUM for performance issues</li>\n<li>It&#8217;s always time to improve performance and start over</li>\n</ul>\n<h3>Best case: RUM + YSlow on CI + WPT</h3>\n<p>For high performance applications in a well defined building cycle, YSlow scores might become stale always reporting A or B. This doesn&#8217;t tell much about smaller performance regressions and still might lead to some regression being pushed to the end user. It&#8217;s very important to keep monitoring RUM data in order to detect any unplanned variation. This is the most accurate info one can get about the user experience.</p>\n<p>Once the YSlow score is satisfied, i.e., it doesn&#8217;t break the build, the next layer of proactive WPO is benchmarking the build in real browsers with a reasonable sample (the greater the better) to avoid variations. Use either the median or average of these runs. This should be compared to the current production baseline and within a certain threshold this should either pass or break the build to avoid fine performance regression.</p>\n<p>To automate the benchmarking part, <a href=\"http://www.webpagetest.org/\">WebPagetest</a> is a good fit and the <a href=\"http://marcelduran.com/webpagetest-api/\">WebPagetest API Wrapper</a> can be used power NodeJS applications (more info on <a href=\"http://calendar.perfplanet.com/2012/xmas-gift-webpagetest-api-swiss-army-knife/\">previous post</a>).</p>\n<p><img alt=\"best case: RUM + YSlow on CI + WPT development cycle\" src=\"http://i.imgur.com/3UsfC.jpg\" /></p>\n<ul>\n<li>Build the application</li>\n<li>Test to ensure nothing is broken</li>\n<li>Analyze build with YSlow to either pass or fail web performance acceptance criteria</li>\n<li><em>If it fails, proactively go back and fix the performance issue</em></li>\n<li>Once YSlow is satisfied, it&#8217;s time to benchmark and compare against the current production baseline</li>\n<li><em>If it fails, proactively go back and fix the performance issue</em></li>\n<li>Once the two layers of performance prevention are satisfied, deploy to production</li>\n<li>Very likely lots of happy users only</li>\n<li>Keep monitoring RUM for other performance issues</li>\n<li>There&#8217;s always a few <em>ms</em> to squeeze in order to improve performance and start over</li>\n</ul>\n<p>When comparing new builds (branches) to the production baseline, the ideal scenario is to have performance boxes as close as possible to production boxes (replica ideally) and performance WPT benchmarks in an isolated environment so test results are reproducible and don&#8217;t get too much deviation.</p>\n<p>The last two cases: <strong>RUM + YSlow on CI</strong> <em>vs</em> <strong>RUM + YSlow on CI + WPT</strong> are quite similar to prevent performance regression whereas the latter gets more in-depth performance metrics to either pass or fail minimum performance acceptance criteria. <strong>RUM + YSlow on CI</strong> is analogous to going to the doctor for a routine checkup. The doctor asks a few questions and checks for heart beat amongst other superficial exams, eventually recommending lab exams. <strong>RUM + YSlow on CI + WPT</strong> on the other hand is analogous to going straight to the lab for a full body exam. It&#8217;s more invasive however more precise and would tell exactly what&#8217;s wrong.</p>\n<h3>Takeaway</h3>\n<p>Stop introducing performance regressions. Don&#8217;t let your end users be the ones raising the red flag for performance when you can proactively prevent regressions by simply plugging YSlow into the CI pipeline, and even better by benchmarking before releasing.</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/proactive-web-performance-optimization/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "0"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      },
      {
         "title": "Make your mobile pages render in under one second",
         "description": "<p>Over the past few years, we&#8217;ve made great strides in understanding and optimizing mobile web performance. However, for the most part, mobile web browsing continues to be slow. Google Analytics data shows that the average web page takes over <a href=\"http://analytics.blogspot.com/2012/04/global-site-speed-overview-how-fast-are.html\">10 seconds to load on mobile</a>. We know that a user&#8217;s thought process is typically interrupted after waiting for just one second, resulting in that <a href=\"http://www.useit.com/papers/responsetime.html\">user starting to become disengaged</a>. So at a minimum, the &#8220;above the fold&#8221; content of a web page should render in less than one second. Clearly, we&#8217;ve still got a lot of work to do.</p>\n<p>But where should we focus our attention when optimizing mobile web performance? We know that mobile networks have highly variable latency and bandwidth characteristics, and that in general <a href=\"http://calendar.perfplanet.com/2011/carrier-networks-down-the-rabbit-hole/\">mobile network latency is substantially higher</a> than that on desktop connections. We also know that for modern networks, <a href=\"http://www.belshe.com/2010/05/24/more-bandwidth-doesnt-matter-much/\">it is round trip time, not bandwidth</a>, that is the dominating factor in page load time. Given this, to make the mobile web fast, our attention should be focused on minimizing the number of blocking round trips incurred before a web page can render its content to the device&#8217;s screen.</p>\n<h2>What blocks rendering a web page to the screen?</h2>\n<p>First, let&#8217;s look at the sequence of events that happens between the time a user initiates a page navigation and the time the browser can render that page to the screen. Round trips may be incurred for DNS resolution, TCP connection, and the request being sent to the server and the response being streamed back. Unfortunately, there&#8217;s not much developers can do to avoid these round trips. For repeat visitors, a longer DNS TTL can help, but TCP connection and request/response overhead will be incurred on every new navigation to a page (assuming there is no warm TCP connection ready to be reused by the client).</p>\n<p>Once these initial round trips are incurred, the mobile device can begin parsing the HTML response. But the browser can&#8217;t paint content to the screen just yet. Before content in the HTML can be painted to the screen, the browser must construct the render tree to determine where the elements in the DOM will appear on screen. And before the render tree can be constructed, the DOM tree must be constructed. The DOM tree is constructed through a combination of parsing HTML and possibly JavaScript execution.</p>\n<p>So what are the things that block parsing of HTML, DOM tree construction, and render tree construction? Most of the time, parsing, DOM tree, and render tree construction are very fast. However, there are a few antipatterns that can cause these processes to get blocked on the network.</p>\n<h2>Sources of delay during rendering: external JavaScript and CSS</h2>\n<p>The most significant source of delay during HTML parsing is external JavaScript. When a browser encounters a (non-async) external script during HTML parsing, it must halt parsing of subsequent HTML until that JavaScript is downloaded, parsed, and executed. This incurs additional round trips, which are especially expensive on mobile. If the script is loaded from a hostname other than the hostname the HTML was served from, additional round trips may be incurred for DNS resolution and TCP connection.</p>\n<p>In addition, render tree construction gets blocked on stylesheets, so just as external JavaScript introduces delays during DOM tree construction, external stylesheets introduce delays during render tree construction.</p>\n<p>In short, external JavaScript and CSS loaded early in the document (e.g. in the <code>&lt;head&gt;</code>) are performance killers, and they are especially expensive on mobile due to the higher round trip times associated with mobile networks.</p>\n<h2>Making mobile pages fast</h2>\n<p>To be fast, a mobile web page must include all of the content needed to render the above the fold region in the initial HTML payload without blocking on external JavaScript or CSS resources. Ideally, all the content needed to render the above the fold region should be in the first 15kB on the network (this is post-gzip-compression size; pre-gzip can be larger), since this is the size of the initial congestion window on modern Linux kernels. This does not mean simply inlining all of the JavaScript and CSS that used to be loaded externally. Instead, just the JavaScript and CSS needed to render the above the fold region should be inlined, and JavaScript or CSS needed to add additional functionality to the page should be loaded asynchronously. For instance, if we have a page like the following:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">html</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">head</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">stylesheet</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">my.css</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">script</span><span class=\"hl-code\"> </span><span class=\"hl-var\">src</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">my.js</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">script</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">head</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">body</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">div</span><span class=\"hl-code\"> </span><span class=\"hl-var\">class</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">main</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n    Here is my content.\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">div</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">div</span><span class=\"hl-code\"> </span><span class=\"hl-var\">class</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">leftnav</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n    Perhaps there is a left nav bar here.\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">div</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  ...\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">body</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">html</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<p>We need to identify the parts of <code>my.js</code> and <code>my.css</code> needed to render the initial content, inline those parts, and delay or async load the remaining JavaScript and CSS needed for the page. This may end up looking something like:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">html</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">head</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">style</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  .main { ... }\r\n  .leftnav { ... }\r\n  /* ... any other styles needed for the initial render here ... */\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">style</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">script</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  // Any script needed for initial render here.\r\n  // Ideally, there should be no JS needed for the initial render\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">script</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">head</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">body</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">div</span><span class=\"hl-code\"> </span><span class=\"hl-var\">class</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">main</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n    Here is my content.\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">div</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">div</span><span class=\"hl-code\"> </span><span class=\"hl-var\">class</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">leftnav</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n    Perhaps there is a left nav bar here.\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">div</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  ...\r\n  </span><span class=\"hl-comment\">&lt;!--</span><span class=\"hl-comment\"> \r\n    NOTE: delay loading of script and stylesheet may best be done\r\n     in an asynchronous callback such as `requestAnimationFrame` \r\n     rather than inline in HTML, since the callback will be invoked \r\n     after the browser has rendered the earlier HTML content to the screen.\r\n   </span><span class=\"hl-comment\">--&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">stylesheet</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">my_leftover.css</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">script</span><span class=\"hl-code\"> </span><span class=\"hl-var\">src</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">my_leftover.js</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">script</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">body</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">html</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<h2>The current state of the mobile web</h2>\n<p>A brief survey of mobile web pages shows that nearly all pages include blocking external JavaScript and/or CSS before any above the fold content is displayed. Exceptions include Google Maps, Google Search, Yahoo! News, and sites like <a href=\"http://www.yummly.com/\">http://www.yummly.com/</a> and Kayak. Unfortunately, some of these sites inline JavaScript and CSS that isn&#8217;t needed for the initial render, which unnecessarily delays the time it takes to render these pages.</p>\n<p>Interestingly, browsing the mobile web with JavaScript disabled reveals that even though most pages load blocking external JavaScript in the head, few of these pages actually need that JavaScript to render their initial content. These pages would benefit from delay or async loading their JavaScript to get the JavaScript out of the critical path of the initial render of the page.</p>\n<h2>What else can block the initial render of a web page?</h2>\n<p>Blocking external JavaScript and CSS are the most common sources of delay on the web. However, there are other common sources of delay that mobile developers should be aware of. One source is HTTP redirects of the main HTML document. These redirects incur additional round trips, and if the redirect navigates to a different hostname (e.g. <code>www.example.com</code> to <code>m.example.com</code>), they add even greater delays due to DNS resolution and TCP connection times. A second source of delay is server backend time spent generating the HTML response. All of the time spent generating the initial HTML response will block rendering on the screen, so server backend time should be kept to a minimum.</p>\n<h2>Rendering in under one second</h2>\n<p>If we estimate 3G network round trip time at 250ms, we can compute the minimum estimated time between when a user initiates a web page navigation and when that page renders its above the fold content on the screen. Assuming no blocking external JavaScript or CSS, we incur three round trips for DNS, TCP, and request/response, for a total of 750ms, plus 100ms for backend time. This brings us to 850ms. As long as render-blocking JavaScript and CSS is inlined and the size of the initial HTML payload is kept to a minimum (e.g. under 15kB compressed), the time it takes to parse and render should be well under 100ms, bringing us in at 950ms, just under our one second target.</p>\n<h2>Summary</h2>\n<p>In summary, to make your mobile web page render in under one second, you should:</p>\n<ul>\n<li>keep server backend time to generate HTML to a minimum (under 100ms)</li>\n<li>avoid HTTP redirects for the main HTML resource</li>\n<li>avoid loading blocking external JavaScript and CSS before the initial render</li>\n<li>inline just the JavaScript and CSS needed for the initial render</li>\n<li>delay or async load any JavaScript and CSS not needed for the initial render</li>\n<li>keep HTML payload needed to render initial content to under 15kB compressed</li>\n</ul>\n<p>If you are looking to improve the performance of your mobile web pages, give these optimizations a try.</p>",
         "summary": "Over the past few years, we&#8217;ve made great strides in understanding and optimizing mobile web performance. However, for the most part, mobile web browsing continues to be slow. Google Analytics data shows that the average web page takes over 10 seconds to load on mobile. We know that a user&#8217;s thought process is typically interrupted [...]",
         "date": "2012-12-23T21:28:46.000Z",
         "pubdate": "2012-12-23T21:28:46.000Z",
         "pubDate": "2012-12-23T21:28:46.000Z",
         "link": "http://calendar.perfplanet.com/2012/make-your-mobile-pages-render-in-under-one-second/",
         "guid": "http://calendar.perfplanet.com/?p=1550",
         "author": "stoyan",
         "comments": "http://calendar.perfplanet.com/2012/make-your-mobile-pages-render-in-under-one-second/#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "performance"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "Make your mobile pages render in under one second"
         },
         "rss:link": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/make-your-mobile-pages-render-in-under-one-second/"
         },
         "rss:comments": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/make-your-mobile-pages-render-in-under-one-second/#comments"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Sun, 23 Dec 2012 21:28:46 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "stoyan"
         },
         "rss:category": {
            "@": {},
            "#": "performance"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "http://calendar.perfplanet.com/?p=1550"
         },
         "rss:description": {
            "@": {},
            "#": "Over the past few years, we&#8217;ve made great strides in understanding and optimizing mobile web performance. However, for the most part, mobile web browsing continues to be slow. Google Analytics data shows that the average web page takes over 10 seconds to load on mobile. We know that a user&#8217;s thought process is typically interrupted [...]"
         },
         "content:encoded": {
            "@": {},
            "#": "<p>Over the past few years, we&#8217;ve made great strides in understanding and optimizing mobile web performance. However, for the most part, mobile web browsing continues to be slow. Google Analytics data shows that the average web page takes over <a href=\"http://analytics.blogspot.com/2012/04/global-site-speed-overview-how-fast-are.html\">10 seconds to load on mobile</a>. We know that a user&#8217;s thought process is typically interrupted after waiting for just one second, resulting in that <a href=\"http://www.useit.com/papers/responsetime.html\">user starting to become disengaged</a>. So at a minimum, the &#8220;above the fold&#8221; content of a web page should render in less than one second. Clearly, we&#8217;ve still got a lot of work to do.</p>\n<p>But where should we focus our attention when optimizing mobile web performance? We know that mobile networks have highly variable latency and bandwidth characteristics, and that in general <a href=\"http://calendar.perfplanet.com/2011/carrier-networks-down-the-rabbit-hole/\">mobile network latency is substantially higher</a> than that on desktop connections. We also know that for modern networks, <a href=\"http://www.belshe.com/2010/05/24/more-bandwidth-doesnt-matter-much/\">it is round trip time, not bandwidth</a>, that is the dominating factor in page load time. Given this, to make the mobile web fast, our attention should be focused on minimizing the number of blocking round trips incurred before a web page can render its content to the device&#8217;s screen.</p>\n<h2>What blocks rendering a web page to the screen?</h2>\n<p>First, let&#8217;s look at the sequence of events that happens between the time a user initiates a page navigation and the time the browser can render that page to the screen. Round trips may be incurred for DNS resolution, TCP connection, and the request being sent to the server and the response being streamed back. Unfortunately, there&#8217;s not much developers can do to avoid these round trips. For repeat visitors, a longer DNS TTL can help, but TCP connection and request/response overhead will be incurred on every new navigation to a page (assuming there is no warm TCP connection ready to be reused by the client).</p>\n<p>Once these initial round trips are incurred, the mobile device can begin parsing the HTML response. But the browser can&#8217;t paint content to the screen just yet. Before content in the HTML can be painted to the screen, the browser must construct the render tree to determine where the elements in the DOM will appear on screen. And before the render tree can be constructed, the DOM tree must be constructed. The DOM tree is constructed through a combination of parsing HTML and possibly JavaScript execution.</p>\n<p>So what are the things that block parsing of HTML, DOM tree construction, and render tree construction? Most of the time, parsing, DOM tree, and render tree construction are very fast. However, there are a few antipatterns that can cause these processes to get blocked on the network.</p>\n<h2>Sources of delay during rendering: external JavaScript and CSS</h2>\n<p>The most significant source of delay during HTML parsing is external JavaScript. When a browser encounters a (non-async) external script during HTML parsing, it must halt parsing of subsequent HTML until that JavaScript is downloaded, parsed, and executed. This incurs additional round trips, which are especially expensive on mobile. If the script is loaded from a hostname other than the hostname the HTML was served from, additional round trips may be incurred for DNS resolution and TCP connection.</p>\n<p>In addition, render tree construction gets blocked on stylesheets, so just as external JavaScript introduces delays during DOM tree construction, external stylesheets introduce delays during render tree construction.</p>\n<p>In short, external JavaScript and CSS loaded early in the document (e.g. in the <code>&lt;head&gt;</code>) are performance killers, and they are especially expensive on mobile due to the higher round trip times associated with mobile networks.</p>\n<h2>Making mobile pages fast</h2>\n<p>To be fast, a mobile web page must include all of the content needed to render the above the fold region in the initial HTML payload without blocking on external JavaScript or CSS resources. Ideally, all the content needed to render the above the fold region should be in the first 15kB on the network (this is post-gzip-compression size; pre-gzip can be larger), since this is the size of the initial congestion window on modern Linux kernels. This does not mean simply inlining all of the JavaScript and CSS that used to be loaded externally. Instead, just the JavaScript and CSS needed to render the above the fold region should be inlined, and JavaScript or CSS needed to add additional functionality to the page should be loaded asynchronously. For instance, if we have a page like the following:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">html</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">head</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">stylesheet</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">my.css</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">script</span><span class=\"hl-code\"> </span><span class=\"hl-var\">src</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">my.js</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">script</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">head</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">body</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">div</span><span class=\"hl-code\"> </span><span class=\"hl-var\">class</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">main</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n    Here is my content.\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">div</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">div</span><span class=\"hl-code\"> </span><span class=\"hl-var\">class</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">leftnav</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n    Perhaps there is a left nav bar here.\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">div</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  ...\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">body</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">html</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<p>We need to identify the parts of <code>my.js</code> and <code>my.css</code> needed to render the initial content, inline those parts, and delay or async load the remaining JavaScript and CSS needed for the page. This may end up looking something like:</p>\n<div class=\"hl-main\">\n<pre><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">html</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">head</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">style</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  .main { ... }\r\n  .leftnav { ... }\r\n  /* ... any other styles needed for the initial render here ... */\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">style</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">script</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  // Any script needed for initial render here.\r\n  // Ideally, there should be no JS needed for the initial render\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">script</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">head</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">body</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">div</span><span class=\"hl-code\"> </span><span class=\"hl-var\">class</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">main</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n    Here is my content.\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">div</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">div</span><span class=\"hl-code\"> </span><span class=\"hl-var\">class</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">leftnav</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n    Perhaps there is a left nav bar here.\r\n  </span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">div</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  ...\r\n  </span><span class=\"hl-comment\">&lt;!--</span><span class=\"hl-comment\"> \r\n    NOTE: delay loading of script and stylesheet may best be done\r\n     in an asynchronous callback such as `requestAnimationFrame` \r\n     rather than inline in HTML, since the callback will be invoked \r\n     after the browser has rendered the earlier HTML content to the screen.\r\n   </span><span class=\"hl-comment\">--&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">link</span><span class=\"hl-code\"> </span><span class=\"hl-var\">rel</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">stylesheet</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-code\"> </span><span class=\"hl-var\">href</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">my_leftover.css</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n  </span><span class=\"hl-brackets\">&lt;</span><span class=\"hl-reserved\">script</span><span class=\"hl-code\"> </span><span class=\"hl-var\">src</span><span class=\"hl-code\">=</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-string\">my_leftover.js</span><span class=\"hl-quotes\">&quot;</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">script</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">body</span><span class=\"hl-brackets\">&gt;</span><span class=\"hl-code\">\r\n</span><span class=\"hl-brackets\">&lt;/</span><span class=\"hl-reserved\">html</span><span class=\"hl-brackets\">&gt;</span></pre>\n</div>\n<h2>The current state of the mobile web</h2>\n<p>A brief survey of mobile web pages shows that nearly all pages include blocking external JavaScript and/or CSS before any above the fold content is displayed. Exceptions include Google Maps, Google Search, Yahoo! News, and sites like <a href=\"http://www.yummly.com/\">http://www.yummly.com/</a> and Kayak. Unfortunately, some of these sites inline JavaScript and CSS that isn&#8217;t needed for the initial render, which unnecessarily delays the time it takes to render these pages.</p>\n<p>Interestingly, browsing the mobile web with JavaScript disabled reveals that even though most pages load blocking external JavaScript in the head, few of these pages actually need that JavaScript to render their initial content. These pages would benefit from delay or async loading their JavaScript to get the JavaScript out of the critical path of the initial render of the page.</p>\n<h2>What else can block the initial render of a web page?</h2>\n<p>Blocking external JavaScript and CSS are the most common sources of delay on the web. However, there are other common sources of delay that mobile developers should be aware of. One source is HTTP redirects of the main HTML document. These redirects incur additional round trips, and if the redirect navigates to a different hostname (e.g. <code>www.example.com</code> to <code>m.example.com</code>), they add even greater delays due to DNS resolution and TCP connection times. A second source of delay is server backend time spent generating the HTML response. All of the time spent generating the initial HTML response will block rendering on the screen, so server backend time should be kept to a minimum.</p>\n<h2>Rendering in under one second</h2>\n<p>If we estimate 3G network round trip time at 250ms, we can compute the minimum estimated time between when a user initiates a web page navigation and when that page renders its above the fold content on the screen. Assuming no blocking external JavaScript or CSS, we incur three round trips for DNS, TCP, and request/response, for a total of 750ms, plus 100ms for backend time. This brings us to 850ms. As long as render-blocking JavaScript and CSS is inlined and the size of the initial HTML payload is kept to a minimum (e.g. under 15kB compressed), the time it takes to parse and render should be well under 100ms, bringing us in at 950ms, just under our one second target.</p>\n<h2>Summary</h2>\n<p>In summary, to make your mobile web page render in under one second, you should:</p>\n<ul>\n<li>keep server backend time to generate HTML to a minimum (under 100ms)</li>\n<li>avoid HTTP redirects for the main HTML resource</li>\n<li>avoid loading blocking external JavaScript and CSS before the initial render</li>\n<li>inline just the JavaScript and CSS needed for the initial render</li>\n<li>delay or async load any JavaScript and CSS not needed for the initial render</li>\n<li>keep HTML payload needed to render initial content to under 15kB compressed</li>\n</ul>\n<p>If you are looking to improve the performance of your mobile web pages, give these optimizations a try.</p>"
         },
         "wfw:commentrss": {
            "@": {},
            "#": "http://calendar.perfplanet.com/2012/make-your-mobile-pages-render-in-under-one-second/feed/"
         },
         "slash:comments": {
            "@": {},
            "#": "7"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "@": [
               {
                  "xmlns:content": "http://purl.org/rss/1.0/modules/content/"
               },
               {
                  "xmlns:wfw": "http://wellformedweb.org/CommentAPI/"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               },
               {
                  "xmlns:atom": "http://www.w3.org/2005/Atom"
               },
               {
                  "xmlns:sy": "http://purl.org/rss/1.0/modules/syndication/"
               },
               {
                  "xmlns:slash": "http://purl.org/rss/1.0/modules/slash/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "UTF-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "Performance Calendar",
            "description": "The speed geek's favorite time of the year",
            "date": "2013-02-26T01:24:02.000Z",
            "pubdate": "2013-02-26T01:24:02.000Z",
            "pubDate": "2013-02-26T01:24:02.000Z",
            "link": "http://calendar.perfplanet.com",
            "xmlurl": "http://calendar.perfplanet.com/feed/",
            "xmlUrl": "http://calendar.perfplanet.com/feed/",
            "author": null,
            "language": "en-US",
            "favicon": null,
            "copyright": null,
            "generator": "http://wordpress.org/?v=3.5.1",
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "Performance Calendar"
            },
            "atom:link": {
               "@": {
                  "href": "http://calendar.perfplanet.com/feed/",
                  "rel": "self",
                  "type": "application/rss+xml"
               }
            },
            "rss:link": {
               "@": {},
               "#": "http://calendar.perfplanet.com"
            },
            "rss:description": {
               "@": {},
               "#": "The speed geek's favorite time of the year"
            },
            "rss:lastbuilddate": {
               "@": {},
               "#": "Tue, 26 Feb 2013 01:24:02 +0000"
            },
            "rss:language": {
               "@": {},
               "#": "en-US"
            },
            "syn:updateperiod": {
               "@": {},
               "#": "hourly"
            },
            "syn:updatefrequency": {
               "@": {},
               "#": "1"
            },
            "rss:generator": {
               "@": {},
               "#": "http://wordpress.org/?v=3.5.1"
            }
         }
      }
   ],
   "http://www.dup2.org/feed.xml": [
      {
         "title": "关于 DSN/ESC",
         "description": "<p>SMTP 协议除了 2XX,4XX,5XX 这些 status codes，后来又发展出一套 Enhanced Status Codes，最基本的在<a href=\"http://tools.ietf.org/html/rfc3463\">http://tools.ietf.org/html/rfc3463</a>，后来又有 RFC 3886, 4468, 4865, 4954, 5248 不断补充。</p>\n<p>除了仔细阅读 RFC 学习之外，微软 Exchange 有份文档描述了若干 ESC 的情况：<a href=\"http://technet.microsoft.com/zh-cn/library/bb232118%28v=exchg.150%29.aspx\">http://technet.microsoft.com/zh-cn/library/bb232118%28v=exchg.150%29.aspx</a></p>\n<p>也有人针对 Postfix 日志写了一个 DSN 翻译脚本：<a href=\"http://melinko2003.blogspot.com/2009/10/centos-postfix-dns-status-script.html\">http://melinko2003.blogspot.com/2009/10/centos-postfix-dns-status-script.html</a></p>",
         "summary": "<p>SMTP 协议除了 2XX,4XX,5XX 这些 status codes，后来又发展出一套 Enhanced Status Codes，最基本的在<a href=\"http://tools.ietf.org/html/rfc3463\">http://tools.ietf.org/html/rfc3463</a>，后来又有 RFC 3886, 4468, 4865, 4954, 5248 不断补充。</p>\n<p>除了仔细阅读 RFC 学习之外，微软 Exchange 有份文档描述了若干 ESC 的情况：<a href=\"http://technet.microsoft.com/zh-cn/library/bb232118%28v=exchg.150%29.aspx\">http://technet.microsoft.com/zh-cn/library/bb232118%28v=exchg.150%29.aspx</a></p>\n<p>也有人针对 Postfix 日志写了一个 DSN 翻译脚本：<a href=\"http://melinko2003.blogspot.com/2009/10/centos-postfix-dns-status-script.html\">http://melinko2003.blogspot.com/2009/10/centos-postfix-dns-status-script.html</a></p>",
         "date": "2013-04-09T18:07:30.000Z",
         "pubdate": "2013-04-09T18:07:30.000Z",
         "pubDate": "2013-04-09T18:07:30.000Z",
         "link": "http://www.dup2.org/node/1516",
         "guid": "1516 at http://www.dup2.org",
         "author": "qyb",
         "comments": "http://www.dup2.org/node/1516#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "电子邮件"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "关于 DSN/ESC"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1516"
         },
         "rss:description": {
            "@": {},
            "#": "<p>SMTP 协议除了 2XX,4XX,5XX 这些 status codes，后来又发展出一套 Enhanced Status Codes，最基本的在<a href=\"http://tools.ietf.org/html/rfc3463\">http://tools.ietf.org/html/rfc3463</a>，后来又有 RFC 3886, 4468, 4865, 4954, 5248 不断补充。</p>\n<p>除了仔细阅读 RFC 学习之外，微软 Exchange 有份文档描述了若干 ESC 的情况：<a href=\"http://technet.microsoft.com/zh-cn/library/bb232118%28v=exchg.150%29.aspx\">http://technet.microsoft.com/zh-cn/library/bb232118%28v=exchg.150%29.aspx</a></p>\n<p>也有人针对 Postfix 日志写了一个 DSN 翻译脚本：<a href=\"http://melinko2003.blogspot.com/2009/10/centos-postfix-dns-status-script.html\">http://melinko2003.blogspot.com/2009/10/centos-postfix-dns-status-script.html</a></p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1516#comments"
         },
         "rss:category": {
            "@": {
               "domain": "http://www.dup2.org/taxonomy/term/53"
            },
            "#": "电子邮件"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Tue, 09 Apr 2013 18:07:30 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyb"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1516 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      },
      {
         "title": "粤语里的卷舌",
         "description": "<p>总觉得自己粤语歌唱的越来越差，很多音自己唱出来都觉得发得不准确</p>\n<p>自我分析是因为长期说普通话，为了把卷舌不卷舌分得清楚，不卷舌的音就会把舌头打得特别平</p>\n<p>这就导致粤语里的有些音我发不出来了！！虽然北方人会觉得南方人说话不卷舌，但其实粤语里面发音细细分辨还是很丰富的，有些音可能舌头卷得不多，但是要在口腔里打转</p>\n<p>总之粤语歌需要重新练练了</p>",
         "summary": "<p>总觉得自己粤语歌唱的越来越差，很多音自己唱出来都觉得发得不准确</p>\n<p>自我分析是因为长期说普通话，为了把卷舌不卷舌分得清楚，不卷舌的音就会把舌头打得特别平</p>\n<p>这就导致粤语里的有些音我发不出来了！！虽然北方人会觉得南方人说话不卷舌，但其实粤语里面发音细细分辨还是很丰富的，有些音可能舌头卷得不多，但是要在口腔里打转</p>\n<p>总之粤语歌需要重新练练了</p>",
         "date": "2013-04-09T06:20:25.000Z",
         "pubdate": "2013-04-09T06:20:25.000Z",
         "pubDate": "2013-04-09T06:20:25.000Z",
         "link": "http://www.dup2.org/node/1515",
         "guid": "1515 at http://www.dup2.org",
         "author": "qyb",
         "comments": "http://www.dup2.org/node/1515#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "文化",
            "音乐"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "粤语里的卷舌"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1515"
         },
         "rss:description": {
            "@": {},
            "#": "<p>总觉得自己粤语歌唱的越来越差，很多音自己唱出来都觉得发得不准确</p>\n<p>自我分析是因为长期说普通话，为了把卷舌不卷舌分得清楚，不卷舌的音就会把舌头打得特别平</p>\n<p>这就导致粤语里的有些音我发不出来了！！虽然北方人会觉得南方人说话不卷舌，但其实粤语里面发音细细分辨还是很丰富的，有些音可能舌头卷得不多，但是要在口腔里打转</p>\n<p>总之粤语歌需要重新练练了</p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1515#comments"
         },
         "rss:category": [
            {
               "@": {
                  "domain": "http://www.dup2.org/taxonomy/term/6"
               },
               "#": "文化"
            },
            {
               "@": {
                  "domain": "http://www.dup2.org/taxonomy/term/5"
               },
               "#": "音乐"
            }
         ],
         "rss:pubdate": {
            "@": {},
            "#": "Tue, 09 Apr 2013 06:20:25 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyb"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1515 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      },
      {
         "title": "邱可心最近二三事",
         "description": "<p>这丫头身高1.53m了，稳步而坚定的向她小姑姑——我们家庭第一女性身高——发起超越</p>\n<p>最近一个月爬了一次香山、一次凤凰岭，她一路都没有叫苦叫累，虽然心里可能还是不爽，路上也老是嚷嚷着要买饮料冰激凌，但至少不再抱怨爬山这件事，比以前有了很大的提高。<br />\n计划什么时候开始带她跑步</p>\n<p>今年暖气停了以后，屋里冷，晚上又不开空调，于是她就挤在大床上睡。结果有一天晚上她居然把被子踢了后，钻到我被子里；然后我把她重新盖好，早上醒来时发现她还是在我被子里。。。。。。真是温馨啊，但是这样的事情以后很可能不再会发生了。sigh</p>\n<p>昨晚回到家里，她给我秀她收到的柯南卡片。我问：是哪个小男生送的呀？她说不知道，课间休息出去玩回到了教室就看到桌上有这张卡片<br />\n-- 也不知道这事是真是假<br />\n-- 如果是真的，她居然没有和她老妈汇报���而是和老爹说（我回家很晚，她有大把时间交流这个）。。是什么潜意识在吗？</p>",
         "summary": "<p>这丫头身高1.53m了，稳步而坚定的向她小姑姑——我们家庭第一女性身高——发起超越</p>\n<p>最近一个月爬了一次香山、一次凤凰岭，她一路都没有叫苦叫累，虽然心里可能还是不爽，路上也老是嚷嚷着要买饮料冰激凌，但至少不再抱怨爬山这件事，比以前有了很大的提高。<br />\n计划什么时候开始带她跑步</p>\n<p>今年暖气停了以后，屋里冷，晚上又不开空调，于是她就挤在大床上睡。结果有一天晚上她居然把被子踢了后，钻到我被子里；然后我把她重新盖好，早上醒来时发现她还是在我被子里。。。。。。真是温馨啊，但是这样的事情以后很可能不再会发生了。sigh</p>\n<p>昨晚回到家里，她给我秀她收到的柯南卡片。我问：是哪个小男生送的呀？她说不知道，课间休息出去玩回到了教室就看到桌上有这张卡片<br />\n-- 也不知道这事是真是假<br />\n-- 如果是真的，她居然没有和她老妈汇报���而是和老爹说（我回家很晚，她有大把时间交流这个）。。是什么潜意识在吗？</p>",
         "date": "2013-04-09T06:15:06.000Z",
         "pubdate": "2013-04-09T06:15:06.000Z",
         "pubDate": "2013-04-09T06:15:06.000Z",
         "link": "http://www.dup2.org/node/1514",
         "guid": "1514 at http://www.dup2.org",
         "author": "qyb",
         "comments": "http://www.dup2.org/node/1514#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "dada"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "邱可心最近二三事"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1514"
         },
         "rss:description": {
            "@": {},
            "#": "<p>这丫头身高1.53m了，稳步而坚定的向她小姑姑——我们家庭第一女性身高——发起超越</p>\n<p>最近一个月爬了一次香山、一次凤凰岭，她一路都没有叫苦叫累，虽然心里可能还是不爽，路上也老是嚷嚷着要买饮料冰激凌，但至少不再抱怨爬山这件事，比以前有了很大的提高。<br />\n计划什么时候开始带她跑步</p>\n<p>今年暖气停了以后，屋里冷，晚上又不开空调，于是她就挤在大床上睡。结果有一天晚上她居然把被子踢了后，钻到我被子里；然后我把她重新盖好，早上醒来时发现她还是在我被子里。。。。。。真是温馨啊，但是这样的事情以后很可能不再会发生了。sigh</p>\n<p>昨晚回到家里，她给我秀她收到的柯南卡片。我问：是哪个小男生送的呀？她说不知道，课间休息出去玩回到了教室就看到桌上有这张卡片<br />\n-- 也不知道这事是真是假<br />\n-- 如果是真的，她居然没有和她老妈汇报���而是和老爹说（我回家很晚，她有大把时间交流这个）。。是什么潜意识在吗？</p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1514#comments"
         },
         "rss:category": {
            "@": {
               "domain": "http://www.dup2.org/taxonomy/term/4"
            },
            "#": "dada"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Tue, 09 Apr 2013 06:15:06 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyb"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1514 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      },
      {
         "title": "记录推推 22 个月的一段对话",
         "description": "<p>【晚上 LP 抱着推推在阳台玻璃前往外面看灯、车什么的，我在旁边凑过来凑过去】<br />\n              我【伸手】：推推，爸爸抱一下？<br />\n  推推【用手拨开我的手】：不要不要<br />\n          我【继续伸手】：推推，爸爸抱一下？妈妈累了！<br />\n【推推紧紧地趴在 LP 身上】<br />\n      LP【可怜地看着我】：推推，爸爸站得高，看得远，让爸爸抱抱好不好？<br />\n【推推直起身，看看我，然后向我伸出了手】<br />\n【我抱了一会儿，从阳台换到客厅继续晃悠，LP 在旁边凑过来凑过去】<br />\n              LP【伸手】：推推，妈妈抱一下？<br />\n推推【紧紧地趴在我身上】：妈妈累了……</p>",
         "summary": "<p>【晚上 LP 抱着推推在阳台玻璃前往外面看灯、车什么的，我在旁边凑过来凑过去】<br />\n              我【伸手】：推推，爸爸抱一下？<br />\n  推推【用手拨开我的手】：不要不要<br />\n          我【继续伸手】：推推，爸爸抱一下？妈妈累了！<br />\n【推推紧紧地趴在 LP 身上】<br />\n      LP【可怜地看着我】：推推，爸爸站得高，看得远，让爸爸抱抱好不好？<br />\n【推推直起身，看看我，然后向我伸出了手】<br />\n【我抱了一会儿，从阳台换到客厅继续晃悠，LP 在旁边凑过来凑过去】<br />\n              LP【伸手】：推推，妈妈抱一下？<br />\n推推【紧紧地趴在我身上】：妈妈累了……</p>",
         "date": "2013-03-28T15:34:27.000Z",
         "pubdate": "2013-03-28T15:34:27.000Z",
         "pubDate": "2013-03-28T15:34:27.000Z",
         "link": "http://www.dup2.org/node/1513",
         "guid": "1513 at http://www.dup2.org",
         "author": "qyt",
         "comments": "http://www.dup2.org/node/1513#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "tuitui"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "记录推推 22 个月的一段对话"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1513"
         },
         "rss:description": {
            "@": {},
            "#": "<p>【晚上 LP 抱着推推在阳台玻璃前往外面看灯、车什么的，我在旁边凑过来凑过去】<br />\n              我【伸手】：推推，爸爸抱一下？<br />\n  推推【用手拨开我的手】：不要不要<br />\n          我【继续伸手】：推推，爸爸抱一下？妈妈累了！<br />\n【推推紧紧地趴在 LP 身上】<br />\n      LP【可怜地看着我】：推推，爸爸站得高，看得远，让爸爸抱抱好不好？<br />\n【推推直起身，看看我，然后向我伸出了手】<br />\n【我抱了一会儿，从阳台换到客厅继续晃悠，LP 在旁边凑过来凑过去】<br />\n              LP【伸手】：推推，妈妈抱一下？<br />\n推推【紧紧地趴在我身上】：妈妈累了……</p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1513#comments"
         },
         "rss:category": {
            "@": {
               "domain": "http://www.dup2.org/taxonomy/term/51"
            },
            "#": "tuitui"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Thu, 28 Mar 2013 15:34:27 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyt"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1513 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      },
      {
         "title": "(13)Permission denied 或者 403 Forbidden",
         "description": "<p>这两天接了个任务，调研 code review 工具。</p>\n<p>前两天在 centos 上折腾 Review Board，rb-site 完成后，启动 httpd，error log 里报 [error] avahi_entry_group_add_service_strlst(\"localhost\") failed: Invalid host name，这问题怎么着也没解决。最后换了 ubuntu，顺利异常，所有包都有二进制的，不像在 centos 里还编译了两个包。啥问题没遇到，就进入设置页面了。</p>\n<p>今天继续在 centos 上试另一个 phabricator，安装很容易。访问页面报 403 You don't have permission to access / on this server，又搞了良久，chmod 了，chown 了，Allow from all 了，全都没有作用。关键字又按照 error log 里的错误(13)Permission denied: access to / denied 来搜，搜到一个 <a href=\"http://www.petefreitag.com/item/793.cfm\">Fixing Apache (13)Permission denied: access to / 403 Forbidden</a>，按照这个里面一步一步又过了一遍，除了倒数第二步 Make sure that the Directory Above has Execute Permission 没仔细看，倒数第一步也研究了一下，根据<a href=\"http://hi.baidu.com/myloveredhat/item/8c1f56ecc1453dc0baf37d4b\">究竟什么是SElinux?</a>发现这台 centos 应该不是 SELinux。回过头来仔细读了一下倒数第二步，这里面说 chmod 不能只改 /path/to/webroot/，应该连整个 /path 都改了才行。怀着死马当活马医的心情执行了一下。哈哈哈哈————</p>",
         "summary": "<p>这两天接了个任务，调研 code review 工具。</p>\n<p>前两天在 centos 上折腾 Review Board，rb-site 完成后，启动 httpd，error log 里报 [error] avahi_entry_group_add_service_strlst(\"localhost\") failed: Invalid host name，这问题怎么着也没解决。最后换了 ubuntu，顺利异常，所有包都有二进制的，不像在 centos 里还编译了两个包。啥问题没遇到，就进入设置页面了。</p>\n<p>今天继续在 centos 上试另一个 phabricator，安装很容易。访问页面报 403 You don't have permission to access / on this server，又搞了良久，chmod 了，chown 了，Allow from all 了，全都没有作用。关键字又按照 error log 里的错误(13)Permission denied: access to / denied 来搜，搜到一个 <a href=\"http://www.petefreitag.com/item/793.cfm\">Fixing Apache (13)Permission denied: access to / 403 Forbidden</a>，按照这个里面一步一步又过了一遍，除了倒数第二步 Make sure that the Directory Above has Execute Permission 没仔细看，倒数第一步也研究了一下，根据<a href=\"http://hi.baidu.com/myloveredhat/item/8c1f56ecc1453dc0baf37d4b\">究竟什么是SElinux?</a>发现这台 centos 应该不是 SELinux。回过头来仔细读了一下倒数第二步，这里面说 chmod 不能只改 /path/to/webroot/，应该连整个 /path 都改了才行。怀着死马当活马医的心情执行了一下。哈哈哈哈————</p>",
         "date": "2013-03-20T09:06:21.000Z",
         "pubdate": "2013-03-20T09:06:21.000Z",
         "pubDate": "2013-03-20T09:06:21.000Z",
         "link": "http://www.dup2.org/node/1512",
         "guid": "1512 at http://www.dup2.org",
         "author": "qyt",
         "comments": "http://www.dup2.org/node/1512#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "技术",
            "网络"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "(13)Permission denied 或者 403 Forbidden"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1512"
         },
         "rss:description": {
            "@": {},
            "#": "<p>这两天接了个任务，调研 code review 工具。</p>\n<p>前两天在 centos 上折腾 Review Board，rb-site 完成后，启动 httpd，error log 里报 [error] avahi_entry_group_add_service_strlst(\"localhost\") failed: Invalid host name，这问题怎么着也没解决。最后换了 ubuntu，顺利异常，所有包都有二进制的，不像在 centos 里还编译了两个包。啥问题没遇到，就进入设置页面了。</p>\n<p>今天继续在 centos 上试另一个 phabricator，安装很容易。访问页面报 403 You don't have permission to access / on this server，又搞了良久，chmod 了，chown 了，Allow from all 了，全都没有作用。关键字又按照 error log 里的错误(13)Permission denied: access to / denied 来搜，搜到一个 <a href=\"http://www.petefreitag.com/item/793.cfm\">Fixing Apache (13)Permission denied: access to / 403 Forbidden</a>，按照这个里面一步一步又过了一遍，除了倒数第二步 Make sure that the Directory Above has Execute Permission 没仔细看，倒数第一步也研究了一下，根据<a href=\"http://hi.baidu.com/myloveredhat/item/8c1f56ecc1453dc0baf37d4b\">究竟什么是SElinux?</a>发现这台 centos 应该不是 SELinux。回过头来仔细读了一下倒数第二步，这里面说 chmod 不能只改 /path/to/webroot/，应该连整个 /path 都改了才行。怀着死马当活马医的心情执行了一下。哈哈哈哈————</p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1512#comments"
         },
         "rss:category": [
            {
               "@": {
                  "domain": "http://www.dup2.org/taxonomy/term/1"
               },
               "#": "技术"
            },
            {
               "@": {
                  "domain": "http://www.dup2.org/taxonomy/term/10"
               },
               "#": "网络"
            }
         ],
         "rss:pubdate": {
            "@": {},
            "#": "Wed, 20 Mar 2013 09:06:21 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyt"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1512 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      },
      {
         "title": "武汉日记（十）",
         "description": "<p>“<a href=\"http://techcrunch.com/2013/02/06/angellists-naval-ravikant-says-the-future-of-vc-is-smaller-funding-rounds-more-enterprise-and-hardware/\">http://t.cn/zYczuAR</a> AngelList某合伙人认为早期创投的趋势是每轮融资额会变少（精细创业？）传统种子$750k，A轮3-5M——将来种子$250-500k，A轮可能只有1M。大VC会减少早期创投的关注，小微型VC在该领域不断增长。创业公司会越来越多”</p>\n<p>以“投资经理”的身份回顾这三个项目，SendCloud和企业网盘大概各花了$250-$350k的钱，下一步就应该是寻求A轮融资，支撑商业发展了。随身看这个项目当前的状态相当于团队接了一个外包项目，暂且养活自己，慢慢寻找新的机会</p>\n<p>Update: 要是给团队70%-80%股权的话，Seed fund 投资额说不定可以再减少30%-50%</p>",
         "summary": "<p>“<a href=\"http://techcrunch.com/2013/02/06/angellists-naval-ravikant-says-the-future-of-vc-is-smaller-funding-rounds-more-enterprise-and-hardware/\">http://t.cn/zYczuAR</a> AngelList某合伙人认为早期创投的趋势是每轮融资额会变少（精细创业？）传统种子$750k，A轮3-5M——将来种子$250-500k，A轮可能只有1M。大VC会减少早期创投的关注，小微型VC在该领域不断增长。创业公司会越来越多”</p>\n<p>以“投资经理”的身份回顾这三个项目，SendCloud和企业网盘大概各花了$250-$350k的钱，下一步就应该是寻求A轮融资，支撑商业发展了。随身看这个项目当前的状态相当于团队接了一个外包项目，暂且养活自己，慢慢寻找新的机会</p>\n<p>Update: 要是给团队70%-80%股权的话，Seed fund 投资额说不定可以再减少30%-50%</p>",
         "date": "2013-02-07T02:58:21.000Z",
         "pubdate": "2013-02-07T02:58:21.000Z",
         "pubDate": "2013-02-07T02:58:21.000Z",
         "link": "http://www.dup2.org/node/1511",
         "guid": "1511 at http://www.dup2.org",
         "author": "qyb",
         "comments": "http://www.dup2.org/node/1511#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "商业"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "武汉日记（十）"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1511"
         },
         "rss:description": {
            "@": {},
            "#": "<p>“<a href=\"http://techcrunch.com/2013/02/06/angellists-naval-ravikant-says-the-future-of-vc-is-smaller-funding-rounds-more-enterprise-and-hardware/\">http://t.cn/zYczuAR</a> AngelList某合伙人认为早期创投的趋势是每轮融资额会变少（精细创业？）传统种子$750k，A轮3-5M——将来种子$250-500k，A轮可能只有1M。大VC会减少早期创投的关注，小微型VC在该领域不断增长。创业公司会越来越多”</p>\n<p>以“投资经理”的身份回顾这三个项目，SendCloud和企业网盘大概各花了$250-$350k的钱，下一步就应该是寻求A轮融资，支撑商业发展了。随身看这个项目当前的状态相当于团队接了一个外包项目，暂且养活自己，慢慢寻找新的机会</p>\n<p>Update: 要是给团队70%-80%股权的话，Seed fund 投资额说不定可以再减少30%-50%</p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1511#comments"
         },
         "rss:category": {
            "@": {
               "domain": "http://www.dup2.org/taxonomy/term/3"
            },
            "#": "商业"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Thu, 07 Feb 2013 02:58:21 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyb"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1511 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      },
      {
         "title": "MailChimp 2012",
         "description": "<p><a href=\"http://mailchimp.com/2012/\">http://mailchimp.com/2012/</a></p>\n<p>期待 SendCloud 2013</p>",
         "summary": "<p><a href=\"http://mailchimp.com/2012/\">http://mailchimp.com/2012/</a></p>\n<p>期待 SendCloud 2013</p>",
         "date": "2013-01-28T09:23:58.000Z",
         "pubdate": "2013-01-28T09:23:58.000Z",
         "pubDate": "2013-01-28T09:23:58.000Z",
         "link": "http://www.dup2.org/node/1510",
         "guid": "1510 at http://www.dup2.org",
         "author": "qyb",
         "comments": "http://www.dup2.org/node/1510#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "电子邮件"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "MailChimp 2012"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1510"
         },
         "rss:description": {
            "@": {},
            "#": "<p><a href=\"http://mailchimp.com/2012/\">http://mailchimp.com/2012/</a></p>\n<p>期待 SendCloud 2013</p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1510#comments"
         },
         "rss:category": {
            "@": {
               "domain": "http://www.dup2.org/taxonomy/term/53"
            },
            "#": "电子邮件"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Mon, 28 Jan 2013 09:23:58 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyb"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1510 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      },
      {
         "title": "再建两只邮件相关的产品技术团队",
         "description": "<p>武汉除了 SendCloud 之外，个人邮箱的团队也开始建设了。继上次立鹏同学之后，本晚我又进行了一次技术的讲解，内容包括：</p>\n<ul>\n<li> sohumc package，立马有同学指出用现在 kan.sohu.com 使用的是 python-virtualenv </li>\n<li> sohu 的 Maildir 内容，以及 deliver\n<ul>\n<li> tmp-&gt;new，隔离 deliver 逻辑 </li>\n<li> new-&gt;cur，隔离 mua 逻辑 </li>\n<li> 过滤器的一个小细节 </li>\n<li> SQL索引和搜索索引</li>\n</ul>\n</li>\n<li> MX 的几个要点\n<ul>\n<li> tcptable、milter 协议 </li>\n<li> 队列、磁盘性能 </li>\n<li> 尽可能早地发现 spammer，断开连接 </li>\n<li> postfix 并发能力限制，以及将来的资源隔离保护的设想 </li>\n<li> 包括向后台投递在内，任何一个环节故障，都会导致 MX 的不良反应 </li>\n</ul>\n</li>\n<li> tcptable ，以及企邮团队在2012年踩的坑 </li>\n<li> 在 postfix 外发层面做过的修改 </li>\n<li> <a href=\"http://www.ituring.com.cn/article/8111\">Berkeley DB</a>，以及我们封装的 SMDB </li>\n</ul>\n<p>说来也奇妙，本来这个团队刚刚开始招聘的时候——大概是2011年11月份左右——我就是计划着做邮件，因此从CPyUG找人；虽然中间有变化，但最终还是落在了这些同学身上，可见真是命运冥冥安排。<br />\n下一次来武汉计划会详细介绍 Milter 和 Antispam 相关的知识，再加上 SMTP、Nginx 吧</p>\n<p>北京计划从邮件周边着手，配合移动端再想想办法启动一组人马</p>\n<p>一手是围绕Email、云存储来深化运营；另一手是云计算平台和大数据的技术积累。就是这样了</p>",
         "summary": "<p>武汉除了 SendCloud 之外，个人邮箱的团队也开始建设了。继上次立鹏同学之后，本晚我又进行了一次技术的讲解，内容包括：</p>\n<ul>\n<li> sohumc package，立马有同学指出用现在 kan.sohu.com 使用的是 python-virtualenv </li>\n<li> sohu 的 Maildir 内容，以及 deliver\n<ul>\n<li> tmp-&gt;new，隔离 deliver 逻辑 </li>\n<li> new-&gt;cur，隔离 mua 逻辑 </li>\n<li> 过滤器的一个小细节 </li>\n<li> SQL索引和搜索索引</li>\n</ul>\n</li>\n<li> MX 的几个要点\n<ul>\n<li> tcptable、milter 协议 </li>\n<li> 队列、磁盘性能 </li>\n<li> 尽可能早地发现 spammer，断开连接 </li>\n<li> postfix 并发能力限制，以及将来的资源隔离保护的设想 </li>\n<li> 包括向后台投递在内，任何一个环节故障，都会导致 MX 的不良反应 </li>\n</ul>\n</li>\n<li> tcptable ，以及企邮团队在2012年踩的坑 </li>\n<li> 在 postfix 外发层面做过的修改 </li>\n<li> <a href=\"http://www.ituring.com.cn/article/8111\">Berkeley DB</a>，以及我们封装的 SMDB </li>\n</ul>\n<p>说来也奇妙，本来这个团队刚刚开始招聘的时候——大概是2011年11月份左右——我就是计划着做邮件，因此从CPyUG找人；虽然中间有变化，但最终还是落在了这些同学身上，可见真是命运冥冥安排。<br />\n下一次来武汉计划会详细介绍 Milter 和 Antispam 相关的知识，再加上 SMTP、Nginx 吧</p>\n<p>北京计划从邮件周边着手，配合移动端再想想办法启动一组人马</p>\n<p>一手是围绕Email、云存储来深化运营；另一手是云计算平台和大数据的技术积累。就是这样了</p>",
         "date": "2013-01-10T14:03:36.000Z",
         "pubdate": "2013-01-10T14:03:36.000Z",
         "pubDate": "2013-01-10T14:03:36.000Z",
         "link": "http://www.dup2.org/node/1509",
         "guid": "1509 at http://www.dup2.org",
         "author": "qyb",
         "comments": "http://www.dup2.org/node/1509#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "电子邮件",
            "生活"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "再建两只邮件相关的产品技术团队"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1509"
         },
         "rss:description": {
            "@": {},
            "#": "<p>武汉除了 SendCloud 之外，个人邮箱的团队也开始建设了。继上次立鹏同学之后，本晚我又进行了一次技术的讲解，内容包括：</p>\n<ul>\n<li> sohumc package，立马有同学指出用现在 kan.sohu.com 使用的是 python-virtualenv </li>\n<li> sohu 的 Maildir 内容，以及 deliver\n<ul>\n<li> tmp-&gt;new，隔离 deliver 逻辑 </li>\n<li> new-&gt;cur，隔离 mua 逻辑 </li>\n<li> 过滤器的一个小细节 </li>\n<li> SQL索引和搜索索引</li>\n</ul>\n</li>\n<li> MX 的几个要点\n<ul>\n<li> tcptable、milter 协议 </li>\n<li> 队列、磁盘性能 </li>\n<li> 尽可能早地发现 spammer，断开连接 </li>\n<li> postfix 并发能力限制，以及将来的资源隔离保护的设想 </li>\n<li> 包括向后台投递在内，任何一个环节故障，都会导致 MX 的不良反应 </li>\n</ul>\n</li>\n<li> tcptable ，以及企邮团队在2012年踩的坑 </li>\n<li> 在 postfix 外发层面做过的修改 </li>\n<li> <a href=\"http://www.ituring.com.cn/article/8111\">Berkeley DB</a>，以及我们封装的 SMDB </li>\n</ul>\n<p>说来也奇妙，本来这个团队刚刚开始招聘的时候——大概是2011年11月份左右——我就是计划着做邮件，因此从CPyUG找人；虽然中间有变化，但最终还是落在了这些同学身上，可见真是命运冥冥安排。<br />\n下一次来武汉计划会详细介绍 Milter 和 Antispam 相关的知识，再加上 SMTP、Nginx 吧</p>\n<p>北京计划从邮件周边着手，配合移动端再想想办法启动一组人马</p>\n<p>一手是围绕Email、云存储来深化运营；另一手是云计算平台和大数据的技术积累。就是这样了</p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1509#comments"
         },
         "rss:category": [
            {
               "@": {
                  "domain": "http://www.dup2.org/taxonomy/term/53"
               },
               "#": "电子邮件"
            },
            {
               "@": {
                  "domain": "http://www.dup2.org/taxonomy/term/2"
               },
               "#": "生活"
            }
         ],
         "rss:pubdate": {
            "@": {},
            "#": "Thu, 10 Jan 2013 14:03:36 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyb"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1509 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      },
      {
         "title": "2012/2013",
         "description": "<p>关于 2012</p>\n<p>年度阅读：《公正》<br />\n年度运动：跑步<br />\n年度品牌：国航<br />\n年度网站：TechCrunch<br />\n年度App：Endomondo<br />\n年度纨绔：威可多<br />\n年度电影：Battleship —— 虽然片子硬伤很多，但是这种大舰巨炮对轰的场景真过瘾</p>\n<p>回顾去年的若干新年愿望 <a href=\"http://www.dup2.org/node/1459\"> 2011/2012 </a>，汗颜啊，有2点：<br />\n1. 俯卧撑没有能坚持。。。任何事情最重要的是坚持，今年继续列上这个目标吧<br />\n2. 这一年明白了自己工作上很多不足的地方，管200人现在看是一个玩笑，把这个作为一个3-4年的发展目标吧</p>\n<p>2013是本命年，希望平平安安，不让身边人失望</p>",
         "summary": "<p>关于 2012</p>\n<p>年度阅读：《公正》<br />\n年度运动：跑步<br />\n年度品牌：国航<br />\n年度网站：TechCrunch<br />\n年度App：Endomondo<br />\n年度纨绔：威可多<br />\n年度电影：Battleship —— 虽然片子硬伤很多，但是这种大舰巨炮对轰的场景真过瘾</p>\n<p>回顾去年的若干新年愿望 <a href=\"http://www.dup2.org/node/1459\"> 2011/2012 </a>，汗颜啊，有2点：<br />\n1. 俯卧撑没有能坚持。。。任何事情最重要的是坚持，今年继续列上这个目标吧<br />\n2. 这一年明白了自己工作上很多不足的地方，管200人现在看是一个玩笑，把这个作为一个3-4年的发展目标吧</p>\n<p>2013是本命年，希望平平安安，不让身边人失望</p>",
         "date": "2013-01-06T01:48:36.000Z",
         "pubdate": "2013-01-06T01:48:36.000Z",
         "pubDate": "2013-01-06T01:48:36.000Z",
         "link": "http://www.dup2.org/node/1508",
         "guid": "1508 at http://www.dup2.org",
         "author": "qyb",
         "comments": "http://www.dup2.org/node/1508#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "生活"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "2012/2013"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1508"
         },
         "rss:description": {
            "@": {},
            "#": "<p>关于 2012</p>\n<p>年度阅读：《公正》<br />\n年度运动：跑步<br />\n年度品牌：国航<br />\n年度网站：TechCrunch<br />\n年度App：Endomondo<br />\n年度纨绔：威可多<br />\n年度电影：Battleship —— 虽然片子硬伤很多，但是这种大舰巨炮对轰的场景真过瘾</p>\n<p>回顾去年的若干新年愿望 <a href=\"http://www.dup2.org/node/1459\"> 2011/2012 </a>，汗颜啊，有2点：<br />\n1. 俯卧撑没有能坚持。。。任何事情最重要的是坚持，今年继续列上这个目标吧<br />\n2. 这一年明白了自己工作上很多不足的地方，管200人现在看是一个玩笑，把这个作为一个3-4年的发展目标吧</p>\n<p>2013是本命年，希望平平安安，不让身边人失望</p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1508#comments"
         },
         "rss:category": {
            "@": {
               "domain": "http://www.dup2.org/taxonomy/term/2"
            },
            "#": "生活"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Sun, 06 Jan 2013 01:48:36 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyb"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1508 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      },
      {
         "title": "八股新闻通稿一篇",
         "description": "<p>搜狐的无数基础服务和业务是基于开源软件构建的，没有开源软件，可以说不仅仅是搜狐，整个互联网行业都会丧失存在的基础。</p>\n<p>从业务的层面，我们依赖于开源社区这个生态环境的繁荣发展；从技术的层面，我们希望工程师在开源社区里同高水平的同行相互学习，磨砺自身的能力；从文化的层面，我们推崇开源社区里以代码为中心，以真实需求为驱动，开放、平等、自由地进行交流合作的氛围。</p>\n<p>为了鼓励工程师和开源社区有更深的合作.....</p>",
         "summary": "<p>搜狐的无数基础服务和业务是基于开源软件构建的，没有开源软件，可以说不仅仅是搜狐，整个互联网行业都会丧失存在的基础。</p>\n<p>从业务的层面，我们依赖于开源社区这个生态环境的繁荣发展；从技术的层面，我们希望工程师在开源社区里同高水平的同行相互学习，磨砺自身的能力；从文化的层面，我们推崇开源社区里以代码为中心，以真实需求为驱动，开放、平等、自由地进行交流合作的氛围。</p>\n<p>为了鼓励工程师和开源社区有更深的合作.....</p>",
         "date": "2012-12-29T03:08:33.000Z",
         "pubdate": "2012-12-29T03:08:33.000Z",
         "pubDate": "2012-12-29T03:08:33.000Z",
         "link": "http://www.dup2.org/node/1507",
         "guid": "1507 at http://www.dup2.org",
         "author": "qyb",
         "comments": "http://www.dup2.org/node/1507#comments",
         "origlink": null,
         "image": {},
         "source": {},
         "categories": [
            "技术"
         ],
         "enclosures": [],
         "rss:@": {},
         "rss:title": {
            "@": {},
            "#": "八股新闻通稿一篇"
         },
         "rss:link": {
            "@": {},
            "#": "http://www.dup2.org/node/1507"
         },
         "rss:description": {
            "@": {},
            "#": "<p>搜狐的无数基础服务和业务是基于开源软件构建的，没有开源软件，可以说不仅仅是搜狐，整个互联网行业都会丧失存在的基础。</p>\n<p>从业务的层面，我们依赖于开源社区这个生态环境的繁荣发展；从技术的层面，我们希望工程师在开源社区里同高水平的同行相互学习，磨砺自身的能力；从文化的层面，我们推崇开源社区里以代码为中心，以真实需求为驱动，开放、平等、自由地进行交流合作的氛围。</p>\n<p>为了鼓励工程师和开源社区有更深的合作.....</p>"
         },
         "rss:comments": {
            "@": {},
            "#": "http://www.dup2.org/node/1507#comments"
         },
         "rss:category": {
            "@": {
               "domain": "http://www.dup2.org/taxonomy/term/1"
            },
            "#": "技术"
         },
         "rss:pubdate": {
            "@": {},
            "#": "Sat, 29 Dec 2012 03:08:33 +0000"
         },
         "dc:creator": {
            "@": {},
            "#": "qyb"
         },
         "rss:guid": {
            "@": {
               "ispermalink": "false"
            },
            "#": "1507 at http://www.dup2.org"
         },
         "meta": {
            "#ns": [
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "@": [
               {
                  "xml:base": "http://www.dup2.org"
               },
               {
                  "xmlns:dc": "http://purl.org/dc/elements/1.1/"
               }
            ],
            "#xml": {
               "version": "1.0",
               "encoding": "utf-8"
            },
            "#type": "rss",
            "#version": "2.0",
            "title": "BT的花 blogs",
            "description": null,
            "date": null,
            "pubdate": null,
            "pubDate": null,
            "link": "http://www.dup2.org/blog",
            "xmlurl": null,
            "xmlUrl": null,
            "author": null,
            "language": "zh-hans",
            "favicon": null,
            "copyright": null,
            "generator": null,
            "cloud": {},
            "image": {},
            "categories": [],
            "rss:@": {},
            "rss:title": {
               "@": {},
               "#": "BT的花 blogs"
            },
            "rss:link": {
               "@": {},
               "#": "http://www.dup2.org/blog"
            },
            "rss:description": {
               "@": {}
            },
            "rss:language": {
               "@": {},
               "#": "zh-hans"
            }
         }
      }
   ]
}
